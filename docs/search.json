[{"path":"index.html","id":"welcome","chapter":"1 Welcome!","heading":"1 Welcome!","text":"Welcome Species Distribution Modeling (SDM) Course! 🎉 course designed provide theoretical insights hands-practical skills understanding predicting species distributions. Whether ’re new SDMs looking deepen expertise, ’re right place!Goal: Equip participants knowledge tools build reliable species distribution models using R.","code":""},{"path":"index.html","id":"what-to-expect","chapter":"1 Welcome!","heading":"1.1 What to Expect","text":"Throughout course, engage :Interactive lectures covering key SDM concepts.Hands-practical sessions involving spatial data analysis R.Collaborative learning discussions group activities.","code":""},{"path":"index.html","id":"learning-path","chapter":"1 Welcome!","heading":"1.1.1 Learning Path","text":"","code":""},{"path":"index.html","id":"how-to-get-started","chapter":"1 Welcome!","heading":"1.2 How to Get Started 🚀","text":"Follow steps set environment begin SDM journey:Check Resource Hub:\nAccess essential files, including datasets, scripts, reading materials, via shared Google Drive.\nAccess essential files, including datasets, scripts, reading materials, via shared Google Drive.Set Environment:\nEnsure R RStudio installed.\nRun installlibs.R script install required packages.\nEnsure R RStudio installed.Run installlibs.R script install required packages.Stay Engaged:\nParticipate actively lectures practicals.\nAsk questions share insights sessions.\nParticipate actively lectures practicals.Ask questions share insights sessions.Tip: Bookmark guide easy reference throughout course. 📖","code":""},{"path":"index.html","id":"course-overview","chapter":"1 Welcome!","heading":"1.3 Course Overview 🌍","text":"Species Distribution Models (SDMs) powerful tools understanding species likely occur based environmental variables. play crucial role :Biodiversity conservationEcological researchEnvironmental management","code":""},{"path":"index.html","id":"core-topics-covered","chapter":"1 Welcome!","heading":"1.3.1 Core Topics Covered","text":"Theoretical Framework:\nLearn fundamental principles SDMs, including ecological niche theory predictor variables.\nLearn fundamental principles SDMs, including ecological niche theory predictor variables.Spatial Data Handling:\nGain hands-experience spatial datasets R (e.g., shapefiles, raster data).\nGain hands-experience spatial datasets R (e.g., shapefiles, raster data).Modeling Algorithms:\nExplore different SDM algorithms, :\nMaxEnt: presence-data.\nGeneralized Linear Models (GLM) Random Forests (RF) presence-absence data.\n\nExplore different SDM algorithms, :\nMaxEnt: presence-data.\nGeneralized Linear Models (GLM) Random Forests (RF) presence-absence data.\nMaxEnt: presence-data.Generalized Linear Models (GLM) Random Forests (RF) presence-absence data.Model Evaluation:\nUse metrics like AUC, TSS, Kappa assess model performance.\nUse metrics like AUC, TSS, Kappa assess model performance.Projection & Scenario Analysis:\nPredict species distributions future climate scenarios using ensemble modeling.\nPredict species distributions future climate scenarios using ensemble modeling.","code":""},{"path":"index.html","id":"logistics-and-structure","chapter":"1 Welcome!","heading":"1.4 Logistics and Structure 🗓️","text":"course spans two weeks, week comprising:Lectures (1-2 hours)Discussions (30 minutes)Practical Exercises (2-3 hours)","code":""},{"path":"index.html","id":"daily-schedule-example","chapter":"1 Welcome!","heading":"1.4.1 Daily Schedule Example","text":"","code":""},{"path":"index.html","id":"instructor-background","chapter":"1 Welcome!","heading":"1.5 Instructor Background 👩‍🏫","text":"course instructor brings unique blend expertise aerospace engineering ecological modeling. research focuses :Climate change impactsInvasive species managementHabitat suitability modelingThis interdisciplinary approach ensures well-rounded learning experience, combining technical rigor ecological insight.","code":""},{"path":"index.html","id":"course-objectives","chapter":"1 Welcome!","heading":"1.6 Course Objectives 🎯","text":"end course, able :Understand principles SDMs spatial data analysis.Prepare spatial datasets modeling.Apply various SDM algorithms using R.Evaluate model performance using appropriate metrics.Interpret project model results different scenarios.Advanced learners: Additional modules ensemble modeling climate downscaling available interested -depth exploration.","code":""},{"path":"index.html","id":"practical-exercises","chapter":"1 Welcome!","heading":"1.7 Practical Exercises 💻","text":"course emphasizes hands-learning. Key practical exercises include:","code":""},{"path":"index.html","id":"handling-spatial-data","chapter":"1 Welcome!","heading":"1.7.1 1. Handling Spatial Data","text":"Objective: Prepare spatial data SDM loading, manipulating, visualizing datasets R.Skills Learned:\nLoading shapefiles raster data.\nPerforming spatial operations.\nVisualizing spatial data using ggplot2 leaflet.\nLoading shapefiles raster data.Performing spatial operations.Visualizing spatial data using ggplot2 leaflet.","code":""},{"path":"index.html","id":"fitting-and-evaluating-sdms","chapter":"1 Welcome!","heading":"1.7.2 2. Fitting and Evaluating SDMs","text":"Objective: Apply SDM algorithms evaluate model performance.Skills Learned:\nRunning MaxEnt GLM models.\nEvaluating models using AUC, TSS, Kappa metrics.\nInterpreting model outputs.\nRunning MaxEnt GLM models.Evaluating models using AUC, TSS, Kappa metrics.Interpreting model outputs.Note: Solutions shared course allow self-assessment.","code":""},{"path":"index.html","id":"communication-and-support","chapter":"1 Welcome!","heading":"1.8 Communication and Support 🤝","text":"Active participation key making course. Participants encouraged :Ask questions sessions.Share insights experiences.Collaborate peers group activities discussions.","code":""},{"path":"index.html","id":"support-channels","chapter":"1 Welcome!","heading":"1.8.1 Support Channels","text":"Email Support: Reach instructor queries.Discussion Forum: dedicated forum set ongoing discussions.Office Hours: Weekly office hours one--one support.Reminder: Collaboration fosters better learning, don’t hesitate engage peers! 🤗","code":""},{"path":"index.html","id":"references-and-readings","chapter":"1 Welcome!","heading":"1.9 References and Readings 📚","text":"course references several recent publications best practices SDM (2019-2023). Key references include:Elith & Leathwick (2019) - comprehensive review SDM methods.Phillips et al. (2020) - Guidelines using MaxEnt ecological modeling.papers available shared Google Drive “Best Practices” folder.","code":""},{"path":"index.html","id":"supplementary-readings","chapter":"1 Welcome!","heading":"1.9.1 Supplementary Readings","text":"Additional readings advanced topics, :Ensemble ModelingClimate Change ProjectionsThese provided participants interested exploration.","code":""},{"path":"index.html","id":"final-words","chapter":"1 Welcome!","heading":"1.10 Final Words 🌟","text":"hope find course insightful enjoyable. Let’s embark exciting journey species distribution modeling together!Happy learning! 🚀","code":""},{"path":"introduction.html","id":"introduction","chapter":"2 Introduction","heading":"2 Introduction","text":"","code":""},{"path":"introduction.html","id":"what-is-species-distribution-modeling-sdm","chapter":"2 Introduction","heading":"2.0.1 1. What is Species Distribution Modeling (SDM)?","text":"Definition:\nSpecies Distribution Modeling (SDM) uses statistical models predict species live based environmental conditions. helps us understand relationship species habitats.Key terms know: - Environmental Niche Models: show range conditions species can live. - Habitat Suitability Models: predict areas suitable species.","code":""},{"path":"introduction.html","id":"why-sdms-are-important","chapter":"2 Introduction","heading":"2.0.2 Why SDMs are Important?","text":"SDMs play crucial role many areas:Biodiversity Conservation\nHelps protect endangered species identifying key habitats.\nAids planning conservation zones protected areas. 🌿\nHelps protect endangered species identifying key habitats.Aids planning conservation zones protected areas. 🌿Ecological Research\nImproves understanding species interact environment.\nSupports studies species respond environmental changes, like temperature rise. 🌍\nImproves understanding species interact environment.Supports studies species respond environmental changes, like temperature rise. 🌍Environmental Management\nHelps manage invasive species predicting areas may spread .\nAssists assessing impact climate change ecosystems. 🌦️\nHelps manage invasive species predicting areas may spread .Assists assessing impact climate change ecosystems. 🌦️","code":""},{"path":"introduction.html","id":"applications-of-sdms","chapter":"2 Introduction","heading":"2.0.3 Applications of SDMs","text":"Tip: SDMs just researchers—used real-world decision-making, like planning city expansions protecting forests.","code":""},{"path":"introduction.html","id":"key-concepts-in-sdm","chapter":"2 Introduction","heading":"2.0.4 2. Key Concepts in SDM","text":"","code":""},{"path":"introduction.html","id":"species-data","chapter":"2 Introduction","heading":"2.0.4.1 Species Data 🐾","text":"build SDMs, need species data, shows species observed. two main types species data:Presence-data\nmeans know species seen, don’t know wasn’t seen.\nExample: Using data citizen science platforms like iNaturalist GBIF.\nModels use presence-data:\nMaxEnt (Maximum Entropy Model) – One popular tools presence-SDMs.\nFun Fact: MaxEnt works finding distribution “maximum randomness” still matching observed data!\n\nmeans know species seen, don’t know wasn’t seen.Example: Using data citizen science platforms like iNaturalist GBIF.Models use presence-data:\nMaxEnt (Maximum Entropy Model) – One popular tools presence-SDMs.\nFun Fact: MaxEnt works finding distribution “maximum randomness” still matching observed data!\nMaxEnt (Maximum Entropy Model) – One popular tools presence-SDMs.Fun Fact: MaxEnt works finding distribution “maximum randomness” still matching observed data!Presence-absence data\n, information species seen (presence) wasn’t (absence).\ntype data gives information allows robust modeling.\nModels use presence-absence data:\nGeneralized Linear Models (GLM)\nRandom Forests (RF) – machine learning approach handles complex relationships well.\n\n, information species seen (presence) wasn’t (absence).type data gives information allows robust modeling.Models use presence-absence data:\nGeneralized Linear Models (GLM)\nRandom Forests (RF) – machine learning approach handles complex relationships well.\nGeneralized Linear Models (GLM)Random Forests (RF) – machine learning approach handles complex relationships well.","code":""},{"path":"introduction.html","id":"environmental-covariates","chapter":"2 Introduction","heading":"2.0.4.2 Environmental Covariates 🌍","text":"SDMs, also need environmental data, often called covariates, describe conditions different locations. covariates help explain species might present absent.Common types environmental covariates:Climate data: Temperature, rainfall, humidity influence species can live.\n> Example: Polar bears need cold temperatures survive, cacti thrive dry, hot deserts. 🌵❄️Climate data: Temperature, rainfall, humidity influence species can live.\n> Example: Polar bears need cold temperatures survive, cacti thrive dry, hot deserts. 🌵❄️Soil properties: Nutrient levels, pH, texture critical plant species.\n> know? plants grow specific soil types, like orchids need well-drained soils.Soil properties: Nutrient levels, pH, texture critical plant species.\n> know? plants grow specific soil types, like orchids need well-drained soils.Land cover elevation: Forests, grasslands, mountains create different habitats.\n> Elevation can affect climate — higher areas colder, influences species can survive.Land cover elevation: Forests, grasslands, mountains create different habitats.\n> Elevation can affect climate — higher areas colder, influences species can survive.","code":""},{"path":"introduction.html","id":"correlative-vs.-mechanistic-models","chapter":"2 Introduction","heading":"2.0.4.3 Correlative vs. Mechanistic Models 🔍","text":"SDMs can correlative mechanistic, depending relate species data environment:Correlative Models\nmodels find statistical relationships species found environmental conditions.\nEasy implement don’t explain underlying biological processes.\nExample: MaxEnt, GLM, RF.\nmodels find statistical relationships species found environmental conditions.Easy implement don’t explain underlying biological processes.Example: MaxEnt, GLM, RF.Mechanistic Models\nmodels based biological processes, like species responds temperature food availability.\nrequire detailed biological knowledge data can make better predictions new environments.\nExample: Models simulate species’ energy balance different climates.\nmodels based biological processes, like species responds temperature food availability.require detailed biological knowledge data can make better predictions new environments.Example: Models simulate species’ energy balance different climates.Tip: Correlative models commonly used SDMs require less data easier build.","code":""},{"path":"introduction.html","id":"the-sdm-workflow","chapter":"2 Introduction","heading":"2.0.5 3. The SDM Workflow","text":"Building species distribution model involves several steps:","code":""},{"path":"introduction.html","id":"step-by-step-overview","chapter":"2 Introduction","heading":"2.0.5.1 Step-by-Step Overview","text":"Data Collection 📊\nCollect species occurrence data (presence presence-absence) environmental covariates.\nSources species data: GBIF, iNaturalist, local biodiversity surveys.\nSources environmental data: WorldClim (climate data), SoilGrids (soil properties).\nSources species data: GBIF, iNaturalist, local biodiversity surveys.Sources environmental data: WorldClim (climate data), SoilGrids (soil properties).Data Preparation 🧹\nClean format data can used modeling.\nEnsure species data environmental layers coordinate system.\nHandle missing values align data resolution (e.g., 1 km resolution raster data).\nEnsure species data environmental layers coordinate system.Handle missing values align data resolution (e.g., 1 km resolution raster data).Model Fitting 🔧\nChoose appropriate modeling algorithm fit model.\nPresence-data → Use MaxEnt.\nPresence-absence data → Use GLM Random Forest.\nPresence-data → Use MaxEnt.Presence-absence data → Use GLM Random Forest.Model Evaluation ✅\nEvaluate model’s performance using metrics like:\nAUC (Area Curve): Measures well model distinguishes presence absence.\nTSS (True Skill Statistic): Evaluates model accuracy comparing predictions actual observations.\nAUC (Area Curve): Measures well model distinguishes presence absence.TSS (True Skill Statistic): Evaluates model accuracy comparing predictions actual observations.Model Prediction 🌍\nUse fitted model predict species distributions current future conditions.\nExample: Predict species’ range might shift different climate change scenarios.\nExample: Predict species’ range might shift different climate change scenarios.","code":""},{"path":"introduction.html","id":"common-tools-and-techniques","chapter":"2 Introduction","heading":"2.0.5.2 Common Tools and Techniques","text":"GIS Tools\nGeographic Information System (GIS) tools essential spatial data handling. can use:\nQGIS: free, open-source GIS tool.\nArcGIS: popular commercial GIS software.\nQGIS: free, open-source GIS tool.ArcGIS: popular commercial GIS software.R Packages\nR offers powerful tools SDM. Key packages include:\nraster/terra: handling spatial data (e.g., raster vector files).\ndismo: running SDM algorithms like MaxEnt.\nggplot2: data visualization.\nsf: handling spatial vector data (points, lines, polygons).\nraster/terra: handling spatial data (e.g., raster vector files).dismo: running SDM algorithms like MaxEnt.ggplot2: data visualization.sf: handling spatial vector data (points, lines, polygons).Fun Fact: MaxEnt often preferred presence-data works well even small datasets avoids overfitting applying regularization.","code":""},{"path":"introduction.html","id":"example-workflow-in-r","chapter":"2 Introduction","heading":"2.0.6 Example Workflow in R","text":"’s simple R code snippet illustrate SDM workflow:","code":"\n# Load necessary libraries\nlibrary(raster)\nlibrary(dismo)\n\n# Load environmental data (stack of raster layers)\nenv <- stack(list.files(pattern = \".tif$\", full.names = TRUE))\n\n# Load species occurrence data\nspecies <- read.csv(\"species_occurrence.csv\")\n\n# Fit a MaxEnt model\nmaxent_model <- maxent(env, species)\n\n# Predict species distribution\nprediction <- predict(maxent_model, env)\n\n# Plot the prediction\nplot(prediction)"},{"path":"introduction.html","id":"why-use-r-for-sdm","chapter":"2 Introduction","heading":"2.0.7 4. Why Use R for SDM?","text":"","code":""},{"path":"introduction.html","id":"benefits-of-r","chapter":"2 Introduction","heading":"2.0.7.1 Benefits of R 🚀","text":"R one best tools Species Distribution Modeling (SDM) :Open-source:\ncan download use R free! makes accessible students, researchers, organizations everywhere. 🌎Open-source:\ncan download use R free! makes accessible students, researchers, organizations everywhere. 🌎Widely used ecological research:\nMany ecologists conservationists use R, means plenty resources, guides, research papers available learn .Widely used ecological research:\nMany ecologists conservationists use R, means plenty resources, guides, research papers available learn .Large ecosystem packages:\nR many packages specifically designed spatial data handling ecological modeling. Whether ’s raster data manipulation running complex SDMs, ’s package !Large ecosystem packages:\nR many packages specifically designed spatial data handling ecological modeling. Whether ’s raster data manipulation running complex SDMs, ’s package !Community support:\nGot stuck? problem! R community huge, platforms like Stack Overflow R-bloggers tons answers tutorials.Community support:\nGot stuck? problem! R community huge, platforms like Stack Overflow R-bloggers tons answers tutorials.know?\nworld’s leading biodiversity databases, like GBIF, tutorials workflows written R!","code":""},{"path":"introduction.html","id":"key-r-packages-for-sdm","chapter":"2 Introduction","heading":"2.0.7.2 Key R Packages for SDM 📦","text":"essential R packages SDM:Tip:\nUse terra instead raster faster performance, especially large datasets. syntax slightly different easy learn.","code":""},{"path":"introduction.html","id":"setting-up-r-for-sdm","chapter":"2 Introduction","heading":"2.0.8 5. Setting Up R for SDM","text":"","code":""},{"path":"introduction.html","id":"installing-r-and-rstudio","chapter":"2 Introduction","heading":"2.0.8.1 Installing R and RStudio 🖥️","text":"’s set R environment step--step:Download R:\nGo CRAN website download latest version R operating system (Windows, Mac, Linux).Download R:\nGo CRAN website download latest version R operating system (Windows, Mac, Linux).Download RStudio:\nRStudio IDE (Integrated Development Environment) makes coding R easier. Download RStudio’s website.Download RStudio:\nRStudio IDE (Integrated Development Environment) makes coding R easier. Download RStudio’s website.Install R RStudio:\nFollow installation instructions. done, open RStudio, ’re ready code!Install R RStudio:\nFollow installation instructions. done, open RStudio, ’re ready code!Fun Fact:\nRStudio features like syntax highlighting, built-plotting, project management make favorite among R users.","code":""},{"path":"introduction.html","id":"installing-required-packages","chapter":"2 Introduction","heading":"2.0.8.2 Installing Required Packages 📦","text":"R RStudio installed, need install required packages. can running following code R console:Tip:\nAlways keep packages updated using update.packages() regularly. ensures compatibility latest R version.","code":"\n# Install essential SDM packages\ninstall.packages(c(\"raster\", \"terra\", \"dismo\", \"ggplot2\", \"sf\"))"},{"path":"introduction.html","id":"loading-data-in-r","chapter":"2 Introduction","heading":"2.0.8.3 Loading Data in R 📂","text":"Let’s use simple example load environmental data species occurrence data. dismo package comes built-datasets, don’t need download anything!Example: Loading environmental layers species data","code":"\n# Load necessary libraries\nlibrary(dismo)\nlibrary(terra)\n\n# Load example environmental data (bioclim variables)\nenv <- getData(\"worldclim\", var = \"bio\", res = 10)\n\n# View the environmental layers\nplot(env[[1]])  # Plot the first layer (Bio1: Annual Mean Temperature)\n\n# Load example species occurrence data (Bradypus variegatus)\ndata(bradypus)  # Comes with the dismo package\nhead(bradypus)  # View the first few rows of data\n\n# Plot species occurrences on top of the environmental layer\nplot(env[[1]])\npoints(bradypus, col = \"red\", pch = 20)"},{"path":"introduction.html","id":"explanation","chapter":"2 Introduction","heading":"2.0.8.4 Explanation","text":"Environmental Data\ngetData() function dismo package downloads climate data (WorldClim) 10-minute resolution. example, load 19 bioclimatic variables, temperature precipitation.Environmental Data\ngetData() function dismo package downloads climate data (WorldClim) 10-minute resolution. example, load 19 bioclimatic variables, temperature precipitation.Species Occurrence Data\nbradypus dataset contains occurrence points brown-throated sloth (Bradypus variegatus), species found Central South America.Species Occurrence Data\nbradypus dataset contains occurrence points brown-throated sloth (Bradypus variegatus), species found Central South America.Plotting\nfirst plot environmental layer (annual mean temperature) overlay species occurrence points red.Plotting\nfirst plot environmental layer (annual mean temperature) overlay species occurrence points red.know?\nWorldClim one popular sources global climate data, providing high-resolution layers past, present, future climate conditions!","code":""},{"path":"introduction.html","id":"practical-example-building-a-simple-sdm-in-r","chapter":"2 Introduction","heading":"2.0.9 6. Practical Example: Building a Simple SDM in R 🛠️","text":"section, build simple Species Distribution Model (SDM) using R. ’ll use built-dataset dismo package, don’t need download external files.","code":""},{"path":"introduction.html","id":"dataset-tsuga-canadensis-eastern-hemlock","chapter":"2 Introduction","heading":"2.0.9.1 Dataset: Tsuga canadensis (Eastern Hemlock) 🌲","text":"Species: Tsuga canadensis, tree species native eastern North America.Goal: Predict potential distribution species based environmental conditions.","code":""},{"path":"introduction.html","id":"steps-to-build-the-sdm","chapter":"2 Introduction","heading":"2.0.9.2 Steps to Build the SDM","text":"Load Environmental Layers Occurrence DataFit Basic MaxEnt ModelEvaluate Model Using AUCGenerate Plot Predictions","code":""},{"path":"introduction.html","id":"step-1-load-environmental-layers-and-occurrence-data","chapter":"2 Introduction","heading":"2.0.9.3 Step 1: Load Environmental Layers and Occurrence Data","text":", use WorldClim data, contains 19 bioclimatic variables (like temperature precipitation) 10-minute resolution.Fun Fact:\nWorldClim data widely used ecological studies provides historical, current, future climate layers.","code":"\n# Load required libraries\nlibrary(dismo)\nlibrary(terra)\n\n# Load environmental data (bioclim variables)\nenv <- getData(\"worldclim\", var = \"bio\", res = 10)\n\n# Load species occurrence data (for Tsuga canadensis)\ndata(tsuga)  # Built-in dataset in dismo\nhead(tsuga)  # View first few rows of occurrence data"},{"path":"introduction.html","id":"step-2-fit-a-basic-maxent-model","chapter":"2 Introduction","heading":"2.0.9.4 Step 2: Fit a Basic MaxEnt Model","text":"use maxent() function dismo package fit Maximum Entropy (MaxEnt) model. algorithm ideal presence-data, means need locations species observed.","code":"\n# Fit the MaxEnt model\nmaxent_model <- maxent(env, tsuga)\n\n# View model summary\nsummary(maxent_model)"},{"path":"introduction.html","id":"step-3-evaluate-the-model-using-auc","chapter":"2 Introduction","heading":"2.0.9.5 Step 3: Evaluate the Model Using AUC","text":"AUC (Area Curve) value tells us well model distinguishes suitable unsuitable habitats:AUC > 0.9: Excellent modelAUC 0.7 – 0.9: Good modelAUC < 0.7: Poor modelTip:\nAlways aim AUC 0.7, remember high AUC doesn’t always mean model perfect—might just overfitting!","code":"\n# Evaluate model using AUC (Area Under the Curve)\nevaluate_model <- evaluate(maxent_model, env, tsuga)\nauc_value <- evaluate_model@auc\nprint(paste(\"AUC:\", auc_value))"},{"path":"introduction.html","id":"step-4-generate-and-plot-predictions","chapter":"2 Introduction","heading":"2.0.9.6 Step 4: Generate and Plot Predictions","text":"plot shows predicted distribution Tsuga canadensis based environmental conditions. red points represent locations species observed.","code":"\n# Generate predictions\nprediction <- predict(maxent_model, env)\n\n# Plot the predicted distribution\nplot(prediction, main = \"Predicted Distribution of Tsuga canadensis\")\npoints(tsuga, col = \"red\", pch = 20)"},{"path":"introduction.html","id":"best-practices-in-sdm","chapter":"2 Introduction","heading":"2.0.10 7. Best Practices in SDM ✅","text":"","code":""},{"path":"introduction.html","id":"ensuring-data-quality","chapter":"2 Introduction","heading":"2.0.10.1 1. Ensuring Data Quality 🧐","text":"Quality data foundation good model. ’s ensure data reliable:Avoid Bias Species Occurrence Data\nSampling bias can lead incorrect predictions. example, observations come areas near roads, model might think species lives near roads!\nUse tools like spatial thinning reduce bias.\nSampling bias can lead incorrect predictions. example, observations come areas near roads, model might think species lives near roads!Use tools like spatial thinning reduce bias.Use Appropriate Resolution Environmental Layers\nChoose resolution matches study scale.\nExample: ’re modeling local species, use high-resolution data (e.g., 1 km). global models, 10 km resolution might fine.\nChoose resolution matches study scale.Example: ’re modeling local species, use high-resolution data (e.g., 1 km). global models, 10 km resolution might fine.","code":""},{"path":"introduction.html","id":"model-validation","chapter":"2 Introduction","heading":"2.0.10.2 2. Model Validation 🏅","text":"Model validation helps assess well SDM performs:Cross-validation Techniques\nUse k-fold cross-validation split data training testing sets multiple times. helps ensure model isn’t overfitting.\nUse k-fold cross-validation split data training testing sets multiple times. helps ensure model isn’t overfitting.Independent Test Data\npossible, use completely independent dataset validate model. gives realistic estimate well model performs real world.\npossible, use completely independent dataset validate model. gives realistic estimate well model performs real world.","code":""},{"path":"introduction.html","id":"interpreting-results","chapter":"2 Introduction","heading":"2.0.10.3 3. Interpreting Results 🔍","text":"Interpreting SDM results correctly crucial:Understand Limitations\nSDMs predict potential distributions, actual ones. Just model says species can live somewhere doesn’t mean ’s actually !\nSDMs predict potential distributions, actual ones. Just model says species can live somewhere doesn’t mean ’s actually !Account Uncertainties\nAlways report uncertainties predictions. example, use confidence intervals ensemble models show variation predictions.\nAlways report uncertainties predictions. example, use confidence intervals ensemble models show variation predictions.Know?\nEnsemble models combine predictions multiple algorithms improve accuracy reduce uncertainty.","code":""},{"path":"understanding-spatial-data.html","id":"understanding-spatial-data","chapter":"3 Understanding Spatial Data","heading":"3 Understanding Spatial Data","text":"","code":""},{"path":"understanding-spatial-data.html","id":"introduction-to-spatial-data","chapter":"3 Understanding Spatial Data","heading":"3.0.1 1. Introduction to Spatial Data 🌍","text":"","code":""},{"path":"understanding-spatial-data.html","id":"what-is-spatial-data","chapter":"3 Understanding Spatial Data","heading":"3.0.1.1 What is Spatial Data?","text":"Spatial data data tells us something located Earth. tied specific location often includes information features like:Natural elements: Rivers, forests, mountains.Human-made structures: Roads, cities, bridges.Environmental conditions: Temperature, rainfall, soil type.Think :\nEvery time check weather map search place Google Maps, ’re working spatial data!","code":""},{"path":"understanding-spatial-data.html","id":"why-is-spatial-data-important","chapter":"3 Understanding Spatial Data","heading":"3.0.1.2 Why is Spatial Data Important?","text":"Spatial data helps us understand patterns relationships environment. essential answering questions like:species found?\nHelps map species distributions habitats.\nHelps map species distributions habitats.environmental conditions present?\nexample, ’s warm wet enough specific plants grow.\nexample, ’s warm wet enough specific plants grow.things changing time?\nSpatial data crucial monitoring climate change, urban expansion, deforestation.\nSpatial data crucial monitoring climate change, urban expansion, deforestation.Fun Fact:\nSpatial data used conservationists protect endangered species, city planners design better transport systems, even gamers build realistic maps video games!","code":""},{"path":"understanding-spatial-data.html","id":"real-world-example","chapter":"3 Understanding Spatial Data","heading":"3.0.1.3 Real-World Example","text":"Let’s say want study distribution sloths South America. can combine species occurrence data (locations sloths spotted) environmental data (temperature, forest cover) build Species Distribution Model (SDM). helps us predict sloths might live—even areas haven’t explored yet.","code":""},{"path":"understanding-spatial-data.html","id":"types-of-spatial-data","chapter":"3 Understanding Spatial Data","heading":"3.0.1.4 Types of Spatial Data","text":"Spatial data comes two main forms:Vector Data\nRepresents features clear boundaries, like roads, rivers, city borders.\nThink detailed map showing shapes lines.\nRepresents features clear boundaries, like roads, rivers, city borders.Think detailed map showing shapes lines.Raster Data\nRepresents continuous data, like temperature elevation, grid cells (pixels).\nThink heatmap showing variations across region.\nRepresents continuous data, like temperature elevation, grid cells (pixels).Think heatmap showing variations across region.Let’s Explore :\nnext sections, ’ll dive two types spatial data learn work R. Stay curious! 🐾","code":""},{"path":"understanding-spatial-data.html","id":"types-of-spatial-data-1","chapter":"3 Understanding Spatial Data","heading":"3.0.2 2. Types of Spatial Data 🗺️","text":"Spatial data comes two main types: vector data raster data. unique characteristics used different purposes. Let’s dive !","code":""},{"path":"understanding-spatial-data.html","id":"vector-data","chapter":"3 Understanding Spatial Data","heading":"3.0.2.1 2.1. Vector Data ✏️","text":"","code":""},{"path":"understanding-spatial-data.html","id":"what-is-vector-data","chapter":"3 Understanding Spatial Data","heading":"3.0.2.1.1 What is Vector Data?","text":"Vector data represents features points, lines, polygons. ’s great capturing objects clear boundaries.","code":""},{"path":"understanding-spatial-data.html","id":"components-of-vector-data","chapter":"3 Understanding Spatial Data","heading":"3.0.2.1.2 Components of Vector Data","text":"Points: Represent specific locations, like:\nCities\nTree locations\nObservation points species occurrence.\nCitiesTree locationsObservation points species occurrence.Lines: Represent linear features, :\nRoads\nRivers\nTrails.\nRoadsRiversTrails.Polygons: Represent areas, :\nCountries\nLakes\nForest boundaries.\nCountriesLakesForest boundaries.","code":""},{"path":"understanding-spatial-data.html","id":"attributes-of-vector-data","chapter":"3 Understanding Spatial Data","heading":"3.0.2.1.3 Attributes of Vector Data","text":"vector feature can attributes, provide additional information.\nexample:Fun Fact: shapefile format (.shp) one common ways store vector data, used since 1990s!","code":""},{"path":"understanding-spatial-data.html","id":"vector-data-in-r","chapter":"3 Understanding Spatial Data","heading":"3.0.2.1.4 Vector Data in R","text":"R, use packages like sf terra work vector data. Let’s see example:Tip: Use sf modern workflows. ’s faster flexible older sp package.","code":"\n# Load the 'sf' package\nlibrary(sf)\n#> Linking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2()\n#> is TRUE\n\n# Load a sample shapefile (comes with the 'sf' package)\nnc <- st_read(system.file(\"shape/nc.shp\", package = \"sf\"))\n#> Reading layer `nc' from data source \n#>   `/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/sf/shape/nc.shp' \n#>   using driver `ESRI Shapefile'\n#> Simple feature collection with 100 features and 14 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\n#> Geodetic CRS:  NAD27\n\n# View the first few rows of the data\nhead(nc)\n#> Simple feature collection with 6 features and 14 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -81.74107 ymin: 36.07282 xmax: -75.77316 ymax: 36.58965\n#> Geodetic CRS:  NAD27\n#>    AREA PERIMETER CNTY_ CNTY_ID        NAME  FIPS FIPSNO\n#> 1 0.114     1.442  1825    1825        Ashe 37009  37009\n#> 2 0.061     1.231  1827    1827   Alleghany 37005  37005\n#> 3 0.143     1.630  1828    1828       Surry 37171  37171\n#> 4 0.070     2.968  1831    1831   Currituck 37053  37053\n#> 5 0.153     2.206  1832    1832 Northampton 37131  37131\n#> 6 0.097     1.670  1833    1833    Hertford 37091  37091\n#>   CRESS_ID BIR74 SID74 NWBIR74 BIR79 SID79 NWBIR79\n#> 1        5  1091     1      10  1364     0      19\n#> 2        3   487     0      10   542     3      12\n#> 3       86  3188     5     208  3616     6     260\n#> 4       27   508     1     123   830     2     145\n#> 5       66  1421     9    1066  1606     3    1197\n#> 6       46  1452     7     954  1838     5    1237\n#>                         geometry\n#> 1 MULTIPOLYGON (((-81.47276 3...\n#> 2 MULTIPOLYGON (((-81.23989 3...\n#> 3 MULTIPOLYGON (((-80.45634 3...\n#> 4 MULTIPOLYGON (((-76.00897 3...\n#> 5 MULTIPOLYGON (((-77.21767 3...\n#> 6 MULTIPOLYGON (((-76.74506 3...\n\n# Plot the shapefile\nplot(nc[\"NAME\"], main = \"Counties in North Carolina\")"},{"path":"understanding-spatial-data.html","id":"raster-data","chapter":"3 Understanding Spatial Data","heading":"3.0.2.2 2.2. Raster Data 🖼️","text":"","code":""},{"path":"understanding-spatial-data.html","id":"what-is-raster-data","chapter":"3 Understanding Spatial Data","heading":"3.0.2.2.1 What is Raster Data?","text":"Raster data represents world grid equally sized cells (pixels). cell contains value representing specific property.","code":""},{"path":"understanding-spatial-data.html","id":"examples-of-raster-data","chapter":"3 Understanding Spatial Data","heading":"3.0.2.2.2 Examples of Raster Data","text":"Land cover: Shows different vegetation types urban areas.Elevation: Represents height sea level.Temperature: Shows temperature variations across region.","code":""},{"path":"understanding-spatial-data.html","id":"properties-of-raster-data","chapter":"3 Understanding Spatial Data","heading":"3.0.2.2.3 Properties of Raster Data","text":"Resolution: size cell (e.g., 1 km vs. 10 m).\nHigher resolution = detail (smaller cells).\nHigher resolution = detail (smaller cells).Values: cell holds value:\nExample: cell temperature raster might store 25°C.\nExample: cell temperature raster might store 25°C.","code":""},{"path":"understanding-spatial-data.html","id":"raster-data-in-r","chapter":"3 Understanding Spatial Data","heading":"3.0.2.2.4 Raster Data in R","text":"R, raster terra packages used handle raster data. Let’s explore:","code":"\n# Load the 'raster' package\nlibrary(raster)\n#> Loading required package: sp\n\n# Load a sample raster dataset (comes with the 'raster' package)\nr <- raster(system.file(\"external/test.grd\", package = \"raster\"))\n\n# View basic information about the raster\nprint(r)\n#> class      : RasterLayer \n#> dimensions : 115, 80, 9200  (nrow, ncol, ncell)\n#> resolution : 40, 40  (x, y)\n#> extent     : 178400, 181600, 329400, 334000  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=sterea +lat_0=52.1561605555556 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +datum=WGS84 +units=m +no_defs \n#> source     : test.grd \n#> names      : test \n#> values     : 138.7071, 1736.058  (min, max)\n\n# Plot the raster\nplot(r, main = \"Example Raster\")"},{"path":"understanding-spatial-data.html","id":"types-of-raster-objects","chapter":"3 Understanding Spatial Data","heading":"3.0.2.2.5 Types of Raster Objects","text":"R, different types raster objects:\n1. RasterLayer: single raster layer (e.g., temperature).\n2. RasterStack: collection multiple raster layers (e.g., temperature, precipitation, elevation).\n3. RasterBrick: Similar RasterStack, efficient large datasets.","code":""},{"path":"understanding-spatial-data.html","id":"trade-offs-of-raster-data","chapter":"3 Understanding Spatial Data","heading":"3.0.2.2.6 Trade-offs of Raster Data","text":"Finer Resolution:\nCaptures detail (e.g., 1 m grid cells show small features).\n: Increases file size processing time.\nCaptures detail (e.g., 1 m grid cells show small features).: Increases file size processing time.Coarser Resolution:\nFaster processing.\n: Loses fine details.\nFaster processing.: Loses fine details.Fun Fact: know satellite images, like NASA’s Landsat program, raster data?","code":""},{"path":"understanding-spatial-data.html","id":"key-takeaways","chapter":"3 Understanding Spatial Data","heading":"3.0.3 Key Takeaways","text":"Next, ’ll explore Coordinate Reference Systems (CRS) learn ensure spatial data aligns correctly map. Stay tuned! 🌐","code":""},{"path":"understanding-spatial-data.html","id":"coordinate-reference-systems-crs","chapter":"3 Understanding Spatial Data","heading":"3.0.4 3. Coordinate Reference Systems (CRS) 🌍","text":"Coordinate Reference System (CRS) fundamental working spatial data. ensures data layers align properly map distances, areas, geographic relationships accurately represented.","code":""},{"path":"understanding-spatial-data.html","id":"what-is-a-crs","chapter":"3 Understanding Spatial Data","heading":"3.0.4.1 3.1. What is a CRS?","text":"CRS defines locations Earth’s surface mapped onto flat 2D plane. Since Earth 3D sphere (ellipsoid), CRSs necessary projecting curved surface flat map.Two Main Types CRSAngular Coordinates\nUse latitude (y) longitude (x) represent positions.\nCommonly used global datasets.\nExample: WGS84 (EPSG:4326) standard CRS GPS systems.\nUse latitude (y) longitude (x) represent positions.Commonly used global datasets.Example: WGS84 (EPSG:4326) standard CRS GPS systems.Planar Coordinates\nTransform Earth’s 3D surface 2D map using projections.\nExamples:\nMercator: Great navigation, distorts areas near poles.\nUTM (Universal Transverse Mercator): Divides world zones high-accuracy mapping.\nAlbers Equal-Area: Maintains accurate area measurements, often used environmental studies.\n\nTransform Earth’s 3D surface 2D map using projections.Examples:\nMercator: Great navigation, distorts areas near poles.\nUTM (Universal Transverse Mercator): Divides world zones high-accuracy mapping.\nAlbers Equal-Area: Maintains accurate area measurements, often used environmental studies.\nMercator: Great navigation, distorts areas near poles.UTM (Universal Transverse Mercator): Divides world zones high-accuracy mapping.Albers Equal-Area: Maintains accurate area measurements, often used environmental studies.Know?\nshape Earth perfect sphere; ’s ellipsoid. slight flattening poles affects CRSs designed.","code":""},{"path":"understanding-spatial-data.html","id":"why-crs-matters","chapter":"3 Understanding Spatial Data","heading":"3.0.4.2 3.2. Why CRS Matters","text":"CRS critical spatial datasets different sources often use different coordinate systems. CRSs don’t match, layers won’t align, leading inaccurate results.","code":""},{"path":"understanding-spatial-data.html","id":"key-reasons-why-crs-is-important","chapter":"3 Understanding Spatial Data","heading":"3.0.4.2.1 Key Reasons Why CRS is Important","text":"Alignment\nEnsures datasets different sources overlap correctly map.\nExample: road layer WGS84 might align satellite image UTM reprojected CRS.\nExample: road layer WGS84 might align satellite image UTM reprojected CRS.Accuracy\nPreserves integrity spatial relationships like distance area.\nExample: Calculating area forest wrong CRS result significant errors.\nExample: Calculating area forest wrong CRS result significant errors.","code":""},{"path":"understanding-spatial-data.html","id":"real-world-example-1","chapter":"3 Understanding Spatial Data","heading":"3.0.4.2.2 Real-World Example","text":"Incorrect CRS:two layers won’t overlap different CRSs.Correct CRS:","code":"\nlibrary(sf)\n# Load two layers\nlayer1 <- st_read(system.file(\"shape/nc.shp\", package = \"sf\"))\n#> Reading layer `nc' from data source \n#>   `/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/sf/shape/nc.shp' \n#>   using driver `ESRI Shapefile'\n#> Simple feature collection with 100 features and 14 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\n#> Geodetic CRS:  NAD27\nlayer2 <- st_transform(layer1, crs = 3857)  # Transform to Web Mercator\n\n# Plot misaligned layers\nplot(st_geometry(layer1), col = \"blue\", main = \"Misaligned Layers\")\nplot(st_geometry(layer2), col = \"red\", add = TRUE)\n# Transform both layers to the same CRS\nlayer2 <- st_transform(layer2, crs = st_crs(layer1))\n\n# Plot aligned layers\nplot(st_geometry(layer1), col = \"blue\", main = \"Aligned Layers\")\nplot(st_geometry(layer2), col = \"red\", add = TRUE)"},{"path":"understanding-spatial-data.html","id":"crs-in-r","chapter":"3 Understanding Spatial Data","heading":"3.0.4.3 3.3. CRS in R","text":"R provides powerful tools handle CRSs, primarily sf terra packages.","code":""},{"path":"understanding-spatial-data.html","id":"checking-crs-in-r","chapter":"3 Understanding Spatial Data","heading":"3.0.4.3.1 Checking CRS in R","text":"Use st_crs() check CRS vector object:","code":"\nlibrary(sf)\n# Load a shapefile\nnc <- st_read(system.file(\"shape/nc.shp\", package = \"sf\"))\n#> Reading layer `nc' from data source \n#>   `/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/sf/shape/nc.shp' \n#>   using driver `ESRI Shapefile'\n#> Simple feature collection with 100 features and 14 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\n#> Geodetic CRS:  NAD27\n\n# Check the CRS\nst_crs(nc)\n#> Coordinate Reference System:\n#>   User input: NAD27 \n#>   wkt:\n#> GEOGCRS[\"NAD27\",\n#>     DATUM[\"North American Datum 1927\",\n#>         ELLIPSOID[\"Clarke 1866\",6378206.4,294.978698213898,\n#>             LENGTHUNIT[\"metre\",1]]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     CS[ellipsoidal,2],\n#>         AXIS[\"latitude\",north,\n#>             ORDER[1],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>         AXIS[\"longitude\",east,\n#>             ORDER[2],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     ID[\"EPSG\",4267]]"},{"path":"understanding-spatial-data.html","id":"transforming-crs-in-r","chapter":"3 Understanding Spatial Data","heading":"3.0.4.3.2 Transforming CRS in R","text":"can reproject (transform) spatial object different CRS using st_transform():","code":"\n# Transform to WGS84 (EPSG:4326)\nnc_wgs84 <- st_transform(nc, crs = 4326)\n\n# Verify the new CRS\nst_crs(nc_wgs84)\n#> Coordinate Reference System:\n#>   User input: EPSG:4326 \n#>   wkt:\n#> GEOGCRS[\"WGS 84\",\n#>     ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n#>         MEMBER[\"World Geodetic System 1984 (Transit)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G730)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G873)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G1150)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G1674)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G1762)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G2139)\"],\n#>         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#>             LENGTHUNIT[\"metre\",1]],\n#>         ENSEMBLEACCURACY[2.0]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     CS[ellipsoidal,2],\n#>         AXIS[\"geodetic latitude (Lat)\",north,\n#>             ORDER[1],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>         AXIS[\"geodetic longitude (Lon)\",east,\n#>             ORDER[2],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     USAGE[\n#>         SCOPE[\"Horizontal component of 3D system.\"],\n#>         AREA[\"World.\"],\n#>         BBOX[-90,-180,90,180]],\n#>     ID[\"EPSG\",4326]]"},{"path":"understanding-spatial-data.html","id":"working-with-raster-data","chapter":"3 Understanding Spatial Data","heading":"3.0.4.3.3 Working with Raster Data","text":"raster data, use crs() function terra raster packages:","code":"\nlibrary(terra)\n#> terra 1.8.5\n\n# Load a raster\nr <- rast(system.file(\"ex/elev.tif\", package = \"terra\"))\n\n# Check the CRS\ncrs(r)\n#> [1] \"GEOGCRS[\\\"WGS 84\\\",\\n    ENSEMBLE[\\\"World Geodetic System 1984 ensemble\\\",\\n        MEMBER[\\\"World Geodetic System 1984 (Transit)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G730)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G873)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1150)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1674)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1762)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2139)\\\"],\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ENSEMBLEACCURACY[2.0]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\"\n\n# Transform CRS\nr_transformed <- project(r, \"+proj=utm +zone=33 +datum=WGS84 +units=m\")"},{"path":"understanding-spatial-data.html","id":"common-crs-notations","chapter":"3 Understanding Spatial Data","heading":"3.0.4.4 3.4. Common CRS Notations","text":"PROJ.4 Strings:\nText-based format describing CRS properties. Example:\n+proj=utm +zone=33 +datum=WGS84 +units=m +no_defsPROJ.4 Strings:\nText-based format describing CRS properties. Example:EPSG Codes:\nNumeric identifiers CRSs. Examples:\nEPSG:4326: WGS84, commonly used CRS.\nEPSG:3857: Web Mercator, used Google Maps.\nEPSG Codes:\nNumeric identifiers CRSs. Examples:EPSG:4326: WGS84, commonly used CRS.EPSG:3857: Web Mercator, used Google Maps.","code":"+proj=utm +zone=33 +datum=WGS84 +units=m +no_defs"},{"path":"understanding-spatial-data.html","id":"key-takeaways-1","chapter":"3 Understanding Spatial Data","heading":"3.0.4.5 Key Takeaways","text":"Reminder: Always ensure spatial layers CRS performing analysis.mastering CRS concepts tools, ’ll avoid alignment errors ensure accurate spatial analysis. next section, ’ll dive spatial data tools R! 🚀","code":""},{"path":"understanding-spatial-data.html","id":"spatial-data-tools-in-r","chapter":"3 Understanding Spatial Data","heading":"3.0.5 4. Spatial Data Tools in R 🛠️","text":"section, ’ll explore key tools functions R working spatial data. tools allow us load, manipulate, visualize vector raster data.","code":""},{"path":"understanding-spatial-data.html","id":"vector-data-tools","chapter":"3 Understanding Spatial Data","heading":"3.0.5.1 4.1. Vector Data Tools ✏️","text":"Vector data consists points, lines, polygons represent discrete features map (e.g., cities, rivers, country boundaries).","code":""},{"path":"understanding-spatial-data.html","id":"key-functions-for-vector-data","chapter":"3 Understanding Spatial Data","heading":"3.0.5.1.1 Key Functions for Vector Data","text":"st_read():\nLoads vector data various file formats (e.g., shapefiles, GeoJSON).plot():\nVisualizes vector data.st_transform():\nReprojects vector data different CRS.st_write():\nSaves vector data file (e.g., shapefile GeoJSON).","code":"\n   library(sf)\n   # Load a shapefile of North Carolina counties\n   nc <- st_read(system.file(\"shape/nc.shp\", package = \"sf\"))\n#> Reading layer `nc' from data source \n#>   `/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/sf/shape/nc.shp' \n#>   using driver `ESRI Shapefile'\n#> Simple feature collection with 100 features and 14 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\n#> Geodetic CRS:  NAD27\n# Plot the shapefile\nplot(nc[\"NAME\"], main = \"Counties in North Carolina\")\n# Transform to WGS84\nnc_wgs84 <- st_transform(nc, crs = 4326)\nst_write(nc, \"nc_counties.geojson\", append=TRUE)"},{"path":"understanding-spatial-data.html","id":"common-file-formats-for-vector-data","chapter":"3 Understanding Spatial Data","heading":"3.0.5.1.2 Common File Formats for Vector Data","text":"Know?\nshapefile format introduced Esri early 1990s remains one commonly used vector data formats, despite limitations.","code":""},{"path":"understanding-spatial-data.html","id":"raster-data-tools","chapter":"3 Understanding Spatial Data","heading":"3.0.5.2 4.2. Raster Data Tools 🌄","text":"Raster data represents world grid cells, cell value (e.g., elevation, temperature).","code":""},{"path":"understanding-spatial-data.html","id":"key-functions-for-raster-data","chapter":"3 Understanding Spatial Data","heading":"3.0.5.2.1 Key Functions for Raster Data","text":"raster():\nLoads raster data file.terra::rast():\nalternative raster() terra package, offers better performance.plot():\nVisualizes raster layers.writeRaster():\nSaves raster data file (e.g., GeoTIFF).","code":"\nlibrary(raster)\n# Load a sample raster\nr <- raster(system.file(\"external/test.grd\", package = \"raster\"))\nlibrary(terra)\n# Load the raster using terra but from raster's sample file\n# in your local machine, run this commented line:\n#r_terra <- rast(system.file(\"external/test.grd\", package = \"raster\"))\n# for demonstration, we do this:\nr_terra = rast(\"datasets/sample_raster.tif\")\nplot(r_terra, main = \"Sample Raster Loaded with terra\")\n# Plot the raster\nplot(r, main = \"Example Raster Layer\")\n# Save the raster as a GeoTIFF file\nwriteRaster(r, \"example_raster.tif\", format = \"GTiff\", overwrite=TRUE)"},{"path":"understanding-spatial-data.html","id":"common-file-formats-for-raster-data","chapter":"3 Understanding Spatial Data","heading":"3.0.5.2.2 Common File Formats for Raster Data","text":"Fun Fact:\nGeoTIFF georeferenced version popular TIFF image format. ’s widely used remote sensing GIS applications stores spatial information alongside image data.","code":""},{"path":"understanding-spatial-data.html","id":"key-differences-between-vector-and-raster-tools","chapter":"3 Understanding Spatial Data","heading":"3.0.5.3 Key Differences Between Vector and Raster Tools","text":"","code":""},{"path":"understanding-spatial-data.html","id":"practical-example-combining-vector-and-raster-data","chapter":"3 Understanding Spatial Data","heading":"3.0.5.4 Practical Example: Combining Vector and Raster Data","text":"Let’s say want extract elevation values specific observation points (vector data) digital elevation model (raster data):Tip:\nUse terra package faster processing working large raster datasets.mastering tools, ’ll well-equipped handle wide range spatial data tasks R. Next, ’ll explore spatial analysis techniques, including buffering, overlay operations, spatial joins! 🚀","code":"\nlibrary(sf)\nlibrary(raster)\n\n# Load vector data (polygons)\nnc <- st_read(system.file(\"shape/nc.shp\", package = \"sf\"))\n#> Reading layer `nc' from data source \n#>   `/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/sf/shape/nc.shp' \n#>   using driver `ESRI Shapefile'\n#> Simple feature collection with 100 features and 14 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\n#> Geodetic CRS:  NAD27\n\n# Load the saved raster file\nelev <- raster(\"datasets/elevation_data.grd\")\n\n# Calculate centroids of the polygons\ncentroids <- st_centroid(nc)\n#> Warning: st_centroid assumes attributes are constant over\n#> geometries\n\n# Extract only the X and Y coordinates of the centroids\ncoords <- st_coordinates(centroids)\n\n# Extract elevation values at the centroids\nelevation_values <- extract(elev, coords)\n\n# Add extracted values as a new column\nnc$elevation <- elevation_values\n\n# View the updated data\nhead(nc)\n#> Simple feature collection with 6 features and 15 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -81.74107 ymin: 36.07282 xmax: -75.77316 ymax: 36.58965\n#> Geodetic CRS:  NAD27\n#>    AREA PERIMETER CNTY_ CNTY_ID        NAME  FIPS FIPSNO\n#> 1 0.114     1.442  1825    1825        Ashe 37009  37009\n#> 2 0.061     1.231  1827    1827   Alleghany 37005  37005\n#> 3 0.143     1.630  1828    1828       Surry 37171  37171\n#> 4 0.070     2.968  1831    1831   Currituck 37053  37053\n#> 5 0.153     2.206  1832    1832 Northampton 37131  37131\n#> 6 0.097     1.670  1833    1833    Hertford 37091  37091\n#>   CRESS_ID BIR74 SID74 NWBIR74 BIR79 SID79 NWBIR79\n#> 1        5  1091     1      10  1364     0      19\n#> 2        3   487     0      10   542     3      12\n#> 3       86  3188     5     208  3616     6     260\n#> 4       27   508     1     123   830     2     145\n#> 5       66  1421     9    1066  1606     3    1197\n#> 6       46  1452     7     954  1838     5    1237\n#>                         geometry elevation\n#> 1 MULTIPOLYGON (((-81.47276 3...        NA\n#> 2 MULTIPOLYGON (((-81.23989 3...        NA\n#> 3 MULTIPOLYGON (((-80.45634 3...        NA\n#> 4 MULTIPOLYGON (((-76.00897 3...        NA\n#> 5 MULTIPOLYGON (((-77.21767 3...        NA\n#> 6 MULTIPOLYGON (((-76.74506 3...        NA"},{"path":"understanding-spatial-data.html","id":"practical-considerations","chapter":"3 Understanding Spatial Data","heading":"3.0.6 5. Practical Considerations 🛠️","text":"Working spatial data requires careful attention detail ensure accurate analysis meaningful results. Let’s go important practical considerations using spatial data.","code":""},{"path":"understanding-spatial-data.html","id":"data-alignment","chapter":"3 Understanding Spatial Data","heading":"3.0.6.1 5.1. Data Alignment","text":"combining different spatial datasets, ’s crucial ensure properly aligned. Misaligned data can lead incorrect analysis misleading results.Key Points Data Alignment:CRS\nspatial layers (vector raster) use Coordinate Reference System (CRS).\nExample: vector data uses EPSG:4326 (WGS84), raster data reprojected match CRS.\nspatial layers (vector raster) use Coordinate Reference System (CRS).Example: vector data uses EPSG:4326 (WGS84), raster data reprojected match CRS.Align Raster Layers\nworking multiple raster layers (e.g., temperature, precipitation), ensure resolution, extent, origin.\nensures can used together without errors analysis.\nworking multiple raster layers (e.g., temperature, precipitation), ensure resolution, extent, origin.ensures can used together without errors analysis.","code":"\nlibrary(sf)\nlibrary(terra)\n \n# Load vector data\nnc <- st_read(system.file(\"shape/nc.shp\", package = \"sf\"))\n#> Reading layer `nc' from data source \n#>   `/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/sf/shape/nc.shp' \n#>   using driver `ESRI Shapefile'\n#> Simple feature collection with 100 features and 14 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\n#> Geodetic CRS:  NAD27\n\n# Load raster data\nelev <- rast(\"datasets/elevation_data.grd\")\n   \n# Check CRS of both datasets\nst_crs(nc)  # Vector CRS\n#> Coordinate Reference System:\n#>   User input: NAD27 \n#>   wkt:\n#> GEOGCRS[\"NAD27\",\n#>     DATUM[\"North American Datum 1927\",\n#>         ELLIPSOID[\"Clarke 1866\",6378206.4,294.978698213898,\n#>             LENGTHUNIT[\"metre\",1]]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     CS[ellipsoidal,2],\n#>         AXIS[\"latitude\",north,\n#>             ORDER[1],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>         AXIS[\"longitude\",east,\n#>             ORDER[2],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     ID[\"EPSG\",4267]]\ncrs(elev)   # Raster CRS\n#> [1] \"PROJCRS[\\\"unknown\\\",\\n    BASEGEOGCRS[\\\"unknown\\\",\\n        DATUM[\\\"World Geodetic System 1984\\\",\\n            ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n                LENGTHUNIT[\\\"metre\\\",1]],\\n            ID[\\\"EPSG\\\",6326]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8901]]],\\n    CONVERSION[\\\"unknown\\\",\\n        METHOD[\\\"Oblique Stereographic\\\",\\n            ID[\\\"EPSG\\\",9809]],\\n        PARAMETER[\\\"Latitude of natural origin\\\",52.1561605555556,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8801]],\\n        PARAMETER[\\\"Longitude of natural origin\\\",5.38763888888889,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8802]],\\n        PARAMETER[\\\"Scale factor at natural origin\\\",0.9999079,\\n            SCALEUNIT[\\\"unity\\\",1],\\n            ID[\\\"EPSG\\\",8805]],\\n        PARAMETER[\\\"False easting\\\",155000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8806]],\\n        PARAMETER[\\\"False northing\\\",463000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8807]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"(E)\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]],\\n        AXIS[\\\"(N)\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]]]\"\n   \n# Reproject vector data to match raster CRS\nnc_aligned <- st_transform(nc, crs(elev))"},{"path":"understanding-spatial-data.html","id":"data-quality","chapter":"3 Understanding Spatial Data","heading":"3.0.6.2 5.2. Data Quality","text":"Always check data missing values inconsistencies analysis. Poor data quality leads unreliable models.Steps Ensure Data Quality:Check Missing Values\nRaster data often contains cells NoData values.\ncan use terra check handle missing values:\nRaster data often contains cells NoData values.can use terra check handle missing values:observe now missing observations data.Remove Duplicates Vector Data\nvector data contains duplicate points, can bias analysis.\nUse distinct() dplyr st_as_sf() remove duplicates:\nvector data contains duplicate points, can bias analysis.Use distinct() dplyr st_as_sf() remove duplicates:","code":"\n# Check for NoData cells\nsummary(elev)\n#>       test       \n#>  Min.   : 138.7  \n#>  1st Qu.: 294.0  \n#>  Median : 371.9  \n#>  Mean   : 425.6  \n#>  3rd Qu.: 501.0  \n#>  Max.   :1736.1  \n#>  NA's   :6022\n# Replace NoData cells with 0 (if appropriate)\nelev[is.na(elev)] <- 0\nsummary(elev)\n#>       test       \n#>  Min.   :   0.0  \n#>  1st Qu.:   0.0  \n#>  Median :   0.0  \n#>  Mean   : 147.0  \n#>  3rd Qu.: 302.8  \n#>  Max.   :1736.1\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:terra':\n#> \n#>     intersect, union\n#> The following objects are masked from 'package:raster':\n#> \n#>     intersect, select, union\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nnc <- distinct(nc)"},{"path":"understanding-spatial-data.html","id":"computational-trade-offs","chapter":"3 Understanding Spatial Data","heading":"3.0.6.3 5.3. Computational Trade-offs","text":"High-resolution data provides detail increases computation time memory usage. Choose resolution appropriate study area.Key Considerations:Resolution\nHigh-resolution data (e.g., 1 m cells) great detailed analysis requires significant computational power.\nLow-resolution data (e.g., 10 km cells) faster process may miss important details.\n\nTip: Start lower resolution data prototyping analysis, switch high-resolution data final results.\nHigh-resolution data (e.g., 1 m cells) great detailed analysis requires significant computational power.Low-resolution data (e.g., 10 km cells) faster process may miss important details.Tip: Start lower resolution data prototyping analysis, switch high-resolution data final results.Extent\nLimiting spatial extent data can reduce processing time. ’re interested specific region, crop raster region:\n\n# Crop raster extent vector data\nelev_cropped <- crop(elev, st_bbox(nc))Limiting spatial extent data can reduce processing time. ’re interested specific region, crop raster region:","code":"\n# Crop raster to the extent of vector data\nelev_cropped <- crop(elev, st_bbox(nc))"},{"path":"understanding-spatial-data.html","id":"fun-facts-and-tips","chapter":"3 Understanding Spatial Data","heading":"3.0.7 6. Fun Facts and Tips 🎉","text":"Know?\nRaster data like image grid, pixel stores environmental information, temperature elevation.\nRaster data like image grid, pixel stores environmental information, temperature elevation.Pro Tip:\nUse terra package instead raster faster computations large datasets. ’s designed handle big spatial data efficiently.\nUse terra package instead raster faster computations large datasets. ’s designed handle big spatial data efficiently.Fun Fact:\nMercator projection distorts areas near poles, Greenland looks much larger actually !\nMercator projection distorts areas near poles, Greenland looks much larger actually !","code":""},{"path":"understanding-spatial-data.html","id":"summary-and-key-takeaways","chapter":"3 Understanding Spatial Data","heading":"3.0.8 7. Summary and Key Takeaways 📝","text":"Let’s wrap ’ve learned:Spatial Data Types\nVector data: Points, lines, polygons.\nRaster data: Grids cells values representing continuous data.\nVector data: Points, lines, polygons.Raster data: Grids cells values representing continuous data.CRS Importance\nAlways ensure spatial datasets CRS avoid alignment issues.\nUse functions like st_transform() (vector) project() (raster) reproject data needed.\nAlways ensure spatial datasets CRS avoid alignment issues.Use functions like st_transform() (vector) project() (raster) reproject data needed.R Packages Spatial Data\nUse sf vector data terra raster data.\npackages provide modern, efficient workflows handling spatial data R.\nUse sf vector data terra raster data.packages provide modern, efficient workflows handling spatial data R.concepts mind, ’re well way becoming proficient handling spatial data R! Next, ’ll explore advanced topics, spatial analysis techniques model building. Stay tuned! 🚀","code":""},{"path":"spartial-data-in-r.html","id":"spartial-data-in-r","chapter":"4 Spartial Data in R","heading":"4 Spartial Data in R","text":"","code":""},{"path":"spartial-data-in-r.html","id":"introduction-1","chapter":"4 Spartial Data in R","heading":"4.0.1 1. Introduction","text":"previous section, learned spatial data ’s important understanding species distribution environment. Now, move forward learning actually work data R.part tutorial show step step:create vector data (like points, lines, polygons).make raster layers combine stacks bricks.set change projections data lines correctly map.get real-world data, like climate information species locations, can use build models.following along, ’ll learn basic methods handling vector raster data R. ’ll also get familiar common GIS tasks like reading, writing, transforming spatial data.Let’s start! 🌍","code":""},{"path":"spartial-data-in-r.html","id":"working-with-vector-data-in-r","chapter":"4 Spartial Data in R","heading":"4.1 ### 2. Working with Vector Data in R","text":"Vector data includes points, lines, polygons. section, focus creating point data, converting spatial object, adding attribute data.","code":""},{"path":"spartial-data-in-r.html","id":"creating-point-data","chapter":"4 Spartial Data in R","heading":"4.1.0.1 2.1. Creating Point Data","text":"start, need create set points using longitude latitude values. R, can done using simple vectors.Order matters: Always specify coordinates (longitude, latitude), (latitude, longitude). follows common geographic convention.plot() function gives quick view points.","code":"\n# Creating vectors for longitude and latitude\nlongitude <- c(-116.7, -120.4, -116.7, -113.5, -115.5)\nlatitude <- c(45.3, 42.6, 38.9, 42.1, 35.7)\n\n# Combine into a matrix of coordinates\nlonlat <- cbind(longitude, latitude)\n\n# Plot the points\nplot(lonlat, pch = 19, col = \"blue\", xlab = \"Longitude\", ylab = \"Latitude\", \n     main = \"Plot of Longitude and Latitude Points\")"},{"path":"spartial-data-in-r.html","id":"converting-to-a-spatial-object","chapter":"4 Spartial Data in R","heading":"4.1.0.2 2.2. Converting to a Spatial Object","text":"Although plotted points, yet considered spatial data. work spatial data, need convert points SpatialPoints object.sp package core tool handling vector data R. SpatialPoints objects store spatial information, don’t yet associated attributes.","code":"\n# Load the 'sp' package for spatial data handling\nlibrary(sp)\n\n# Create a SpatialPoints object\npts <- SpatialPoints(lonlat)\n\n# Check the class of the object\nclass(pts)\n#> [1] \"SpatialPoints\"\n#> attr(,\"package\")\n#> [1] \"sp\""},{"path":"spartial-data-in-r.html","id":"assigning-a-coordinate-reference-system-crs","chapter":"4 Spartial Data in R","heading":"4.1.0.3 Assigning a Coordinate Reference System (CRS)","text":"CRS defines spatial data relates locations Earth. Without CRS, points won’t align properly spatial datasets.Always assign CRS working spatial data!\ncases, use \"+proj=longlat +datum=WGS84\" data longitude latitude coordinates.","code":"\n# Assign a CRS to the SpatialPoints object\ncrs_string <- \"+proj=longlat +datum=WGS84\"\npts <- SpatialPoints(lonlat, proj4string = CRS(crs_string))\npts\n#> SpatialPoints:\n#>      longitude latitude\n#> [1,]    -116.7     45.3\n#> [2,]    -120.4     42.6\n#> [3,]    -116.7     38.9\n#> [4,]    -113.5     42.1\n#> [5,]    -115.5     35.7\n#> Coordinate Reference System (CRS) arguments:\n#> +proj=longlat +datum=WGS84 +no_defs"},{"path":"spartial-data-in-r.html","id":"creating-a-spatialpointsdataframe","chapter":"4 Spartial Data in R","heading":"4.1.0.4 2.3. Creating a SpatialPointsDataFrame","text":"SpatialPointsDataFrame combines spatial data attribute data. Let’s add random precipitation values points.","code":"\n# Create a data frame of attribute data\nset.seed(42)  # For reproducibility\nprecipValue <- runif(nrow(lonlat), min = 0, max = 100)  # Random precipitation values\ndf <- data.frame(ID = 1:nrow(lonlat), precip = precipValue)\n\n# Combine the SpatialPoints object with the attribute data\nptsdf <- SpatialPointsDataFrame(pts, data = df)\n\n# View the first few rows of the data\nhead(ptsdf@data)\n#>   ID   precip\n#> 1  1 91.48060\n#> 2  2 93.70754\n#> 3  3 28.61395\n#> 4  4 83.04476\n#> 5  5 64.17455"},{"path":"spartial-data-in-r.html","id":"output-check","chapter":"4 Spartial Data in R","heading":"4.1.0.5 Output Check","text":"can access coordinates attributes separately.Use ptsdf@coords extract spatial coordinates.Use ptsdf@data extract non-spatial attribute data.","code":"\n# Access the spatial coordinates\nptsdf@coords\n#>      longitude latitude\n#> [1,]    -116.7     45.3\n#> [2,]    -120.4     42.6\n#> [3,]    -116.7     38.9\n#> [4,]    -113.5     42.1\n#> [5,]    -115.5     35.7\n# Access the attribute data\nptsdf@data\n#>   ID   precip\n#> 1  1 91.48060\n#> 2  2 93.70754\n#> 3  3 28.61395\n#> 4  4 83.04476\n#> 5  5 64.17455"},{"path":"spartial-data-in-r.html","id":"plotting-the-data","chapter":"4 Spartial Data in R","heading":"4.1.0.6 Plotting the Data","text":"can now visualize points simple plot, coloring based precipitation values.","code":"\n# Plot with color based on precipitation\nplot(ptsdf, pch = 19, col = heat.colors(5)[cut(ptsdf$precip, breaks = 5)],\n     main = \"Spatial Points with Precipitation\", cex = 1.5)\nlegend(\"topright\", legend = c(\"Low\", \"Medium\", \"High\"), fill = heat.colors(5), \n       title = \"Precipitation\")"},{"path":"spartial-data-in-r.html","id":"summary","chapter":"4 Spartial Data in R","heading":"4.1.1 Summary","text":"learned create point data using longitude latitude.converted points spatial object using sp package.added attribute data create SpatialPointsDataFrame.Finally, plotted points precipitation values.","code":""},{"path":"spartial-data-in-r.html","id":"creating-and-manipulating-raster-data-in-r","chapter":"4 Spartial Data in R","heading":"4.1.2 3. Creating and Manipulating Raster Data in R","text":"","code":""},{"path":"spartial-data-in-r.html","id":"creating-a-rasterlayer","chapter":"4 Spartial Data in R","heading":"4.1.2.1 3.1. Creating a RasterLayer","text":"raster represents spatial data grid cells, cell value. useful continuous data like temperature, elevation, land cover.’ll start creating blank raster 10 columns 10 rows, ’ll define extent (geographic area covers).raster 10 columns 10 rows, meaning 100 cells total.extent specifies geographic boundaries:\nxmn = -150 xmx = -80 (longitude range)\nymn = 20 ymx = 60 (latitude range)\nxmn = -150 xmx = -80 (longitude range)ymn = 20 ymx = 60 (latitude range)default Coordinate Reference System (CRS) WGS84, uses longitude latitude.","code":"\n# Load the 'raster' package\nlibrary(raster)\n\n# Create a blank raster with specific extent and resolution\nr <- raster(ncol = 10, nrow = 10, xmx = -80, xmn = -150, ymn = 20, ymx = 60)\n\n# Print raster details\nr\n#> class      : RasterLayer \n#> dimensions : 10, 10, 100  (nrow, ncol, ncell)\n#> resolution : 7, 4  (x, y)\n#> extent     : -150, -80, 20, 60  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=longlat +datum=WGS84 +no_defs"},{"path":"spartial-data-in-r.html","id":"adding-values-to-the-raster","chapter":"4 Spartial Data in R","heading":"4.1.2.2 3.2. Adding Values to the Raster","text":"Now blank raster, can assign values cells. Let’s assign random values 0 100 using runif() function, generates random numbers.ncell() function returns total number cells raster.terrain.colors() function used create nice color gradient plot.","code":"\n# Assign random values to the raster cells\nvalues(r) <- runif(ncell(r), min = 0, max = 100)\n\n# Plot the raster\nplot(r, main = \"Random Values Raster\", col = terrain.colors(10))"},{"path":"spartial-data-in-r.html","id":"creating-a-rasterstack","chapter":"4 Spartial Data in R","heading":"4.1.2.3 3.3. Creating a RasterStack","text":"RasterStack collection multiple rasters extent resolution. useful multiple layers data area, temperature, precipitation, elevation.Let’s create two rasters stack together.rasters stack must resolution extent.can stack many rasters need.","code":"\n# Create two more rasters by performing operations on the original raster\nr2 <- r * 2   # Multiply all values by 2\nr3 <- sqrt(r) # Take the square root of all values\n\n# Create a RasterStack\ns <- stack(r, r2, r3)\n\n# Plot the RasterStack with enhanced visualization\nplot(s, main = c(\"Layer 1: Random Values\", \"Layer 2: Values * 2\", \"Layer 3: Square Root\"),\n     col = terrain.colors(10), nr = 1)"},{"path":"spartial-data-in-r.html","id":"creating-a-rasterbrick","chapter":"4 Spartial Data in R","heading":"4.1.2.4 3.4. Creating a RasterBrick","text":"RasterBrick similar RasterStack, stored efficiently memory. makes faster process working large datasets.Use RasterBrick need better performance memory efficiency.RasterBrick particularly useful dealing large datasets, satellite imagery time-series data.","code":"\n# Create a RasterBrick from the RasterStack\nb <- brick(s)\n\n# Plot the RasterBrick with enhanced visualization\nplot(b, main = c(\"Layer 1: Random Values\", \"Layer 2: Values * 2\", \"Layer 3: Square Root\"),\n     col = heat.colors(10), nr = 1)"},{"path":"spartial-data-in-r.html","id":"key-takeaways-2","chapter":"4 Spartial Data in R","heading":"4.1.2.4.1 Key Takeaways","text":"RasterLayer: grid cells representing spatial data.RasterStack: collection rasters resolution extent.RasterBrick: memory-efficient version RasterStack.","code":""},{"path":"spartial-data-in-r.html","id":"working-with-coordinate-reference-systems-crs","chapter":"4 Spartial Data in R","heading":"4.2 ### 4. Working with Coordinate Reference Systems (CRS)","text":"","code":""},{"path":"spartial-data-in-r.html","id":"assigning-and-transforming-crs","chapter":"4 Spartial Data in R","heading":"4.2.0.1 4.1. Assigning and Transforming CRS","text":"working spatial data, ensuring layers share common CRS crucial proper alignment accurate analysis. Let’s walk assigning CRS transforming new one using vector raster data.","code":""},{"path":"spartial-data-in-r.html","id":"assigning-a-crs-to-vector-data","chapter":"4 Spartial Data in R","heading":"4.2.0.1.1 Assigning a CRS to Vector Data","text":"EPSG code 4326 corresponds WGS84 geographic CRS, commonly used data longitude latitude coordinates.Using st_as_sf() converts data frame spatial object.","code":"\n# Load necessary library\nlibrary(sf)\n#> Linking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2()\n#> is TRUE\n\n# Create sample point data\nlongitude <- c(-116.7, -120.4, -116.7, -113.5, -115.5)\nlatitude <- c(45.3, 42.6, 38.9, 42.1, 35.7)\npts <- data.frame(longitude, latitude)\n\n# Convert to an sf object and assign WGS84 CRS\nsf_pts <- st_as_sf(pts, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n# View CRS of the spatial object\nst_crs(sf_pts)\n#> Coordinate Reference System:\n#>   User input: EPSG:4326 \n#>   wkt:\n#> GEOGCRS[\"WGS 84\",\n#>     ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n#>         MEMBER[\"World Geodetic System 1984 (Transit)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G730)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G873)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G1150)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G1674)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G1762)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G2139)\"],\n#>         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#>             LENGTHUNIT[\"metre\",1]],\n#>         ENSEMBLEACCURACY[2.0]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     CS[ellipsoidal,2],\n#>         AXIS[\"geodetic latitude (Lat)\",north,\n#>             ORDER[1],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>         AXIS[\"geodetic longitude (Lon)\",east,\n#>             ORDER[2],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     USAGE[\n#>         SCOPE[\"Horizontal component of 3D system.\"],\n#>         AREA[\"World.\"],\n#>         BBOX[-90,-180,90,180]],\n#>     ID[\"EPSG\",4326]]"},{"path":"spartial-data-in-r.html","id":"transforming-vector-data-to-a-new-crs","chapter":"4 Spartial Data in R","heading":"4.2.0.1.2 Transforming Vector Data to a New CRS","text":"Let’s transform spatial data WGS84 Robinson projection.Different CRSs can distort distances, areas, angles differently, depending projection method.Always choose CRS fits analysis needs (e.g., UTM local-scale accuracy, Albers Equal-Area area-preserving studies).","code":"\n# Define the new CRS (Robinson projection)\nnew_crs <- \"+proj=robin +datum=WGS84\"\n\n# Transform the spatial points to the new CRS\nsf_pts_transformed <- st_transform(sf_pts, crs = new_crs)\n\n# Plot the original and transformed data side by side\npar(mfrow = c(1, 2))  # Set plotting layout\n\n# Original data in WGS84\nplot(sf_pts, main = \"Original CRS: WGS84\", col = \"blue\", pch = 19)\n\n# Transformed data in Robinson projection\nplot(sf_pts_transformed, main = \"Transformed CRS: Robinson\", col = \"red\", pch = 19)"},{"path":"spartial-data-in-r.html","id":"projecting-raster-data","chapter":"4 Spartial Data in R","heading":"4.2.0.2 4.2. Projecting Raster Data","text":"Projecting raster data involves recalculating cell values adjusting resolution fit new CRS. Let’s demonstrate using raster dataset.","code":""},{"path":"spartial-data-in-r.html","id":"reprojecting-a-raster","chapter":"4 Spartial Data in R","heading":"4.2.0.2.1 Reprojecting a Raster","text":"project() function terra package used raster reprojection.Ensure target CRS defined using PROJ.4 strings EPSG codes.","code":"\n# Load necessary library\nlibrary(terra)\n#> terra 1.8.5\n\n# Create a sample raster\nr <- rast(ncol = 10, nrow = 10, xmin = -150, xmax = -80, ymin = 20, ymax = 60, crs = \"EPSG:4326\")\nvalues(r) <- runif(ncell(r), min = 0, max = 100)  # Assign random values\n\n# Define a new CRS (Lambert Conformal Conic projection)\nnew_crs_raster <- \"+proj=lcc +lat_1=48 +lat_2=33 +lon_0=-100 +datum=WGS84\"\n\n# Project the raster to the new CRS\nr_projected <- project(r, new_crs_raster)\n\n# Plot original and reprojected raster\npar(mfrow = c(1, 2))\n\n# Original raster in WGS84\nplot(r, main = \"Original CRS: WGS84\", col = terrain.colors(10))\n\n# Projected raster in Lambert Conformal Conic\nplot(r_projected, main = \"Projected CRS: LCC\", col = terrain.colors(10))"},{"path":"spartial-data-in-r.html","id":"key-takeaways-3","chapter":"4 Spartial Data in R","heading":"4.2.1 Key Takeaways","text":"Always ensure spatial datasets use CRS performing analysis.Use st_crs() st_transform() vector data crs() project() raster data.Choose appropriate CRSs based scope goals project.","code":""},{"path":"spartial-data-in-r.html","id":"reading-and-writing-spatial-data","chapter":"4 Spartial Data in R","heading":"4.3 ### 5. Reading and Writing Spatial Data","text":"","code":""},{"path":"spartial-data-in-r.html","id":"reading-shapefiles-and-rasters","chapter":"4 Spartial Data in R","heading":"4.3.0.1 5.1. Reading Shapefiles and Rasters","text":"Working spatial data often starts reading external files shapefiles raster datasets. R provides convenient functions load files spatial objects analysis.","code":""},{"path":"spartial-data-in-r.html","id":"reading-a-shapefile","chapter":"4 Spartial Data in R","heading":"4.3.0.1.1 Reading a Shapefile","text":"Let’s read sample shapefile provided raster package.function shapefile() reads vector data shapefile format loads Spatial* object.system.file() function retrieves path sample shapefile provided package.","code":"\n# Load necessary library\nlibrary(raster)\n\n# Read a sample shapefile (comes with the 'raster' package)\nshapefile_path <- system.file(\"external/lux.shp\", package = \"raster\")\nshape_data <- shapefile(shapefile_path)\n\n# Print basic information about the shapefile\nprint(shape_data)\n#> class       : SpatialPolygonsDataFrame \n#> features    : 12 \n#> extent      : 5.74414, 6.528252, 49.44781, 50.18162  (xmin, xmax, ymin, ymax)\n#> crs         : +proj=longlat +datum=WGS84 +no_defs \n#> variables   : 5\n#> names       : ID_1,     NAME_1, ID_2,   NAME_2, AREA \n#> min values  :    1,   Diekirch,    1, Capellen,   76 \n#> max values  :    3, Luxembourg,   12,    Wiltz,  312\n\n# Plot the shapefile\nplot(shape_data, main = \"Sample Shapefile (Luxembourg)\", col = \"lightblue\")"},{"path":"spartial-data-in-r.html","id":"reading-a-raster-file","chapter":"4 Spartial Data in R","heading":"4.3.0.1.2 Reading a Raster File","text":"Now, let’s load sample raster dataset, also provided raster package.function raster() reads raster files loads RasterLayer objects.Use print() display basic metadata, resolution, extent, CRS.","code":"\n# Load a sample raster file (comes with the 'raster' package)\nraster_path <- system.file(\"external/test.grd\", package = \"raster\")\nraster_data <- raster(raster_path)\n\n# Print raster information\nprint(raster_data)\n#> class      : RasterLayer \n#> dimensions : 115, 80, 9200  (nrow, ncol, ncell)\n#> resolution : 40, 40  (x, y)\n#> extent     : 178400, 181600, 329400, 334000  (xmin, xmax, ymin, ymax)\n#> crs        : +proj=sterea +lat_0=52.1561605555556 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +datum=WGS84 +units=m +no_defs \n#> source     : test.grd \n#> names      : test \n#> values     : 138.7071, 1736.058  (min, max)\n# Plot the raster\nplot(raster_data, main = \"Sample Raster Data\", col = terrain.colors(10))"},{"path":"spartial-data-in-r.html","id":"writing-spatial-data-to-disk","chapter":"4 Spartial Data in R","heading":"4.3.0.2 5.2. Writing Spatial Data to Disk","text":"’ve processed spatial data, ’ll often want save disk sharing future use. Let’s see save shapefiles rasters.","code":""},{"path":"spartial-data-in-r.html","id":"saving-a-shapefile","chapter":"4 Spartial Data in R","heading":"4.3.0.2.1 Saving a Shapefile","text":"shapefile() function writes Spatial* object disk shapefile format.Always set overwrite = TRUE want overwrite existing files.","code":"\n# Define output file path\noutput_shapefile <- \"output_shapefile.shp\"\n\n# Save the shapefile to disk\nshapefile(shape_data, filename = output_shapefile, overwrite = TRUE)\n\n# Confirm the file was created\nlist.files(pattern = \"output_shapefile*\")\n#> [1] \"output_shapefile.cpg\" \"output_shapefile.dbf\"\n#> [3] \"output_shapefile.prj\" \"output_shapefile.shp\"\n#> [5] \"output_shapefile.shx\""},{"path":"spartial-data-in-r.html","id":"saving-a-raster","chapter":"4 Spartial Data in R","heading":"4.3.0.2.2 Saving a Raster","text":"Let’s save raster read earlier new file GeoTIFF format.writeRaster() function writes raster data various formats, including GeoTIFF, NetCDF, ASCII Grid.Set format = \"GTiff\" specify file format GeoTIFF.","code":"\n# Define output file path\noutput_raster <- \"output_raster.tif\"\n\n# Save the raster to disk\nwriteRaster(raster_data, filename = output_raster, format = \"GTiff\", overwrite = TRUE)\n\n# Confirm the file was created\nlist.files(pattern = \"output_raster*\")\n#> [1] \"output_raster.tif\""},{"path":"spartial-data-in-r.html","id":"key-takeaways-4","chapter":"4 Spartial Data in R","heading":"4.3.1 Key Takeaways","text":"Use shapefile() read write shapefiles.Use raster() writeRaster() reading saving raster data.system.file() function useful accessing sample datasets bundled R packages.Always check CRS, resolution, extent data ensure proper alignment saving files.","code":""},{"path":"spartial-data-in-r.html","id":"practical-applications-in-sdm","chapter":"4 Spartial Data in R","heading":"4.4 ### 6. Practical Applications in SDM","text":"Species Distribution Modeling (SDM) involves predicting geographic distribution species based environmental data (e.g., climate variables) species occurrence data (e.g., observed locations species). section, ’ll demonstrate :Download environmental data using getData() function dismo package.Retrieve species occurrence data GBIF using gbif() function.","code":""},{"path":"spartial-data-in-r.html","id":"downloading-environmental-data","chapter":"4 Spartial Data in R","heading":"4.4.0.1 6.1. Downloading Environmental Data","text":"Bioclimatic variables commonly used SDM. variables represent different climate characteristics, annual mean temperature, temperature seasonality, annual precipitation.function getData() retrieves bioclimatic variables WorldClim dataset.var = \"bio\" specifies want bioclimatic data, res = 10 sets grid resolution 10 minutes.result, bioclim_data, RasterStack containing multiple environmental layers.","code":"\n# Load necessary library\nlibrary(geodata)\n\n# Specify a directory to save the data\ndata_path <- tempdir()  # Temporary directory for demonstration purposes\n\n# Download bioclimatic variables at 10-minute resolution\nbioclim_data <- worldclim_global(var = \"bio\", res = 10, path = data_path)\n\n# Check the structure of the raster stack\nprint(bioclim_data)\n#> class       : SpatRaster \n#> dimensions  : 1080, 2160, 19  (nrow, ncol, nlyr)\n#> resolution  : 0.1666667, 0.1666667  (x, y)\n#> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\n#> coord. ref. : lon/lat WGS 84 (EPSG:4326) \n#> sources     : wc2.1_10m_bio_1.tif  \n#>               wc2.1_10m_bio_2.tif  \n#>               wc2.1_10m_bio_3.tif  \n#>               ... and 16 more sources\n#> names       : wc2.1~bio_1, wc2.1~bio_2, wc2.1~bio_3, wc2.1~bio_4, wc2.1~bio_5, wc2.1~bio_6, ... \n#> min values  :   -54.72435,     1.00000,    9.131122,       0.000,   -29.68600,   -72.50025, ... \n#> max values  :    30.98764,    21.14754,  100.000000,    2363.846,    48.08275,    26.30000, ...\n# Plot the first bioclimatic variable: Annual Mean Temperature (Bio1)\nplot(bioclim_data[[1]], main = \"Annual Mean Temperature (Bio1)\", col = terrain.colors(10))"},{"path":"spartial-data-in-r.html","id":"key-bioclimatic-variables","chapter":"4 Spartial Data in R","heading":"4.4.0.1.1 Key Bioclimatic Variables","text":"","code":""},{"path":"spartial-data-in-r.html","id":"retrieving-species-occurrence-data","chapter":"4 Spartial Data in R","heading":"4.4.0.2 6.2. Retrieving Species Occurrence Data","text":"Global Biodiversity Information Facility (GBIF) provides open-access data species occurrences worldwide. Let’s retrieve occurrence data African lion (Panthera leo).Data quality check:\nAlways inspect GBIF data missing values incorrect coordinates using models.\nparticular, check NA values longitude (lon) latitude (lat) columns.","code":"\n# Load necessary library\nlibrary(dismo)\n\n# Define a file path to save the data\nfile_path <- \"lion_gbif_data.rds\"\n\n# Check if the data already exists locally\nif (file.exists(file_path)) {\n  # Load the data from the local file\n  lion_data <- readRDS(file_path)\n  message(\"Data loaded from local file.\")\n} else {\n  # Retrieve the data from GBIF and save it locally\n  lion_data <- gbif(genus = \"Panthera\", species = \"leo\")\n  saveRDS(lion_data, file_path)\n  message(\"Data downloaded and saved locally.\")\n}\n#> Data loaded from local file.\n# Subset the first 300 records for demonstration\nlion_data_subset <- lion_data\n\n# to limit the point to view, for example, 3000 points:\n# lion_data_subset <- lion_data[1:3000, ]\n\n# Plot the occurrence data on a world map\nlibrary(maps)\nmap(\"world\", col = \"gray90\", fill = TRUE, bg = \"lightblue\", lwd = 0.5)\npoints(lion_data_subset$lon, lion_data_subset$lat, col = \"red\", pch = 10, cex = 0.7)"},{"path":"spartial-data-in-r.html","id":"summary-and-key-takeaways-1","chapter":"4 Spartial Data in R","heading":"4.4.1 7. Summary and Key Takeaways","text":"tutorial, covered several essential steps working spatial data R, particularly focusing Species Distribution Modeling (SDM).","code":""},{"path":"spartial-data-in-r.html","id":"summary-table","chapter":"4 Spartial Data in R","heading":"4.4.1.1 Summary Table","text":"","code":""},{"path":"spartial-data-in-r.html","id":"key-takeaways-5","chapter":"4 Spartial Data in R","heading":"4.4.2 Key Takeaways","text":"Always ensure spatial datasets share CRS performing analysis.Use reliable sources environmental data, WorldClim, carefully inspect species occurrence data GBIF.Properly handle visualize vector raster data R using functions packages like sf, raster, dismo.","code":""},{"path":"spartial-data-in-r.html","id":"task","chapter":"4 Spartial Data in R","heading":"4.5 Task","text":"Try following tasks test knowledge:","code":""},{"path":"spartial-data-in-r.html","id":"next-steps","chapter":"4 Spartial Data in R","heading":"4.5.1 Next Steps","text":"next part, dive deeper :Advanced spatial analysis techniques:\nBuffering, spatial joins, overlay operations.\nBuffering, spatial joins, overlay operations.Building SDMs:\nUsing machine learning methods (e.g., MaxEnt, Random Forest) predict species distributions.\nUsing machine learning methods (e.g., MaxEnt, Random Forest) predict species distributions.Predictive modeling:\nProjecting species distributions future climate scenarios using environmental datasets.\nProjecting species distributions future climate scenarios using environmental datasets.","code":""},{"path":"key-spatial-concepts-and-terminologies.html","id":"key-spatial-concepts-and-terminologies","chapter":"5 Key Spatial Concepts and Terminologies","heading":"5 Key Spatial Concepts and Terminologies","text":"section, clarify important concepts terminologies appeared throughout tutorial. Understanding terms crucial working effectively spatial data.","code":""},{"path":"key-spatial-concepts-and-terminologies.html","id":"vector-data-1","chapter":"5 Key Spatial Concepts and Terminologies","heading":"5.0.1 1. Vector Data","text":"Definition: Vector data represents geographic features discrete shapes, including:Points: Specific locations (e.g., observation sites cities).Lines: Connected points forming linear features (e.g., roads rivers).Polygons: Closed areas (e.g., country borders lakes).","code":""},{"path":"key-spatial-concepts-and-terminologies.html","id":"raster-data-1","chapter":"5 Key Spatial Concepts and Terminologies","heading":"5.0.2 2. Raster Data","text":"Definition: Raster data represents world grid equally sized cells, cell holds value representing specific attribute (e.g., temperature, elevation).","code":""},{"path":"key-spatial-concepts-and-terminologies.html","id":"coordinate-reference-system-crs","chapter":"5 Key Spatial Concepts and Terminologies","heading":"5.0.3 3. Coordinate Reference System (CRS)","text":"Definition: CRS defines spatial data projected onto flat surface, ensuring different datasets align correctly.CRS matters: Without common CRS, spatial layers align properly, leading inaccurate analysis.","code":""},{"path":"key-spatial-concepts-and-terminologies.html","id":"worldclim-bioclimatic-variables","chapter":"5 Key Spatial Concepts and Terminologies","heading":"5.0.4 4. WorldClim Bioclimatic Variables","text":"Definition: WorldClim provides high-resolution climate data used environmental ecological modeling. bioclimatic variables summarize annual trends, seasonality, extreme limiting environmental factors.","code":""},{"path":"key-spatial-concepts-and-terminologies.html","id":"species-distribution-modeling-sdm","chapter":"5 Key Spatial Concepts and Terminologies","heading":"5.0.5 5. Species Distribution Modeling (SDM)","text":"Definition: SDM method used predict potential distribution species based environmental conditions known occurrence data.","code":""},{"path":"key-spatial-concepts-and-terminologies.html","id":"global-biodiversity-information-facility-gbif","chapter":"5 Key Spatial Concepts and Terminologies","heading":"5.0.6 6. Global Biodiversity Information Facility (GBIF)","text":"Definition: GBIF international network providing access biodiversity data, including species occurrence records around world.","code":""},{"path":"key-spatial-concepts-and-terminologies.html","id":"proj.4-strings","chapter":"5 Key Spatial Concepts and Terminologies","heading":"5.0.7 7. PROJ.4 Strings","text":"Definition: PROJ.4 strings text representations CRS parameters used spatial analysis software.","code":""},{"path":"key-spatial-concepts-and-terminologies.html","id":"shapefiles","chapter":"5 Key Spatial Concepts and Terminologies","heading":"5.0.8 8. Shapefiles","text":"Definition: shapefile popular file format storing vector data. consists multiple files together represent geographic features attributes.","code":""},{"path":"key-spatial-concepts-and-terminologies.html","id":"geotiff","chapter":"5 Key Spatial Concepts and Terminologies","heading":"5.0.9 9. GeoTIFF","text":"Definition: GeoTIFF raster file format stores geographic information along raster data, making suitable spatial analysis.","code":""},{"path":"key-spatial-concepts-and-terminologies.html","id":"key-points-to-remember","chapter":"5 Key Spatial Concepts and Terminologies","heading":"5.0.10 Key Points to Remember","text":"Always check CRS spatial data performing analysis.Use appropriate vector raster data types depending whether working discrete features (e.g., cities, roads) continuous surfaces (e.g., temperature, elevation).downloading large datasets (e.g., GBIF WorldClim), always save locally avoid repeated downloads.feel terms need explained expanded upon, feel free let know! 🚀","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"steps-in-species-distribution-modeling-sdm","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6 Steps in Species Distribution Modeling (SDM)","text":"section, break five critical steps building Species Distribution Model (SDM). step unique goals, methodologies, challenges guide understanding predicting species distributions.— *****\n## Step 1: Conceptualization\nObjective: Define research question identify biological environmental data needs.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"key-points-to-consider","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.0.1 Key Points to Consider","text":"","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"research-design","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.0.1.1 1. Research Design","text":"well-thought-research design ensures SDM focuses species factors matter . Ask :species studying?\nfocusing rare endangered species?\nspecies ecological economic importance?\nknown relationships species specific environmental variables?\nfocusing rare endangered species?species ecological economic importance?known relationships species specific environmental variables?environmental factors (predictors) likely influence distribution?\nConsider variables directly indirectly affect species:\nDirect factors: Temperature, precipitation, soil type.\nIndirect factors: Elevation, aspect, vegetation cover.\n\nIdentify predictors based ecological theory prior studies.\nConsider variables directly indirectly affect species:\nDirect factors: Temperature, precipitation, soil type.\nIndirect factors: Elevation, aspect, vegetation cover.\nDirect factors: Temperature, precipitation, soil type.Indirect factors: Elevation, aspect, vegetation cover.Identify predictors based ecological theory prior studies.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"data-sources","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.0.1.2 2. Data Sources","text":"ensure data quality relevance, rely combination biological environmental data sources:Biological Data Sources:\nField Observations:\nUse surveys, citizen science data (e.g., iNaturalist, eBird), camera trap records.\nEnsure data georeferenced accurate longitude latitude.\n\nHistorical Records:\nExtract data herbarium specimens, museum records, published literature.\n\nField Observations:\nUse surveys, citizen science data (e.g., iNaturalist, eBird), camera trap records.\nEnsure data georeferenced accurate longitude latitude.\nUse surveys, citizen science data (e.g., iNaturalist, eBird), camera trap records.Ensure data georeferenced accurate longitude latitude.Historical Records:\nExtract data herbarium specimens, museum records, published literature.\nExtract data herbarium specimens, museum records, published literature.Environmental Data Sources:\nRemote Sensing Datasets:\nGather variables like land cover, vegetation indices (e.g., NDVI), soil moisture satellites like MODIS Landsat.\n\nGlobal Climate Datasets:\nUse datasets like WorldClim, CHELSA, CMIP6 extract temperature, precipitation, bioclimatic variables.\n\nRemote Sensing Datasets:\nGather variables like land cover, vegetation indices (e.g., NDVI), soil moisture satellites like MODIS Landsat.\nGather variables like land cover, vegetation indices (e.g., NDVI), soil moisture satellites like MODIS Landsat.Global Climate Datasets:\nUse datasets like WorldClim, CHELSA, CMIP6 extract temperature, precipitation, bioclimatic variables.\nUse datasets like WorldClim, CHELSA, CMIP6 extract temperature, precipitation, bioclimatic variables.Example Sources:\n- Biological data: GBIF occurrence records.\n- Environmental data: WorldClim climate variables (e.g., annual mean temperature, precipitation).","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"assumptions","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.0.1.3 3. Assumptions","text":"Every SDM built ecological statistical assumptions. Identifying addressing assumptions ensures model reliability:Ecological Assumptions:\nspecies equilibrium environment (.e., distribution reflects ecological preferences).\nenvironmental variables included model sufficient explain distribution.\nspecies equilibrium environment (.e., distribution reflects ecological preferences).environmental variables included model sufficient explain distribution.Data Assumptions:\nAbsence data (available) reflects true absences rather sampling gaps.\nPresence-data (p-o) overly biased sampling effort geographic bias.\nAbsence data (available) reflects true absences rather sampling gaps.Presence-data (p-o) overly biased sampling effort geographic bias.Modeling Assumptions:\nchosen algorithm can capture species-environment relationships effectively.\nPredictors independent (e.g., collinearity).\nchosen algorithm can capture species-environment relationships effectively.Predictors independent (e.g., collinearity).","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"practical-example-eastern-hemlock-tsuga-canadensis","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.0.2 Practical Example: Eastern Hemlock (Tsuga canadensis)","text":"Let’s conceptualize SDM eastern hemlock, late-successional conifer North America.Research Design:\nQuestion: climate change affect future distribution Tsuga canadensis?\nPredictors: Key variables might include:\nClimate: Annual mean temperature (Bio1), precipitation seasonality (Bio15).\nTopography: Elevation, slope, aspect.\nSoil Characteristics: Drainage class, organic content.\n\nQuestion: climate change affect future distribution Tsuga canadensis?Predictors: Key variables might include:\nClimate: Annual mean temperature (Bio1), precipitation seasonality (Bio15).\nTopography: Elevation, slope, aspect.\nSoil Characteristics: Drainage class, organic content.\nClimate: Annual mean temperature (Bio1), precipitation seasonality (Bio15).Topography: Elevation, slope, aspect.Soil Characteristics: Drainage class, organic content.Data Sources:\nBiological data: Combine presence-absence data field surveys occurrence records GBIF.\nEnvironmental data: Use WorldClim bioclimatic variables digital elevation model (DEM).\nBiological data: Combine presence-absence data field surveys occurrence records GBIF.Environmental data: Use WorldClim bioclimatic variables digital elevation model (DEM).Assumptions:\ndistribution Tsuga canadensis primarily limited climate soil factors.\nPresence records accurately represent locations species occurs current climate conditions.\ndistribution Tsuga canadensis primarily limited climate soil factors.Presence records accurately represent locations species occurs current climate conditions.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"why-conceptualization-matters","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.0.3 Why Conceptualization Matters","text":"Proper conceptualization ensures SDM focuses biologically meaningful relationships avoids common pitfalls like using irrelevant predictors biased occurrence data. lays groundwork robust, interpretable models can inform conservation management strategies.Pro Tip: Use literature review identify relevant predictors validate assumptions. saves time ensures model grounded existing ecological knowledge.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"step-2-data-preparation","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.1 Step 2: Data Preparation","text":"Objective: Gather, clean, process biological environmental data ensure ready modeling.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"data-preparation-is-a-critical-step-in-species-distribution-modeling-sdm.-the-quality-of-your-input-data-significantly-influences-the-accuracy-and-reliability-of-the-resulting-model.-this-step-involves-cleaning-and-processing-both-biological-species-occurrence-data-and-environmental-predictors.","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.2 Data preparation is a critical step in Species Distribution Modeling (SDM). The quality of your input data significantly influences the accuracy and reliability of the resulting model. This step involves cleaning and processing both biological (species occurrence) data and environmental predictors.","text":"","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"key-tasks","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.2.1 Key Tasks","text":"","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"preparing-biological-data","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.2.1.1 1. Preparing Biological Data","text":"Biological data forms basis understanding species distributions. can come two main formats:Presence-(p-o): Records species observed information absence (e.g., citizen science data GBIF).Presence-Absence (p-): Data explicitly indicating locations species present absent (e.g., survey data).Steps Cleaning Biological Data: - Remove Duplicates: - Ensure duplicate records, especially presence data.\n- Duplicates can -represent certain locations bias model. - Fix Erroneous Coordinates: - Check invalid missing latitude/longitude values. - Remove records extreme outliers (e.g., points plotted ocean terrestrial species). - Address Spatial Bias: - Sampling effort can vary across regions. Use techniques like spatial thinning reduce -representation areas high sampling intensity.Tool Spotlight: Use packages like CoordinateCleaner R identify clean problematic occurrence records, duplicates points impossible locations.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"preparing-environmental-data","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.2.1.2 2. Preparing Environmental Data","text":"Environmental data consists predictors describe abiotic biotic conditions influencing species distributions. predictors typically derived :Climate Variables: Temperature, precipitation, seasonality (e.g., WorldClim CMIP6).Topographic Variables: Elevation, slope, aspect (e.g., DEMs).Land Cover: Vegetation type, NDVI (e.g., MODIS ESA datasets).Key Considerations: - Alignment: - Ensure raster layers (predictors) : - Resolution: raster cell represent area (e.g., 10 km × 10 km).\n- Extent: rasters cover geographic boundaries.\n- CRS: Use coordinate reference system (e.g., WGS84).\n- Quality Check: - Inspect rasters missing unrealistic values (e.g., negative precipitation).Alignment Matters: Mismatched resolution, extent, CRS can lead errors spatial analysis, misaligned layers invalid predictions.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"scaling-and-temporal-matching","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.2.1.3 3. Scaling and Temporal Matching","text":"Temporal spatial consistency biological environmental data essential robust modeling:Temporal Alignment:\nBiological data (e.g., species occurrences) environmental data (e.g., climate variables) reflect time period.\nExample: occurrence data 2020, use climate data year comparable period.\nBiological data (e.g., species occurrences) environmental data (e.g., climate variables) reflect time period.Example: occurrence data 2020, use climate data year comparable period.Scaling:\nStandardize numerical predictors (e.g., z-scores min-max scaling) ensure variables contribute equally model.\nStandardize numerical predictors (e.g., z-scores min-max scaling) ensure variables contribute equally model.Watch : - Predictors different temporal scales (e.g., historical vs. future climate data).\n- Variables units aren’t comparable (e.g., temperature °C vs. precipitation mm).","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"pro-tip-checking-multicollinearity","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.2.2 Pro Tip: Checking Multicollinearity","text":"Multicollinearity occurs predictors highly correlated, can distort model’s ability attribute importance variables. Common examples include: - Annual mean temperature (Bio1) maximum temperature warmest month (Bio5). - Annual precipitation (Bio12) precipitation seasonality (Bio15).Address : - Calculate correlation matrix predictors remove one variable highly correlated pair (e.g., correlation > 0.7). - Use dimensionality reduction techniques like Principal Component Analysis (PCA) summarize predictors uncorrelated components.Tool Spotlight: Use vif() function car package identify predictors high variance inflation factors (VIF), indicate multicollinearity.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"example-preparing-data-for-eastern-hemlock-sdm","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.2.3 Example: Preparing Data for Eastern Hemlock SDM","text":"model distribution eastern hemlock (Tsuga canadensis), :Biological Data:\nDownload occurrence records GBIF.\nRemove duplicates points missing invalid coordinates.\nThin data spatially reduce sampling bias.\nDownload occurrence records GBIF.Remove duplicates points missing invalid coordinates.Thin data spatially reduce sampling bias.Environmental Data:\nDownload bioclimatic variables (e.g., Bio1: Annual Mean Temperature, Bio12: Annual Precipitation) WorldClim.\nAlign rasters common resolution 10 km × 10 km, WGS84 CRS.\nDownload bioclimatic variables (e.g., Bio1: Annual Mean Temperature, Bio12: Annual Precipitation) WorldClim.Align rasters common resolution 10 km × 10 km, WGS84 CRS.Scaling Matching:\nStandardize climate variables (e.g., z-scores).\nEnsure climate data corresponds year occurrence data.\nStandardize climate variables (e.g., z-scores).Ensure climate data corresponds year occurrence data.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"why-data-preparation-matters","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.2.4 Why Data Preparation Matters","text":"High-quality data backbone SDM. Poorly prepared data can lead : - Biased inaccurate predictions. - Overfitting, model learns noise rather meaningful patterns. - Misleading conservation decisions based faulty models.Checklist Data Preparation: 1. Biological Data: - Remove duplicates erroneous coordinates. - Address spatial bias occurrence data. 2. Environmental Data: - Align predictors (resolution, extent, CRS). - Standardize numerical variables. 3. Temporal Matching: - Ensure biological environmental data reflect time period. 4. Multicollinearity Check: - Remove reduce highly correlated predictors.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"step-3-model-fitting","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.3 Step 3: Model Fitting","text":"Objective: Select apply appropriate modeling algorithm fit SDM, ensuring model captures meaningful species-environment relationships avoiding overfitting.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"model-fitting-is-the-core-step-in-sdm-where-the-relationship-between-species-occurrence-and-environmental-predictors-is-quantified.-the-choice-of-algorithm-and-careful-selection-of-predictors-are-critical-to-building-an-accurate-and-interpretable-model.","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.4 Model fitting is the core step in SDM, where the relationship between species occurrence and environmental predictors is quantified. The choice of algorithm and careful selection of predictors are critical to building an accurate and interpretable model.","text":"","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"key-considerations","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.4.1 Key Considerations","text":"","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"algorithm-selection","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.4.1.1 1. Algorithm Selection","text":"Different algorithms suitable different data types modeling goals. Choose algorithm based biological data type (e.g., presence-presence-absence) complexity study.Pro Tip: Start simple model (e.g., GLM) establish baseline performance explore complex algorithms like RF BRT improved accuracy.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"variable-selection","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.4.1.2 2. Variable Selection","text":"Careful selection predictors ensures model captures meaningful relationships without overfitting. Multicollinearity (high correlation predictors) can distort model’s interpretation reliability.Steps Variable Selection: 1. Check Multicollinearity: - Calculate correlation matrix Variance Inflation Factor (VIF) predictors. - Remove one variable pair correlation > 0.7 (e.g., Bio1 Bio5 climate data).Prioritize Ecological Relevance:\nRetain variables biologically relevant species’ niche.\nExample: eastern hemlock (Tsuga canadensis), focus predictors like temperature seasonality (Bio4) annual precipitation (Bio12).\nRetain variables biologically relevant species’ niche.Example: eastern hemlock (Tsuga canadensis), focus predictors like temperature seasonality (Bio4) annual precipitation (Bio12).Iterative Refinement:\nTest model’s performance different sets predictors.\nUse techniques like backward elimination stepwise selection identify best set.\nTest model’s performance different sets predictors.Use techniques like backward elimination stepwise selection identify best set.Example: 19 bioclimatic variables WorldClim, select subset (e.g., 4–6 variables) based correlation ecological significance.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"overfitting-avoidance","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.4.1.3 3. Overfitting Avoidance","text":"Overfitting occurs model becomes complex performs well training data poorly unseen data. Avoid overfitting using following strategies:Regularization:\nMaxEnt, adjust regularization multiplier penalize overly complex models.\nMaxEnt, adjust regularization multiplier penalize overly complex models.Cross-Validation:\nSplit data training testing subsets (e.g., 70% training, 30% testing).\nUse k-fold cross-validation evaluate model multiple splits data.\nSplit data training testing subsets (e.g., 70% training, 30% testing).Use k-fold cross-validation evaluate model multiple splits data.Simplify Model:\nAvoid using many predictors, especially sample size small.\nExample: dataset 100 species occurrence records, limit predictors ~5–6 variables.\nAvoid using many predictors, especially sample size small.Example: dataset 100 species occurrence records, limit predictors ~5–6 variables.Watch : - Using available predictors without assessing collinearity. - Relying solely training accuracy without testing model independent data.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"example-fitting-a-model-for-eastern-hemlock","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.4.2 Example: Fitting a Model for Eastern Hemlock","text":"eastern hemlock (Tsuga canadensis), aim understand environmental factors influence distribution:Data:\nPresence-absence records species across North America.\nBioclimatic predictors (e.g., Bio4: Temperature Seasonality, Bio12: Annual Precipitation).\nPresence-absence records species across North America.Bioclimatic predictors (e.g., Bio4: Temperature Seasonality, Bio12: Annual Precipitation).Algorithm:\nUse Random Forest (RF) due ability handle complex, nonlinear relationships.\nUse Random Forest (RF) due ability handle complex, nonlinear relationships.Variable Selection:\nStart 19 bioclimatic variables.\nRemove highly correlated variables (e.g., Bio1 Bio5).\nRetain key predictors based ecological knowledge.\nStart 19 bioclimatic variables.Remove highly correlated variables (e.g., Bio1 Bio5).Retain key predictors based ecological knowledge.Cross-Validation:\nPerform 5-fold cross-validation assess model performance.\nEvaluate metrics like AUC (Area Curve) discrimination ability.\nPerform 5-fold cross-validation assess model performance.Evaluate metrics like AUC (Area Curve) discrimination ability.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"key-metrics-for-model-assessment","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.4.3 Key Metrics for Model Assessment","text":"model fitting, evaluate performance using appropriate metrics:Best Practice: Use combination metrics (e.g., AUC + TSS) evaluate accuracy ecological validity model.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"final-thoughts-on-model-fitting","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.4.4 Final Thoughts on Model Fitting","text":"goal model fitting create balance simplicity accuracy. good SDM: 1. Captures biologically meaningful relationships species environment. 2. Avoids overfitting maintaining high predictive power. 3. Uses well-documented reproducible methodology variable selection model evaluation.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"step-4-model-evaluation","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.5 Step 4: Model Evaluation","text":"Objective: Assess model’s accuracy, predictive performance, ecological validity ensure robust reliable species distribution predictions.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"why-evaluate-the-model","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.5.1 Why Evaluate the Model?","text":"Model evaluation critical step verify: 1. Predictive Power: well model generalize unseen data? 2. Ecological Relevance: relationships species environment biologically meaningful? 3. Model Limitations: Identify overfitting potential biases.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"evaluation-metrics","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.5.2 Evaluation Metrics","text":"Model evaluation metrics help quantify predictive performance SDM. Use combination metrics assess discrimination ability agreement predictions observations.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"validation-techniques","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.5.3 Validation Techniques","text":"good SDM validated using appropriate techniques assess performance independent data ecological soundness.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"traintest-split","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.5.3.1 1. Train/Test Split","text":"Method:\nSplit data training (e.g., 70%) testing (e.g., 30%) subsets.\nTrain model one subset evaluate performance .\nSplit data training (e.g., 70%) testing (e.g., 30%) subsets.Train model one subset evaluate performance .Purpose:\nEnsures model overfitting can generalize unseen data.\nEnsures model overfitting can generalize unseen data.Best Practice: Repeat train/test split multiple times different random seeds average evaluation metrics reduce variability.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"cross-validation","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.5.3.2 2. Cross-Validation","text":"Method:\nUse k-fold cross-validation, dataset split k folds (e.g., 5 10).\nTrain model k-1 folds evaluate remaining fold. Repeat process k times.\nUse k-fold cross-validation, dataset split k folds (e.g., 5 10).Train model k-1 folds evaluate remaining fold. Repeat process k times.Purpose:\nProvides robust estimate model accuracy, especially small datasets.\nProvides robust estimate model accuracy, especially small datasets.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"ecological-plausibility","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.5.3.3 3. Ecological Plausibility","text":"Steps:\nExamine fitted relationships environmental predictors species presence.\nValidate whether results align biological knowledge species.\nExamine fitted relationships environmental predictors species presence.Validate whether results align biological knowledge species.Example:\neastern hemlock, ensure higher probabilities presence associated cool, damp climates lower temperatures.\neastern hemlock, ensure higher probabilities presence associated cool, damp climates lower temperatures.Watch : - Counterintuitive relationships (e.g., predicting higher probabilities unsuitable environments). - -reliance predictors lack ecological significance.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"example-evaluating-an-sdm-for-eastern-hemlock","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.5.4 Example: Evaluating an SDM for Eastern Hemlock","text":"Let’s evaluate fitted SDM eastern hemlock:Metrics:\nCalculate AUC TSS test dataset.\nUse Kappa assess agreement observed predicted distributions.\nCalculate AUC TSS test dataset.Use Kappa assess agreement observed predicted distributions.Validation:\nVisualize fitted response curves key predictors (e.g., Bio1: Annual Mean Temperature).\nCheck areas predicted suitable match known hemlock habitats.\nVisualize fitted response curves key predictors (e.g., Bio1: Annual Mean Temperature).Check areas predicted suitable match known hemlock habitats.Ecological Validation:\nVerify predicted distributions correspond known locations cool, damp regions.\nVerify predicted distributions correspond known locations cool, damp regions.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"key-takeaways-6","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.5.5 Key Takeaways","text":"Use multiple metrics (e.g., AUC, TSS, Kappa) comprehensive evaluation.Train/test splits cross-validation essential estimating predictive accuracy.Validate ecological plausibility ensure biological relevance.Pro Tip: Always interpret model results context ecological knowledge. high AUC score alone doesn’t guarantee biologically meaningful model.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"step-5-model-prediction-and-projection","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.6 Step 5: Model Prediction and Projection","text":"Objective: Leverage fitted model predict species distributions current conditions project potential changes future scenarios, including climate change.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"key-applications","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.6.1 Key Applications","text":"Species Distribution Models (SDMs) can used various predictive projection-based tasks:","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"current-range-prediction","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.6.1.1 1. Current Range Prediction","text":"Purpose:\nUse environmental predictors estimate species’ potential distribution current conditions.\nGenerate suitability maps highlight areas species likely occur.\nUse environmental predictors estimate species’ potential distribution current conditions.Generate suitability maps highlight areas species likely occur.Example:\neastern hemlock (Tsuga canadensis), map current habitat suitability based predictors like temperature, precipitation, soil type.\neastern hemlock (Tsuga canadensis), map current habitat suitability based predictors like temperature, precipitation, soil type.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"future-projections","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.6.1.2 2. Future Projections","text":"Purpose:\nAssess species distributions might shift future climate conditions.\nUse global climate models (GCMs) future emission scenarios (e.g., Shared Socioeconomic Pathways (SSPs) CMIP6 datasets).\nAssess species distributions might shift future climate conditions.Use global climate models (GCMs) future emission scenarios (e.g., Shared Socioeconomic Pathways (SSPs) CMIP6 datasets).Method:\nReplace current climate predictors future climate layers (e.g., projected temperature precipitation 2050 2100).\nExplore multiple scenarios, low emissions (SSP1-2.6) high emissions (SSP5-8.5), understand range possible outcomes.\nReplace current climate predictors future climate layers (e.g., projected temperature precipitation 2050 2100).Explore multiple scenarios, low emissions (SSP1-2.6) high emissions (SSP5-8.5), understand range possible outcomes.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"uncertainty-quantification","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.6.2 Uncertainty Quantification","text":"Predicting species distributions involves several sources uncertainty. ’s essential quantify communicate uncertainties reliable decision-making.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"ensemble-modeling","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.6.2.1 1. Ensemble Modeling","text":"Approach:\nUse projections multiple models (e.g., several GCMs SDM algorithms).\nGenerate ensemble prediction averaging results across models.\nUse projections multiple models (e.g., several GCMs SDM algorithms).Generate ensemble prediction averaging results across models.Benefits:\nReduces reliance single model, accounting variability projections.\nReduces reliance single model, accounting variability projections.Example:\nCombine habitat suitability maps eastern hemlock several GCMs identify areas high agreement.\nCombine habitat suitability maps eastern hemlock several GCMs identify areas high agreement.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"scenario-comparisons","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.6.2.2 2. Scenario Comparisons","text":"Approach:\nCompare projections across different future scenarios (e.g., SSP1-2.6 vs. SSP5-8.5).\nHighlight areas predictions differ significantly.\nCompare projections across different future scenarios (e.g., SSP1-2.6 vs. SSP5-8.5).Highlight areas predictions differ significantly.Benefits:\nProvides range possible outcomes, helping policymakers prepare varying conditions.\nProvides range possible outcomes, helping policymakers prepare varying conditions.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"confidence-intervals-and-uncertainty-maps","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.6.2.3 3. Confidence Intervals and Uncertainty Maps","text":"Approach:\nCalculate confidence intervals predicted suitability values.\nGenerate uncertainty maps showing areas higher prediction variability.\nCalculate confidence intervals predicted suitability values.Generate uncertainty maps showing areas higher prediction variability.Benefits:\nIdentifies regions predictions less reliable, aiding risk assessment.\nIdentifies regions predictions less reliable, aiding risk assessment.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"example-projecting-eastern-hemlock-distribution","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.6.3 Example: Projecting Eastern Hemlock Distribution","text":"eastern hemlock, future climate warming might lead : 1. Habitat Loss: - Warmer temperatures may reduce habitat suitability southern regions.Range Shifts:\nSuitable habitats might shift higher elevations farther north climate changes.\nSuitable habitats might shift higher elevations farther north climate changes.Steps: - Replace current bioclimatic variables projections 2050 2100 CMIP6 datasets. - Run model using different SSP scenarios (e.g., SSP1-2.6 SSP5-8.5). - Map areas predicted remain suitable, gain suitability, lose suitability.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"key-considerations-for-projections","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.6.4 Key Considerations for Projections","text":"","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"key-takeaways-7","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.6.5 Key Takeaways","text":"Predict Current Distributions:\nUse current environmental data identify suitable habitats.\nUse current environmental data identify suitable habitats.Project Future Ranges:\nIncorporate climate projections assess potential impacts climate change.\nIncorporate climate projections assess potential impacts climate change.Quantify Uncertainty:\nAdopt ensemble approaches scenario comparisons enhance prediction reliability.\nAdopt ensemble approaches scenario comparisons enhance prediction reliability.Pro Tip: Visualize current future suitability maps side side clearly illustrate range shifts areas risk habitat loss.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"illustrative-example-hemlock-distribution-under-climate-change","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.7 Illustrative Example: Hemlock Distribution Under Climate Change","text":"Let’s walk practical application Species Distribution Modeling (SDM) using eastern hemlock (Tsuga canadensis). example demonstrates apply five steps SDM predict species’ current distribution project future ranges changing climate conditions.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"conceptualization","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.7.1 1. Conceptualization","text":"Goal: Understand climate change might affect distribution Tsuga canadensis.Key considerations:Species Ecology:\nEastern hemlock thrives cool, damp environments, often found regions high soil moisture moderate temperatures.\nEastern hemlock thrives cool, damp environments, often found regions high soil moisture moderate temperatures.Predictors:\nRelevant environmental variables include:\nTemperature: Annual mean temperature (Bio1).\nSoil Moisture: Precipitation driest quarter (Bio17).\nTopography: Elevation data digital elevation models (DEMs).\n\nRelevant environmental variables include:\nTemperature: Annual mean temperature (Bio1).\nSoil Moisture: Precipitation driest quarter (Bio17).\nTopography: Elevation data digital elevation models (DEMs).\nTemperature: Annual mean temperature (Bio1).Soil Moisture: Precipitation driest quarter (Bio17).Topography: Elevation data digital elevation models (DEMs).Research Questions:\nwarming temperatures impact hemlock’s suitable habitat?\nspecies shift higher elevations latitudes?\nwarming temperatures impact hemlock’s suitable habitat?species shift higher elevations latitudes?","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"data-preparation","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.7.2 2. Data Preparation","text":"Objective: Gather, clean, align biological environmental data.Steps involved:Biological Data:\nUse presence-absence data field surveys GBIF occurrence records.\nClean dataset:\nRemove duplicates outliers.\nVerify coordinate accuracy.\nAddress spatial bias (e.g., oversampling near roads).\n\nUse presence-absence data field surveys GBIF occurrence records.Clean dataset:\nRemove duplicates outliers.\nVerify coordinate accuracy.\nAddress spatial bias (e.g., oversampling near roads).\nRemove duplicates outliers.Verify coordinate accuracy.Address spatial bias (e.g., oversampling near roads).Environmental Data:\nDownload bioclimatic variables WorldClim CMIP6:\nCurrent climate data baseline predictions.\nFuture climate projections RCP 4.5 (moderate emissions) RCP 8.5 (high emissions).\n\nEnsure:\nResolution: cell size layers (e.g., 1 km).\nExtent: Cover geographic area biological data.\nCRS: Use common coordinate reference system (e.g., WGS84).\n\nDownload bioclimatic variables WorldClim CMIP6:\nCurrent climate data baseline predictions.\nFuture climate projections RCP 4.5 (moderate emissions) RCP 8.5 (high emissions).\nCurrent climate data baseline predictions.Future climate projections RCP 4.5 (moderate emissions) RCP 8.5 (high emissions).Ensure:\nResolution: cell size layers (e.g., 1 km).\nExtent: Cover geographic area biological data.\nCRS: Use common coordinate reference system (e.g., WGS84).\nResolution: cell size layers (e.g., 1 km).Extent: Cover geographic area biological data.CRS: Use common coordinate reference system (e.g., WGS84).Scaling:\nStandardize predictors comparability.\nAlign temporal scales (e.g., match occurrence data climate data year).\nStandardize predictors comparability.Align temporal scales (e.g., match occurrence data climate data year).","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"model-fitting","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.7.3 3. Model Fitting","text":"Objective: Develop model accurately predicts suitable habitats Tsuga canadensis.Steps:Algorithm:\nUse MaxEnt presence-data Random Forest presence-absence data.\nUse MaxEnt presence-data Random Forest presence-absence data.Variable Selection:\nRefine predictors iteratively :\nCalculating Variance Inflation Factor (VIF) identify collinear variables.\nPrioritizing ecologically meaningful variables.\n\nRefine predictors iteratively :\nCalculating Variance Inflation Factor (VIF) identify collinear variables.\nPrioritizing ecologically meaningful variables.\nCalculating Variance Inflation Factor (VIF) identify collinear variables.Prioritizing ecologically meaningful variables.Model Tuning:\nAdjust MaxEnt regularization avoid overfitting.\nUse cross-validation optimize model performance.\nAdjust MaxEnt regularization avoid overfitting.Use cross-validation optimize model performance.Example:\n- Fit model using predictors like Bio1 (temperature) Bio17 (precipitation driest quarter).\n- Visualize suitability map current conditions.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"model-evaluation","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.7.4 4. Model Evaluation","text":"Objective: Validate model ensure makes biologically statistically sound predictions.Steps:Metrics:\nUse AUC assess model’s ability distinguish suitable unsuitable areas.\nUse TSS Kappa additional validation.\nUse AUC assess model’s ability distinguish suitable unsuitable areas.Use TSS Kappa additional validation.Ecological Validation:\nCheck species-environment relationships.\nEnsure predictions align known habitat preferences (e.g., cool, damp climates hemlock).\nCheck species-environment relationships.Ensure predictions align known habitat preferences (e.g., cool, damp climates hemlock).Cross-Validation:\nSplit data training testing sets evaluate predictive performance unseen data.\nSplit data training testing sets evaluate predictive performance unseen data.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"projection","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.7.5 5. Projection","text":"Objective: Map current future distributions assess potential impacts climate change.Steps:Current Distribution:\nPredict hemlock’s habitat suitability current climate conditions.\nMap species’ potential range.\nPredict hemlock’s habitat suitability current climate conditions.Map species’ potential range.Future Projections:\nUse RCP 4.5 (moderate emissions) RCP 8.5 (high emissions) project future ranges.\nMap potential range shifts due warming temperatures.\nUse RCP 4.5 (moderate emissions) RCP 8.5 (high emissions) project future ranges.Map potential range shifts due warming temperatures.Scenario Comparison:\nIdentify areas habitat loss, persistence, gain.\nQuantify uncertainty using ensemble models multiple climate projections.\nIdentify areas habitat loss, persistence, gain.Quantify uncertainty using ensemble models multiple climate projections.Example:\n- RCP 8.5, hemlock may lose low-altitude habitats shift higher elevations northern regions.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"visualization","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.7.6 Visualization","text":"Current Future Range Maps:\n1. Current Range:\n- Map areas high habitat suitability current conditions.\n2. Future Projections:\n- Create side--side maps RCP 4.5 RCP 8.5 scenarios.\n- Highlight regions range expansion, contraction, persistence.","code":""},{"path":"steps-in-species-distribution-modeling-sdm.html","id":"summary-1","chapter":"6 Steps in Species Distribution Modeling (SDM)","heading":"6.8 Summary","text":"applying steps Tsuga canadensis, can:\n- Understand current habitat preferences.\n- Predict range might shift different climate change scenarios.\n- Inform conservation strategies mitigate potential habitat loss.Remember :\n1. Prioritize data quality ensure alignment predictors.\n2. Validate model using appropriate metrics.\n3. Account uncertainties projections using ensemble approaches.framework can adapted study species ecological questions, making powerful tool conservation planning ecological research.","code":""},{"path":"sdm-techniques.html","id":"sdm-techniques","chapter":"7 SDM Techniques","heading":"7 SDM Techniques","text":"","code":""},{"path":"sdm-techniques.html","id":"introduction-to-sdm-techniques","chapter":"7 SDM Techniques","heading":"7.1 1. Introduction to SDM Techniques","text":"SDM Techniques?Species Distribution Modeling (SDM) techniques statistical machine learning methods used predict distribution species based environmental biological data. techniques analyze relationships observed species occurrences environmental predictors create maps potential distributions.","code":""},{"path":"sdm-techniques.html","id":"why-are-sdm-techniques-important","chapter":"7 SDM Techniques","heading":"7.1.1 Why Are SDM Techniques Important?","text":"Understanding Species-Environment Interactions:\nSDM techniques help ecologists identify factors influencing species thrive struggle.Understanding Species-Environment Interactions:\nSDM techniques help ecologists identify factors influencing species thrive struggle.Conservation Planning:\nsupport decisions habitat protection, restoration, species reintroduction highlighting critical areas biodiversity.Conservation Planning:\nsupport decisions habitat protection, restoration, species reintroduction highlighting critical areas biodiversity.Climate Change Adaptation:\nPredict species’ ranges might shift future environmental scenarios, aiding proactive conservation strategies.Climate Change Adaptation:\nPredict species’ ranges might shift future environmental scenarios, aiding proactive conservation strategies.Choosing Right Technique Matters!Different SDM techniques suited specific types data research objectives. success model depends matching right approach study goals dataset characteristics.","code":""},{"path":"sdm-techniques.html","id":"how-sdm-techniques-work","chapter":"7 SDM Techniques","heading":"7.1.2 How SDM Techniques Work","text":"core, SDM techniques rely three main components:Species Data:\nPresence-data: species observed (e.g., herbarium records).\nPresence-absence data: Locations species present absent (e.g., systematic surveys).\nPresence-data: species observed (e.g., herbarium records).Presence-absence data: Locations species present absent (e.g., systematic surveys).Environmental Predictors:\nVariables like temperature, precipitation, soil type, elevation, influence species distributions.\nVariables like temperature, precipitation, soil type, elevation, influence species distributions.Modeling Algorithms:\nStatistical computational methods used analyze relationships generate predictions.\nStatistical computational methods used analyze relationships generate predictions.","code":""},{"path":"sdm-techniques.html","id":"key-goals-of-sdm-techniques","chapter":"7 SDM Techniques","heading":"7.1.3 Key Goals of SDM Techniques","text":"Example:\neastern hemlock (Tsuga canadensis), SDM techniques might explore temperature, soil moisture, elevation influence distribution, providing insights response climate change.","code":""},{"path":"sdm-techniques.html","id":"takeaway","chapter":"7 SDM Techniques","heading":"7.1.4 Takeaway","text":"SDM techniques powerful tools addressing ecological conservation challenges. following sections, explore various SDM approaches, strengths, choose best one research.","code":""},{"path":"sdm-techniques.html","id":"primary-categories-of-sdm-techniques","chapter":"7 SDM Techniques","heading":"7.2 2. Primary Categories of SDM Techniques","text":"Species Distribution Modeling (SDM) techniques diverse, tailored specific types data research objectives. can broadly categorized four main groups:","code":""},{"path":"sdm-techniques.html","id":"envelopes-and-distance-based-methods","chapter":"7 SDM Techniques","heading":"7.2.1 2.1. Envelopes and Distance-Based Methods","text":"Overview: methods define environmental conditions (“envelopes”) suitable species use similarity metrics assess habitat suitability.","code":""},{"path":"sdm-techniques.html","id":"bioclim","chapter":"7 SDM Techniques","heading":"7.2.1.1 BIOCLIM","text":":\nIdentifies climatic envelopes considering species occurrences within range environmental conditions.Strengths:\nSimple intuitive.\nEffective presence-data.\nSimple intuitive.Effective presence-data.Limitations:\nAssumes conditions within envelope equally suitable.\nAssumes conditions within envelope equally suitable.Use Case: Mapping climatic suitability plant species based temperature precipitation.","code":""},{"path":"sdm-techniques.html","id":"domain","chapter":"7 SDM Techniques","heading":"7.2.1.2 DOMAIN","text":":\nCalculates environmental similarity using Gower’s distance assess habitat suitability.Strengths:\nEasy implement.\nHandles multi-dimensional environmental data well.\nEasy implement.Handles multi-dimensional environmental data well.Limitations:\nSensitive outliers.\nSensitive outliers.Use Case: Predicting distribution insect species based microhabitat conditions.","code":""},{"path":"sdm-techniques.html","id":"mahalanobis-distance","chapter":"7 SDM Techniques","heading":"7.2.1.3 Mahalanobis Distance","text":":\nMeasures multivariate similarity based covariance structure data.Strengths:\nRobust multicollinearity among predictors.\nSuitable continuous environmental data.\nRobust multicollinearity among predictors.Suitable continuous environmental data.Limitations:\nAssumes linear relationship variables.\nAssumes linear relationship variables.Use Case: Modeling habitat suitability mammals forested landscapes.","code":""},{"path":"sdm-techniques.html","id":"ecological-niche-factor-analysis-enfa","chapter":"7 SDM Techniques","heading":"7.2.1.4 Ecological Niche Factor Analysis (ENFA)","text":":\nIdentifies species’ ecological niche comparing environmental conditions presence locations overall study area.Strengths:\nProvides insights ecological specialization.\nUseful rare species limited data.\nProvides insights ecological specialization.Useful rare species limited data.Limitations:\nRequires careful interpretation niche parameters.\nRequires careful interpretation niche parameters.Use Case: Studying niche shifts invasive species.","code":""},{"path":"sdm-techniques.html","id":"regression-based-methods","chapter":"7 SDM Techniques","heading":"7.2.2 2.2. Regression-Based Methods","text":"Overview: methods analyze relationship species occurrences environmental predictors, often providing interpretable models.","code":""},{"path":"sdm-techniques.html","id":"generalized-linear-models-glms","chapter":"7 SDM Techniques","heading":"7.2.2.1 Generalized Linear Models (GLMs)","text":":\nModels species-environment relationships using linear predictors link function.Strengths:\nFlexible interpretable.\nHandles presence-absence data well.\nFlexible interpretable.Handles presence-absence data well.Limitations:\nAssumes linear predefined relationships.\nAssumes linear predefined relationships.Use Case: Analyzing effect temperature precipitation bird distributions.","code":""},{"path":"sdm-techniques.html","id":"generalized-additive-models-gams","chapter":"7 SDM Techniques","heading":"7.2.2.2 Generalized Additive Models (GAMs)","text":":\nExtends GLMs allowing non-linear relationships smooth functions.Strengths:\nCaptures complex, non-linear relationships.\nHighly flexible.\nCaptures complex, non-linear relationships.Highly flexible.Limitations:\nComputationally intensive large datasets.\nComputationally intensive large datasets.Use Case: Modeling species distribution along temperature gradients.","code":""},{"path":"sdm-techniques.html","id":"multivariate-adaptive-regression-splines-mars","chapter":"7 SDM Techniques","heading":"7.2.2.3 Multivariate Adaptive Regression Splines (MARS)","text":":\nBreaks predictor relationships piecewise linear segments.Strengths:\nHandles complex, non-linear interactions.\nAutomatic feature selection.\nHandles complex, non-linear interactions.Automatic feature selection.Limitations:\nRequires careful parameter tuning.\nRequires careful parameter tuning.Use Case: Predicting fish distributions aquatic systems.","code":""},{"path":"sdm-techniques.html","id":"bayesian-approaches","chapter":"7 SDM Techniques","heading":"7.2.2.4 Bayesian Approaches","text":":\nIncorporates prior knowledge updates predictions observed data.Strengths:\nAccounts uncertainty.\nIdeal sparse noisy data.\nAccounts uncertainty.Ideal sparse noisy data.Limitations:\nComputationally demanding.\nComputationally demanding.Use Case: Modeling rare species distributions limited occurrence data.","code":""},{"path":"sdm-techniques.html","id":"decision-tree-methods","chapter":"7 SDM Techniques","heading":"7.2.3 2.3. Decision Tree Methods","text":"Overview: Decision trees split data hierarchical branches, providing interpretable models species-environment relationships.","code":""},{"path":"sdm-techniques.html","id":"classification-and-regression-trees-cart","chapter":"7 SDM Techniques","heading":"7.2.3.1 Classification and Regression Trees (CART)","text":":\nBuilds decision tree classify species presence absence based environmental thresholds.Strengths:\nSimple intuitive.\nHandles non-linear relationships well.\nSimple intuitive.Handles non-linear relationships well.Limitations:\nProne overfitting without pruning.\nProne overfitting without pruning.Use Case: Identifying habitat thresholds amphibian species based wetland characteristics.","code":""},{"path":"sdm-techniques.html","id":"machine-learning-approaches","chapter":"7 SDM Techniques","heading":"7.2.4 2.4. Machine Learning Approaches","text":"Overview: Advanced techniques excel handling complex data high dimensionality interactions.","code":""},{"path":"sdm-techniques.html","id":"maximum-entropy-maxent","chapter":"7 SDM Techniques","heading":"7.2.4.1 Maximum Entropy (MaxEnt)","text":":\nPredicts species distribution maximizing entropy environmental constraints.Strengths:\nWorks well presence-data.\nEasy interpret.\nWorks well presence-data.Easy interpret.Limitations:\nMay overfit small datasets.\nMay overfit small datasets.Use Case: Mapping potential distributions endangered plants.","code":""},{"path":"sdm-techniques.html","id":"random-forests","chapter":"7 SDM Techniques","heading":"7.2.4.2 Random Forests","text":":\nCombines multiple decision trees improve predictive accuracy.Strengths:\nHandles large datasets complex interactions.\nRobust overfitting.\nHandles large datasets complex interactions.Robust overfitting.Limitations:\nDifficult interpret compared simpler models.\nDifficult interpret compared simpler models.Use Case: Predicting bird distributions across diverse habitats.","code":""},{"path":"sdm-techniques.html","id":"boosted-regression-trees-brtgbm","chapter":"7 SDM Techniques","heading":"7.2.4.3 Boosted Regression Trees (BRT/GBM)","text":":\nSequentially builds decision trees minimize prediction errors.Strengths:\nHigh predictive accuracy.\nHandles non-linear relationships interactions.\nHigh predictive accuracy.Handles non-linear relationships interactions.Limitations:\nComputationally expensive.\nComputationally expensive.Use Case: Modeling shifts species ranges future climate scenarios.","code":""},{"path":"sdm-techniques.html","id":"artificial-neural-networks-anns","chapter":"7 SDM Techniques","heading":"7.2.4.4 Artificial Neural Networks (ANNs)","text":":\nMimics human brain processes model complex relationships.Strengths:\nHighly flexible powerful large datasets.\nCaptures non-linear relationships.\nHighly flexible powerful large datasets.Captures non-linear relationships.Limitations:\nRequires large training datasets.\nDifficult interpret.\nRequires large training datasets.Difficult interpret.Use Case: Predicting aquatic species distributions based multiple predictors.","code":""},{"path":"sdm-techniques.html","id":"support-vector-machines-svms","chapter":"7 SDM Techniques","heading":"7.2.4.5 Support Vector Machines (SVMs)","text":":\nSeparates species presence absence data using hyperplanes high-dimensional space.Strengths:\nEffective small datasets complex boundaries.\nHandles high-dimensional data well.\nEffective small datasets complex boundaries.Handles high-dimensional data well.Limitations:\nComputationally intensive.\nComputationally intensive.Use Case: Mapping species distributions fragmented landscapes.","code":""},{"path":"sdm-techniques.html","id":"key-takeaways-8","chapter":"7 SDM Techniques","heading":"7.2.5 Key Takeaways","text":"Different SDM techniques cater different types data (e.g., presence-, presence-absence) study goals.Envelopes Distance-Based Methods simple intuitive limited complexity.Regression-Based Methods balance interpretability flexibility.Decision Tree Methods easy understand can overfit.Machine Learning Approaches provide powerful tools complex, high-dimensional data require careful tuning interpretation.","code":""},{"path":"sdm-techniques.html","id":"focus-on-mahalanobis-distance","chapter":"7 SDM Techniques","heading":"7.3 3. Focus on Mahalanobis Distance","text":"","code":""},{"path":"sdm-techniques.html","id":"what-is-it","chapter":"7 SDM Techniques","heading":"7.3.1 What is it?","text":"Mahalanobis Distance: statistical measure multivariate similarity evaluates similar new observation reference set based multiple variables.","code":""},{"path":"sdm-techniques.html","id":"key-characteristics","chapter":"7 SDM Techniques","heading":"7.3.1.1 Key Characteristics","text":"Accounts Correlations: considers correlations variables adjusts differences scales.Presence-Method: Suitable datasets species presence recorded.Linear Assumptions: Assumes linear relationship predictors.Output: Provides distance metric (D²), smaller values indicate higher similarity reference conditions.","code":""},{"path":"sdm-techniques.html","id":"key-features","chapter":"7 SDM Techniques","heading":"7.3.2 Key Features","text":"","code":""},{"path":"sdm-techniques.html","id":"advantages","chapter":"7 SDM Techniques","heading":"7.3.2.1 Advantages","text":"Collinearity-Free Analysis: Handles correlated predictors effectively considering covariance structure.Continuous Predictors: Works seamlessly continuous environmental data.Simple Accessible: Easy implement minimal data preprocessing.","code":""},{"path":"sdm-techniques.html","id":"limitations","chapter":"7 SDM Techniques","heading":"7.3.2.2 Limitations","text":"Assumes Equal Weights: predictors treated equally important unless modified.Linear Assumptions: May perform well non-linear relationships categorical data.","code":""},{"path":"sdm-techniques.html","id":"implementation-in-r","chapter":"7 SDM Techniques","heading":"7.3.3 Implementation in R","text":"mahal function dismo package commonly used calculate Mahalanobis distances SDM.","code":""},{"path":"sdm-techniques.html","id":"steps-for-implementation","chapter":"7 SDM Techniques","heading":"7.3.3.1 Steps for Implementation","text":"Prepare presence-data environmental variables.Compute Mahalanobis distance cell study area.Map resulting suitability values.","code":""},{"path":"sdm-techniques.html","id":"comparing-sdm-techniques","chapter":"7 SDM Techniques","heading":"7.4 4. Comparing SDM Techniques","text":"","code":""},{"path":"sdm-techniques.html","id":"summary-table-of-sdm-techniques","chapter":"7 SDM Techniques","heading":"7.4.1 Summary Table of SDM Techniques","text":"","code":""},{"path":"sdm-techniques.html","id":"guidance-on-technique-selection","chapter":"7 SDM Techniques","heading":"7.4.2 Guidance on Technique Selection","text":"","code":""},{"path":"sdm-techniques.html","id":"data-type","chapter":"7 SDM Techniques","heading":"7.4.2.1 Data Type","text":"Presence-: Consider MaxEnt, BIOCLIM, Mahalanobis Distance.Presence-Absence: Use GLMs, GAMs, Random Forests.","code":""},{"path":"sdm-techniques.html","id":"study-goals","chapter":"7 SDM Techniques","heading":"7.4.2.2 Study Goals","text":"Ecological Insights: Choose interpretable models like GLMs BIOCLIM.Predictive Accuracy: Opt machine learning approaches like MaxEnt BRT.","code":""},{"path":"sdm-techniques.html","id":"key-considerations-for-choosing-an-sdm-technique","chapter":"7 SDM Techniques","heading":"7.5 5. Key Considerations for Choosing an SDM Technique","text":"","code":""},{"path":"sdm-techniques.html","id":"data-availability","chapter":"7 SDM Techniques","heading":"7.5.1 1. Data Availability","text":"Presence-vs. Presence-Absence:\nPresence-data limits choice models common ecological datasets.\nPresence-absence data allows complex algorithms better evaluation metrics.\nPresence-data limits choice models common ecological datasets.Presence-absence data allows complex algorithms better evaluation metrics.","code":""},{"path":"sdm-techniques.html","id":"predictor-selection","chapter":"7 SDM Techniques","heading":"7.5.2 2. Predictor Selection","text":"Choose variables ecologically relevant species.Avoid multicollinearity prevent misleading results.Pro Tip: Use techniques like Variance Inflation Factor (VIF) identify remove highly correlated predictors.","code":""},{"path":"sdm-techniques.html","id":"computational-resources","chapter":"7 SDM Techniques","heading":"7.5.3 3. Computational Resources","text":"Simpler Methods: Use regression-based distance-based models quick analyses.Machine Learning: Requires resources provides higher predictive accuracy.","code":""},{"path":"sdm-techniques.html","id":"study-objectives","chapter":"7 SDM Techniques","heading":"7.5.4 4. Study Objectives","text":"Align model choice research question. example:\nUse MaxEnt endangered species sparse data.\nOpt BRT model complex interactions project future distributions.\nUse MaxEnt endangered species sparse data.Opt BRT model complex interactions project future distributions.","code":""},{"path":"sdm-techniques.html","id":"summary-and-key-takeaways-2","chapter":"7 SDM Techniques","heading":"7.6 6. Summary and Key Takeaways","text":"","code":""},{"path":"sdm-techniques.html","id":"key-points","chapter":"7 SDM Techniques","heading":"7.6.1 Key Points","text":"SDM techniques vary complexity, data requirements, predictive capabilities.Mahalanobis Distance versatile method presence-data, particularly predictors continuous correlated.Machine learning approaches like Random Forests MaxEnt excel predictive accuracy require careful tuning validation.","code":""},{"path":"sdm-techniques.html","id":"next-steps-1","chapter":"7 SDM Techniques","heading":"7.6.2 Next Steps","text":"Explore hybrid models combine strengths different SDM techniques.Delve advanced topics like ensemble modeling integrating ecological processes.choosing right SDM technique, researchers can effectively address ecological questions inform conservation strategies robust predictions.","code":"\n# Step 1: Load Required Libraries\nlibrary(dismo)  # For species distribution modeling\n#> Loading required package: raster\n#> Loading required package: sp\nlibrary(raster) # For raster data manipulation\nlibrary(maps)   # For base map visualization\nlibrary(pROC)   # For model evaluation (ROC curve)\n#> Type 'citation(\"pROC\")' for a citation.\n#> \n#> Attaching package: 'pROC'\n#> The following objects are masked from 'package:stats':\n#> \n#>     cov, smooth, var\n# Step 2: Load Example Occurrence Data\n# Using Bradypus (sloth) occurrence data included in the `dismo` package\nfile <- paste0(system.file(\"ex\", package = \"dismo\"), \"/bradypus.csv\")\nbradypus <- read.csv(file)\n\n# Keep only longitude and latitude columns\nbradypus <- bradypus[, 2:3]\nnames(bradypus) <- c(\"lon\", \"lat\")\n\n# Visualize the occurrence data on a world map\nmap(\"world\", col = \"gray90\", fill = TRUE, bg = \"lightblue\", lwd = 0.5)\npoints(bradypus, col = \"red\", pch = 20, cex = 0.8)\n# Step 3: Load Environmental Data\n# Load environmental predictors provided in the `dismo` package\npath <- system.file(\"ex\", package = \"dismo\")\nfiles <- list.files(path, pattern = \"grd$\", full.names = TRUE)\npredictors <- stack(files)  # Stack all raster layers\n\n# Ensure predictors have a valid CRS and check their structure\ncrs(predictors)  # Verify CRS\n#> Coordinate Reference System:\n#> Deprecated Proj.4 representation:\n#>  +proj=longlat +ellps=WGS84 +towgs84=0,0,0,0,0,0,0\n#> +no_defs \n#> WKT2 2019 representation:\n#> BOUNDCRS[\n#>     SOURCECRS[\n#>         GEOGCRS[\"unknown\",\n#>             DATUM[\"World Geodetic System 1984\",\n#>                 ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#>                     LENGTHUNIT[\"metre\",1]],\n#>                 ID[\"EPSG\",6326]],\n#>             PRIMEM[\"Greenwich\",0,\n#>                 ANGLEUNIT[\"degree\",0.0174532925199433],\n#>                 ID[\"EPSG\",8901]],\n#>             CS[ellipsoidal,2],\n#>                 AXIS[\"longitude\",east,\n#>                     ORDER[1],\n#>                     ANGLEUNIT[\"degree\",0.0174532925199433,\n#>                         ID[\"EPSG\",9122]]],\n#>                 AXIS[\"latitude\",north,\n#>                     ORDER[2],\n#>                     ANGLEUNIT[\"degree\",0.0174532925199433,\n#>                         ID[\"EPSG\",9122]]]]],\n#>     TARGETCRS[\n#>         GEOGCRS[\"WGS 84\",\n#>             DATUM[\"World Geodetic System 1984\",\n#>                 ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#>                     LENGTHUNIT[\"metre\",1]]],\n#>             PRIMEM[\"Greenwich\",0,\n#>                 ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>             CS[ellipsoidal,2],\n#>                 AXIS[\"geodetic latitude (Lat)\",north,\n#>                     ORDER[1],\n#>                     ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>                 AXIS[\"geodetic longitude (Lon)\",east,\n#>                     ORDER[2],\n#>                     ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>             ID[\"EPSG\",4326]]],\n#>     ABRIDGEDTRANSFORMATION[\"Transformation from unknown to WGS84\",\n#>         METHOD[\"Geocentric translations (geog2D domain)\",\n#>             ID[\"EPSG\",9603]],\n#>         PARAMETER[\"X-axis translation\",0,\n#>             ID[\"EPSG\",8605]],\n#>         PARAMETER[\"Y-axis translation\",0,\n#>             ID[\"EPSG\",8606]],\n#>         PARAMETER[\"Z-axis translation\",0,\n#>             ID[\"EPSG\",8607]]]]\nnames(predictors)  # View predictor names\n#> [1] \"bio1\"  \"bio12\" \"bio16\" \"bio17\" \"bio5\"  \"bio6\"  \"bio7\" \n#> [8] \"bio8\"  \"biome\"\n\n# Visualize one of the predictors\nplot(predictors[[1]], main = \"Environmental Predictor: Layer 1\")\n# Step 4: Extract Environmental Values at Occurrence Points\n# Get the environmental values corresponding to the occurrence locations\npresvals <- extract(predictors, bradypus)\nhead(presvals)  # This contains environmental values, not coordinates\n#>      bio1 bio12 bio16 bio17 bio5 bio6 bio7 bio8 biome\n#> [1,]  263  1639   724    62  338  191  147  261     1\n#> [2,]  263  1639   724    62  338  191  147  261     1\n#> [3,]  253  3624  1547   373  329  150  179  271     1\n#> [4,]  243  1693   775   186  318  150  168  264     1\n#> [5,]  243  1693   775   186  318  150  168  264     1\n#> [6,]  252  2501  1081   280  326  154  172  270     1\n\n# Combine spatial coordinates with environmental values for Mahalanobis\npresence_points <- cbind(bradypus, presvals)\n# Step 5: Generate Background Points\n# Generate random points (pseudo-absences) within the raster extent\nset.seed(123)  # For reproducibility\nbg_points <- randomPoints(predictors, n = 500)  # Generate 500 points\n\n# Extract environmental values at the background points\nbgvals <- extract(predictors, bg_points)\n\n# Combine presence and background data for modeling\npresence_absence <- c(rep(1, nrow(presvals)), rep(0, nrow(bgvals)))\nsdmData <- data.frame(pa = presence_absence, rbind(presvals, bgvals))\n# Step 6: Mahalanobis Distance Model\n# Fit the Mahalanobis model using presence locations and environmental values\nmahal_model <- mahal(presence_points[, -c(1, 2)])  # Exclude coordinates, use predictors\n\nsummary(mahal_model)\n#>      Length       Class        Mode \n#>           1 Mahalanobis          S4\n# Step 7: Create a Prediction Map\n# Predict Mahalanobis distances across the raster extent\nmahal_map <- predict(predictors, mahal_model)\n\n# Visualize the prediction map\nplot(mahal_map, main = \"Mahalanobis Distance Map\")\nmap(\"world\", add = TRUE, col = \"gray\", lwd = 0.5)\n# Step 8: Evaluate the Model\n# Evaluate predictions using presence and background points\npresence_pred <- extract(mahal_map, bradypus)\nbackground_pred <- extract(mahal_map, bg_points)\n\n# Combine predictions into one vector\npredictions <- c(presence_pred, background_pred)\nlabels <- c(rep(1, length(presence_pred)), rep(0, length(background_pred)))\n\n# Generate a simple performance summary\nroc_curve <- roc(labels, predictions)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\nplot(roc_curve, main = \"ROC Curve for Mahalanobis Model\")\nauc_value <- auc(roc_curve)  # Area Under the Curve (AUC)\n\nprint(paste(\"AUC:\", auc_value))\n#> [1] \"AUC: 0.997\""},{"path":"maxent-algorithm.html","id":"maxent-algorithm","chapter":"8 Maxent Algorithm","heading":"8 Maxent Algorithm","text":"","code":""},{"path":"maxent-algorithm.html","id":"introduction-to-maxent","chapter":"8 Maxent Algorithm","heading":"8.0.1 1. Introduction to MaxEnt","text":"MaxEnt?MaxEnt (Maximum Entropy Model) presence-species distribution modeling (SDM) method. predicts species likely occur based environmental conditions locations species observed.","code":""},{"path":"maxent-algorithm.html","id":"why-use-maxent","chapter":"8 Maxent Algorithm","heading":"8.0.1.1 Why Use MaxEnt?","text":"MaxEnt one popular tools ecology conservation :works well even limited data.require absence data, can difficult collect reliably.produces intuitive, easy--interpret habitat suitability maps.","code":""},{"path":"maxent-algorithm.html","id":"key-use-cases","chapter":"8 Maxent Algorithm","heading":"8.0.1.2 Key Use Cases","text":"MaxEnt used tasks like:Predicting species distributions: might species occur based environmental preferences?Assessing climate change impacts: might species’ range shift future environmental changes?Conservation planning: Identifying areas high suitability protect key habitats.Example: Imagine records bird species found forested areas. Using MaxEnt, can predict regions similar forest conditions likely suitable bird.","code":""},{"path":"maxent-algorithm.html","id":"core-concepts-in-maxent","chapter":"8 Maxent Algorithm","heading":"8.0.2 2. Core Concepts in MaxEnt","text":"","code":""},{"path":"maxent-algorithm.html","id":"presence-background-method","chapter":"8 Maxent Algorithm","heading":"8.0.2.1 Presence-Background Method","text":"MaxEnt WorksMaxEnt estimates probability species presence based environmental conditions observed locations. compares conditions randomly selected “background” points, represent available conditions across study area.","code":""},{"path":"maxent-algorithm.html","id":"presence-background-density-estimation","chapter":"8 Maxent Algorithm","heading":"8.0.2.1.1 Presence-Background Density Estimation","text":"Presence data: Locations species observed (e.g., field surveys citizen science platforms like GBIF).Background data: random sample points across landscape representing environmental conditions available species.MaxEnt models distribution suitable habitat conditions finding uniform (spread-) distribution matches constraints provided presence data.","code":""},{"path":"maxent-algorithm.html","id":"role-of-constraints","chapter":"8 Maxent Algorithm","heading":"8.0.2.1.2 Role of Constraints","text":"Constraints based environmental variables presence locations.example:\nspecies found areas temperatures 10–20°C, MaxEnt uses constraint predict suitable areas.\nspecies found areas temperatures 10–20°C, MaxEnt uses constraint predict suitable areas.","code":""},{"path":"maxent-algorithm.html","id":"key-assumptions","chapter":"8 Maxent Algorithm","heading":"8.0.2.2 Key Assumptions","text":"Important Assumptions MaxEntEcological Constraints: species limited environmental factors included model (e.g., climate, soil, vegetation).Representative Background Points:\nselected background points represent available environmental conditions study area.\nselected background points represent available environmental conditions study area.Presence Data Quality:\nPresence data reflects species occurs heavily biased sampling effort.\nPresence data reflects species occurs heavily biased sampling effort.","code":""},{"path":"maxent-algorithm.html","id":"illustrative-example","chapter":"8 Maxent Algorithm","heading":"8.0.2.2.1 Illustrative Example","text":"Suppose modeling habitat butterfly species. Presence records show prefers warm, humid areas moderate vegetation cover. MaxEnt identify regions conditions suitable ignoring drier colder areas.","code":""},{"path":"maxent-algorithm.html","id":"key-features-of-maxent","chapter":"8 Maxent Algorithm","heading":"8.0.3 3. Key Features of MaxEnt","text":"","code":""},{"path":"maxent-algorithm.html","id":"advantages-of-maxent","chapter":"8 Maxent Algorithm","heading":"8.0.3.1 Advantages of MaxEnt","text":"Handles Small Sample Sizes: MaxEnt highly effective limited data, making ideal rare poorly sampled species.Robust Overfitting: using regularization, MaxEnt prevents overly complex models, ensuring predictions realistic.Flexible Intuitive: MaxEnt provides clear outputs like habitat suitability maps response curves, easy interpret.","code":""},{"path":"maxent-algorithm.html","id":"disadvantages-of-maxent","chapter":"8 Maxent Algorithm","heading":"8.0.3.2 Disadvantages of MaxEnt","text":"Sensitive Background Points:\nchoice background data can significantly influence results. Poorly chosen background points can bias predictions.\nchoice background data can significantly influence results. Poorly chosen background points can bias predictions.User-Dependent Parameters:\nMaxEnt requires users make decisions parameters like feature types regularization multipliers. Poor choices may lead misleading models.\nMaxEnt requires users make decisions parameters like feature types regularization multipliers. Poor choices may lead misleading models.","code":""},{"path":"maxent-algorithm.html","id":"flexibility-in-modeling-features","chapter":"8 Maxent Algorithm","heading":"8.0.3.3 Flexibility in Modeling Features","text":"MaxEnt powerful allows nonlinear transformations environmental predictors, making adaptable different species datasets:Linear Features: Assume direct relationship predictors suitability.Quadratic Features: Capture curved relationships.Hinge Features: Represent thresholds suitability sharply increases decreases.Product Interaction Terms: Account combined effects predictors.Nonlinear Features Matter:\nImagine modeling frog thrives warm extreme temperatures. quadratic feature can capture suitability peak moderate temperatures decline high low temperatures.","code":""},{"path":"maxent-algorithm.html","id":"visualizing-response-curves","chapter":"8 Maxent Algorithm","heading":"8.0.3.3.1 Visualizing Response Curves","text":"Response curves show predicted suitability changes environmental variable. curves help interpret species-environment relationships.Use response curves :\n1. Check predictions align ecological knowledge.\n2. Identify variables influential model.","code":""},{"path":"maxent-algorithm.html","id":"decisions-in-maxent-modeling","chapter":"8 Maxent Algorithm","heading":"8.0.4 4. Decisions in MaxEnt Modeling","text":"","code":""},{"path":"maxent-algorithm.html","id":"background-selection","chapter":"8 Maxent Algorithm","heading":"8.0.4.1 1. Background Selection","text":"Background Points?\nBackground points represent environmental conditions available study area. crucial MaxEnt compares presence data points identify suitable habitats.","code":""},{"path":"maxent-algorithm.html","id":"key-considerations-1","chapter":"8 Maxent Algorithm","heading":"8.0.4.1.1 Key Considerations","text":"Study Area Definition:\nBackground points reflect accessible area species (e.g., dispersal range).\nBackground points reflect accessible area species (e.g., dispersal range).Sampling Bias:\npresence data biased (e.g., points near roads), background points must adjusted avoid reinforcing bias.\npresence data biased (e.g., points near roads), background points must adjusted avoid reinforcing bias.","code":""},{"path":"maxent-algorithm.html","id":"how-to-address-sampling-bias","chapter":"8 Maxent Algorithm","heading":"8.0.4.1.2 How to Address Sampling Bias","text":"Target Group Sampling:\nUse background points locations similar species sampled.\nUse background points locations similar species sampled.Spatial Bias Correction:\nWeight background points reflect unbiased sampling effort.\nWeight background points reflect unbiased sampling effort.Watch :\nAvoid using background points include inaccessible areas extreme environments irrelevant species.","code":""},{"path":"maxent-algorithm.html","id":"feature-selection","chapter":"8 Maxent Algorithm","heading":"8.0.4.2 2. Feature Selection","text":"MaxEnt uses “features” describe predictors influence suitability. Choosing appropriate features critical building interpretable models.","code":""},{"path":"maxent-algorithm.html","id":"steps-for-feature-selection","chapter":"8 Maxent Algorithm","heading":"8.0.4.2.1 Steps for Feature Selection","text":"Avoid Overfitting:\nUse fewer, ecologically meaningful predictors keep model simple interpretable.\nUse fewer, ecologically meaningful predictors keep model simple interpretable.Match Features Sample Size:\nUse feature types small number presence records. Complex features require data.\nUse feature types small number presence records. Complex features require data.Example:\n20 presence records, use linear quadratic features. Avoid hinge product features, require data fit accurately.","code":""},{"path":"maxent-algorithm.html","id":"regularization","chapter":"8 Maxent Algorithm","heading":"8.0.4.3 3. Regularization","text":"Regularization helps control model complexity penalizing overfitting. regularization multiplier (beta) determines trade-model simplicity accuracy.","code":""},{"path":"maxent-algorithm.html","id":"how-to-choose-regularization-values","chapter":"8 Maxent Algorithm","heading":"8.0.4.3.1 How to Choose Regularization Values","text":"Explore Different Values:\nTest range beta values find one produces parsimonious model (e.g., using cross-validation).\nTest range beta values find one produces parsimonious model (e.g., using cross-validation).Default Settings:\nMaxEnt provides default regularization settings, may optimal datasets. Adjust based study goals.\nMaxEnt provides default regularization settings, may optimal datasets. Adjust based study goals.Pro Tip:\nUse regularization ensure model generalizes well new data. Overly complex models may fit training data perfectly fail predict real-world patterns.","code":""},{"path":"maxent-algorithm.html","id":"why-regularization-matters","chapter":"8 Maxent Algorithm","heading":"8.0.4.3.2 Why Regularization Matters","text":"Without regularization, MaxEnt might overfit small noisy datasets, producing misleading predictions.","code":""},{"path":"maxent-algorithm.html","id":"visualization-example","chapter":"8 Maxent Algorithm","heading":"8.0.5 Visualization Example:","text":"example MaxEnt model different beta values:Interpretation:\n- Default regularization may produce detailed curves, risks overfitting.\n- Higher regularization produces smoother, simpler response curves, ideal generalization.","code":"\n# Fit MaxEnt models with varying regularization multipliers\nlibrary(dismo)\nmaxent_model_default <- maxent(predictors, presences)\nmaxent_model_high_reg <- maxent(predictors, presences, args = \"betamultiplier=2\")\n\n# Plot response curves to compare\nplot(maxent_model_default, type = \"response\", main = \"Default Regularization\")\nplot(maxent_model_high_reg, type = \"response\", main = \"High Regularization\")"},{"path":"maxent-algorithm.html","id":"model-output-and-interpretation","chapter":"8 Maxent Algorithm","heading":"8.0.6 5. Model Output and Interpretation","text":"","code":""},{"path":"maxent-algorithm.html","id":"output-types","chapter":"8 Maxent Algorithm","heading":"8.0.6.1 Output Types","text":"MaxEnt produces three types outputs, serving specific purpose:Raw Output:\nRepresents relative suitability location based species’ environmental conditions.\nValues probabilities scaled sum 1 across grid cells.\nRepresents relative suitability location based species’ environmental conditions.Values probabilities scaled sum 1 across grid cells.Cumulative Output:\nSums probabilities grid cells equal lower suitability.\nSuitable identifying best locations species within study area.\nSums probabilities grid cells equal lower suitability.Suitable identifying best locations species within study area.Logistic Output:\nTransforms raw output probability-like values (0 1).\nRepresents estimated probability presence certain assumptions sampling effort.\nTransforms raw output probability-like values (0 1).Represents estimated probability presence certain assumptions sampling effort.Pro Tip: logistic output often interpretable commonly used reflects habitat suitability straightforward way.","code":""},{"path":"maxent-algorithm.html","id":"suitability-map-visualization","chapter":"8 Maxent Algorithm","heading":"8.0.6.2 Suitability Map Visualization","text":"Suitability maps one valuable outputs MaxEnt, highlighting areas species likely occur.","code":""},{"path":"maxent-algorithm.html","id":"steps-to-generate-a-suitability-map","chapter":"8 Maxent Algorithm","heading":"8.0.6.2.1 Steps to Generate a Suitability Map","text":"Use predict() function MaxEnt create predictions study area.Visualize predictions using raster plotting tools.","code":""},{"path":"maxent-algorithm.html","id":"code-example","chapter":"8 Maxent Algorithm","heading":"8.0.6.2.2 Code Example:","text":"","code":"\n# Generate predictions from the MaxEnt model\nsuitability_map <- predict(maxent_model, predictors)\n\n# Visualize the suitability map\nlibrary(raster)\nplot(suitability_map, main = \"Habitat Suitability Map\")"},{"path":"maxent-algorithm.html","id":"interpreting-suitability-maps","chapter":"8 Maxent Algorithm","heading":"8.0.6.2.3 Interpreting Suitability Maps","text":"Areas higher suitability values (e.g., 0.7–1.0 logistic output) indicate optimal conditions species.Areas lower values (e.g., 0–0.3) less suitable unsuitable species.Analyze Patterns:\nLook clusters high-suitability areas identify key habitats potential range shifts changing conditions.","code":""},{"path":"maxent-algorithm.html","id":"evaluating-maxent-models","chapter":"8 Maxent Algorithm","heading":"8.0.7 6. Evaluating MaxEnt Models","text":"","code":""},{"path":"maxent-algorithm.html","id":"performance-metrics","chapter":"8 Maxent Algorithm","heading":"8.0.7.1 Performance Metrics","text":"Model evaluation essential ensure predictions accurate ecologically meaningful. MaxEnt provides several metrics assess performance:AUC (Area ROC Curve):\nMeasures model’s ability distinguish presence background points.\nValues range :\n0.5: Random prediction.\n1.0: Perfect prediction.\n\nMeasures model’s ability distinguish presence background points.Values range :\n0.5: Random prediction.\n1.0: Perfect prediction.\n0.5: Random prediction.1.0: Perfect prediction.","code":""},{"path":"maxent-algorithm.html","id":"code-example-1","chapter":"8 Maxent Algorithm","heading":"8.0.7.1.1 Code Example:","text":"Interpretation:\nAUC > 0.7 indicates good model, values > 0.9 suggest excellent predictive power.AIC (Akaike Information Criterion):\nUsed compare models select best one based simplicity goodness fit.\nCan calculated using tools like ENMTools.\nUsed compare models select best one based simplicity goodness fit.Can calculated using tools like ENMTools.","code":"\n# Evaluate the MaxEnt model's performance\nlibrary(pROC)\npredicted <- extract(suitability_map, presence_points)\nbackground <- extract(suitability_map, background_points)\nroc_curve <- roc(c(rep(1, length(predicted)), rep(0, length(background))),\n                 c(predicted, background))\nplot(roc_curve, main = \"ROC Curve for MaxEnt Model\")\nauc(roc_curve)  # Calculate AUC value"},{"path":"maxent-algorithm.html","id":"validation-techniques-1","chapter":"8 Maxent Algorithm","heading":"8.0.7.2 Validation Techniques","text":"Validation ensures MaxEnt model generalizes well new data. Common approaches include:Training vs. Testing Data:\nSplit occurrence data :\nTraining Data: Used build model.\nTesting Data: Used evaluate model’s predictive performance.\n\nExample: Use 70:30 split.\nSplit occurrence data :\nTraining Data: Used build model.\nTesting Data: Used evaluate model’s predictive performance.\nTraining Data: Used build model.Testing Data: Used evaluate model’s predictive performance.Example: Use 70:30 split.Cross-Validation:\nDivide data k-folds (e.g., 5 10).\nTrain model k-1 folds test remaining fold.\nRepeat process k times average results.\nDivide data k-folds (e.g., 5 10).Train model k-1 folds test remaining fold.Repeat process k times average results.","code":""},{"path":"maxent-algorithm.html","id":"code-example-2","chapter":"8 Maxent Algorithm","heading":"8.0.7.2.1 Code Example:","text":"Best Practice:\nUse cross-validation small datasets maximize use available data ensuring robust evaluation.","code":"\n# Perform k-fold cross-validation\nmaxent_model_cv <- maxent(predictors, presence_points, args = \"replicates=5\")"},{"path":"maxent-algorithm.html","id":"key-takeaways-for-model-evaluation","chapter":"8 Maxent Algorithm","heading":"8.0.8 Key Takeaways for Model Evaluation","text":"Combine Metrics: Use multiple metrics (e.g., AUC, AIC) comprehensive assessment.Validate Ecologically: Ensure predicted distributions align species’ known ecology.Test Iterate: Experiment different parameter settings background points optimize model.","code":""},{"path":"maxent-algorithm.html","id":"practical-application","chapter":"8 Maxent Algorithm","heading":"8.0.9 7. Practical Application","text":"","code":""},{"path":"maxent-algorithm.html","id":"building-a-maxent-model-in-r-step-by-step","chapter":"8 Maxent Algorithm","heading":"8.0.9.1 Building a MaxEnt Model in R: Step-by-Step","text":"Let’s walk practical example map distribution hypothetical species using MaxEnt R.","code":""},{"path":"maxent-algorithm.html","id":"step-1-load-required-libraries","chapter":"8 Maxent Algorithm","heading":"8.0.9.1.1 Step 1: Load Required Libraries","text":"","code":"\n# Load necessary libraries\nlibrary(dismo)  # For MaxEnt modeling\nlibrary(raster) # For working with spatial raster data\nlibrary(maps)   # For map visualization"},{"path":"maxent-algorithm.html","id":"step-2-load-occurrence-data","chapter":"8 Maxent Algorithm","heading":"8.0.9.1.2 Step 2: Load Occurrence Data","text":"","code":"\n# Example occurrence data included in the dismo package\noccurrence_file <- system.file(\"ex\", \"bradypus.csv\", package = \"dismo\")\noccurrences <- read.csv(occurrence_file)\n\n# Keep only latitude and longitude columns\noccurrences <- occurrences[, 2:3]\ncolnames(occurrences) <- c(\"lon\", \"lat\")\n\n# Visualize occurrences on a map\nmap(\"world\", col = \"gray90\", fill = TRUE, bg = \"lightblue\", lwd = 0.5)\npoints(occurrences, col = \"red\", pch = 20, cex = 0.8)"},{"path":"maxent-algorithm.html","id":"step-3-load-environmental-predictors","chapter":"8 Maxent Algorithm","heading":"8.0.9.1.3 Step 3: Load Environmental Predictors","text":"","code":"\n# Example environmental data provided in the dismo package\npath <- system.file(\"ex\", package = \"dismo\")\npredictor_files <- list.files(path, pattern = \"grd$\", full.names = TRUE)\n\n# Stack environmental layers\npredictors <- stack(predictor_files)\n\n# Visualize one predictor\nplot(predictors[[1]], main = \"Environmental Predictor: Layer 1\")"},{"path":"maxent-algorithm.html","id":"step-4-fit-the-maxent-model","chapter":"8 Maxent Algorithm","heading":"8.0.9.1.4 Step 4: Fit the MaxEnt Model","text":"","code":"\n# Fit the MaxEnt model\nmaxent_model <- maxent(predictors, occurrences)\n\n# View summary of the MaxEnt model\nprint(maxent_model)"},{"path":"maxent-algorithm.html","id":"step-5-generate-predictions","chapter":"8 Maxent Algorithm","heading":"8.0.9.1.5 Step 5: Generate Predictions","text":"","code":"\n# Create a suitability map\nsuitability_map <- predict(maxent_model, predictors)\n\n# Visualize the suitability map\nplot(suitability_map, main = \"MaxEnt Habitat Suitability Map\")\nmap(\"world\", add = TRUE, col = \"gray\", lwd = 0.5)"},{"path":"maxent-algorithm.html","id":"step-6-evaluate-model-performance","chapter":"8 Maxent Algorithm","heading":"8.0.9.1.6 Step 6: Evaluate Model Performance","text":"","code":"\n# Use training/testing split for evaluation\npresence_values <- extract(suitability_map, occurrences)\nbackground_points <- randomPoints(predictors, 500)  # Generate background points\nbackground_values <- extract(suitability_map, background_points)\n\n# Combine predictions\nlabels <- c(rep(1, length(presence_values)), rep(0, length(background_values)))\npredictions <- c(presence_values, background_values)\n\n# Evaluate model performance using AUC\nlibrary(pROC)\nroc_curve <- roc(labels, predictions)\nplot(roc_curve, main = \"ROC Curve for MaxEnt Model\")\nauc(roc_curve)  # Calculate AUC value"},{"path":"maxent-algorithm.html","id":"tips-for-interpreting-and-troubleshooting-results","chapter":"8 Maxent Algorithm","heading":"8.0.9.2 Tips for Interpreting and Troubleshooting Results","text":"Interpreting Maps:\nHigh suitability values indicate areas species likely occur.\nCompare predicted distributions known habitat preferences.\nHigh suitability values indicate areas species likely occur.Compare predicted distributions known habitat preferences.Troubleshooting Common Issues:\nmodel overfits: Adjust regularization parameters.\npredictions seem unrealistic: Check collinearity predictors sampling bias occurrence data.\nmodel overfits: Adjust regularization parameters.predictions seem unrealistic: Check collinearity predictors sampling bias occurrence data.Pro Tip: Always validate model outputs ecological knowledge ensure meaningful predictions.","code":""},{"path":"maxent-algorithm.html","id":"common-pitfalls-and-best-practices","chapter":"8 Maxent Algorithm","heading":"8.0.10 8. Common Pitfalls and Best Practices","text":"","code":""},{"path":"maxent-algorithm.html","id":"common-pitfalls","chapter":"8 Maxent Algorithm","heading":"8.0.10.1 Common Pitfalls","text":"-reliance Defaults:\ndefault settings MaxEnt always optimal dataset.\nCustomize feature selection regularization settings based species study area.\ndefault settings MaxEnt always optimal dataset.Customize feature selection regularization settings based species study area.Ignoring Background Selection:\nPoorly chosen background points can distort predictions.\nEnsure background points represent environmental availability study area.\nPoorly chosen background points can distort predictions.Ensure background points represent environmental availability study area.Overfitting:\nOverfitting occurs model complex data.\nAvoid overfitting simplifying feature selection increasing regularization.\nOverfitting occurs model complex data.Avoid overfitting simplifying feature selection increasing regularization.","code":""},{"path":"maxent-algorithm.html","id":"best-practices","chapter":"8 Maxent Algorithm","heading":"8.0.10.2 Best Practices","text":"Leverage Ecological Knowledge:\nSelect predictors ecologically relevant species.\nAvoid including highly correlated predictors.\nSelect predictors ecologically relevant species.Avoid including highly correlated predictors.Validate Thoroughly:\nUse cross-validation assess model performance.\nCombine statistical metrics (e.g., AUC) ecological validation.\nUse cross-validation assess model performance.Combine statistical metrics (e.g., AUC) ecological validation.Iterate Refine:\nTest different combinations predictors settings.\nAim balance simplicity predictive accuracy.\nTest different combinations predictors settings.Aim balance simplicity predictive accuracy.","code":""},{"path":"maxent-algorithm.html","id":"summary-and-key-takeaways-3","chapter":"8 Maxent Algorithm","heading":"8.0.11 9. Summary and Key Takeaways","text":"","code":""},{"path":"maxent-algorithm.html","id":"strengths-of-maxent","chapter":"8 Maxent Algorithm","heading":"8.0.11.1 Strengths of MaxEnt","text":"Handles presence-data effectively.Robust small sample sizes.Provides intuitive outputs like habitat suitability maps.","code":""},{"path":"maxent-algorithm.html","id":"limitations-of-maxent","chapter":"8 Maxent Algorithm","heading":"8.0.11.2 Limitations of MaxEnt","text":"Sensitive background point selection sampling bias.Requires careful tuning regularization feature selection.","code":""},{"path":"maxent-algorithm.html","id":"key-guidance-for-applying-maxent","chapter":"8 Maxent Algorithm","heading":"8.0.11.3 Key Guidance for Applying MaxEnt","text":"Start Simple:\nBegin small set predictors refine needed.\nBegin small set predictors refine needed.Validate Ecologically:\nEnsure predictions align species’ known ecology.\nEnsure predictions align species’ known ecology.Iterate Improve:\nRegularly test update model best results.\nRegularly test update model best results.","code":""},{"path":"maxent-algorithm.html","id":"by-understanding-its-strengths-limitations-and-best-practices-you-can-confidently-use-maxent-for-ecological-research-and-conservation-planning.","chapter":"8 Maxent Algorithm","heading":"8.1 By understanding its strengths, limitations, and best practices, you can confidently use MaxEnt for ecological research and conservation planning.","text":"","code":""},{"path":"exercise.html","id":"exercise","chapter":"9 Exercise","heading":"9 Exercise","text":"","code":""},{"path":"exercise.html","id":"exercise-building-and-interpreting-a-maxent-model","chapter":"9 Exercise","heading":"9.1 Exercise: Building and Interpreting a MaxEnt Model","text":"exercise, build MaxEnt model predict distribution species using occurrence data environmental predictors provided dismo package.","code":""},{"path":"exercise.html","id":"objectives","chapter":"9 Exercise","heading":"9.1.1 Objectives","text":"Load explore occurrence data environmental predictors.Fit MaxEnt model predict species distribution.Generate visualize habitat suitability map.Evaluate model using AUC interpret results.","code":""},{"path":"exercise.html","id":"step-1-load-required-libraries-1","chapter":"9 Exercise","heading":"9.1.2 Step 1: Load Required Libraries","text":"Load necessary libraries building visualizing MaxEnt model.","code":"\n# Load required libraries\nlibrary(dismo)  # For MaxEnt modeling\nlibrary(raster) # For spatial raster data\nlibrary(maps)   # For base map visualization"},{"path":"exercise.html","id":"step-2-load-occurrence-data-1","chapter":"9 Exercise","heading":"9.1.3 Step 2: Load Occurrence Data","text":"dismo package includes occurrence data Bradypus variegatus (species sloth). task :\n- Load data.\n- Extract latitude longitude columns.\n- Visualize occurrence points world map.","code":"\n# Load occurrence data\noccurrence_file <- system.file(\"ex\", \"bradypus.csv\", package = \"dismo\")\noccurrences <- read.csv(occurrence_file)\n\n# Keep only longitude and latitude columns\noccurrences <- occurrences[, 2:3]\ncolnames(occurrences) <- c(\"lon\", \"lat\")\n\n# Visualize occurrences on a map\nmap(\"world\", col = \"gray90\", fill = TRUE, bg = \"lightblue\", lwd = 0.5)\npoints(occurrences, col = \"red\", pch = 20, cex = 0.8)"},{"path":"exercise.html","id":"step-3-load-environmental-predictors-1","chapter":"9 Exercise","heading":"9.1.4 Step 3: Load Environmental Predictors","text":"Environmental predictors (e.g., temperature, precipitation) provided raster layers dismo package. task :\n- Load stack predictors.\n- Visualize one layer understand data.","code":"\n# Load and stack environmental predictors\npath <- system.file(\"ex\", package = \"dismo\")\npredictor_files <- list.files(path, pattern = \"grd$\", full.names = TRUE)\npredictors <- stack(predictor_files)\n\n# Visualize one predictor layer\nplot(predictors[[1]], main = \"Environmental Predictor: Layer 1\")"},{"path":"exercise.html","id":"step-4-fit-a-maxent-model","chapter":"9 Exercise","heading":"9.1.5 Step 4: Fit a MaxEnt Model","text":"Use maxent function build MaxEnt model using occurrence data environmental predictors.","code":"\n# Fit the MaxEnt model\nmaxent_model <- maxent(predictors, occurrences)\n\n# View model summary\nprint(maxent_model)"},{"path":"exercise.html","id":"step-5-generate-a-habitat-suitability-map","chapter":"9 Exercise","heading":"9.1.6 Step 5: Generate a Habitat Suitability Map","text":"Use fitted model predict habitat suitability across study area visualize output.","code":"\n# Predict habitat suitability\nsuitability_map <- predict(maxent_model, predictors)\n\n# Visualize the suitability map\nplot(suitability_map, main = \"MaxEnt Habitat Suitability Map\")\nmap(\"world\", add = TRUE, col = \"gray\", lwd = 0.5)"},{"path":"exercise.html","id":"step-6-evaluate-the-model","chapter":"9 Exercise","heading":"9.1.7 Step 6: Evaluate the Model","text":"Evaluate model’s performance using AUC.Extract suitability values occurrence points.Generate random background points extract suitability values.Combine predictions calculate AUC.","code":"\n# Extract predictions for presence and background points\npresence_values <- extract(suitability_map, occurrences)\nbackground_points <- randomPoints(predictors, 500)  # Generate 500 background points\nbackground_values <- extract(suitability_map, background_points)\n\n# Combine predictions\nlabels <- c(rep(1, length(presence_values)), rep(0, length(background_values)))\npredictions <- c(presence_values, background_values)\n\n# Evaluate using AUC\nlibrary(pROC)\nroc_curve <- roc(labels, predictions)\nplot(roc_curve, main = \"ROC Curve for MaxEnt Model\")\nauc_value <- auc(roc_curve)\nprint(paste(\"AUC:\", auc_value))"},{"path":"exercise.html","id":"questions","chapter":"9 Exercise","heading":"9.2 Questions","text":"high-suitability areas map represent? align knowledge Bradypus variegatus habitat?AUC value tell model’s performance?steps take improve model (e.g., addressing sampling bias selecting different predictors)?","code":""},{"path":"exercise.html","id":"challenge-optional","chapter":"9 Exercise","heading":"9.3 Challenge (Optional)","text":"Modify background point selection focus areas near occurrence points (e.g., using buffer).Try using subset predictors compare results.Use cross-validation assess model performance.Happy modeling! 🚀","code":""},{"path":"maxent-bias-correction.html","id":"maxent-bias-correction","chapter":"10 Maxent Bias Correction","heading":"10 Maxent Bias Correction","text":"","code":""},{"path":"maxent-bias-correction.html","id":"introduction-to-bias-in-maxent-models","chapter":"10 Maxent Bias Correction","heading":"10.0.1 1. Introduction to Bias in MaxEnt Models","text":"","code":""},{"path":"maxent-bias-correction.html","id":"what-is-bias","chapter":"10 Maxent Bias Correction","heading":"10.0.1.1 What is Bias?","text":"Bias MaxEnt models refers errors distortions arise data collection modeling, leading inaccurate predictions species distributions. Bias can occur due several reasons, uneven sampling efforts environmental conditions uniformly represented.","code":""},{"path":"maxent-bias-correction.html","id":"impact-of-bias","chapter":"10 Maxent Bias Correction","heading":"10.0.1.2 Impact of Bias","text":"Bias can significantly affect model’s predictions:-representation specific areas: species occurrence points easily accessible areas (e.g., near roads), model may predict areas highly suitable even .-representation important habitats: Regions rarely sampled may overlooked, even critical species.Incorrect response curves: relationships species environmental variables may reflect true ecological preferences.","code":""},{"path":"maxent-bias-correction.html","id":"why-is-bias-correction-important","chapter":"10 Maxent Bias Correction","heading":"10.0.1.3 Why Is Bias Correction Important?","text":"Correcting bias essential reliable meaningful predictions. Without addressing bias:model may overfit areas data, reducing ability generalize regions.Conservation efforts misdirected, focusing areas genuinely suitable species.","code":""},{"path":"maxent-bias-correction.html","id":"callout-blocks-to-clarify-key-points","chapter":"10 Maxent Bias Correction","heading":"10.0.2 Callout Blocks to Clarify Key Points","text":"Key Insight: Bias can distort suitability maps, making areas occurrence points seem favorable species.Pro Tip: Always examine occurrence data patterns suggest bias, clustering near roads urban centers.","code":""},{"path":"maxent-bias-correction.html","id":"real-life-example","chapter":"10 Maxent Bias Correction","heading":"10.0.2.1 Real-Life Example","text":"Imagine ’re modeling habitat bird species using occurrence data. sightings reported near cities easier access. Without correcting sampling bias, model might predict urban areas suitable species, even bird prefers forests.","code":""},{"path":"maxent-bias-correction.html","id":"simple-visualization","chapter":"10 Maxent Bias Correction","heading":"10.0.2.2 Simple Visualization","text":"demonstrate impact bias, can create scatter plot occurrence points overlaid environmental predictors (e.g., temperature elevation). plot can highlight clustering specific areas, indicating sampling bias.simple plot helps visualize occurrence points evenly distributed biased toward specific regions.","code":"\n# Example Visualization in R\nlibrary(ggplot2)\ndata(maps)  # Built-in map data\nggplot() +\n  borders(\"world\", colour = \"gray85\", fill = \"gray80\") +\n  geom_point(data = occurrence_data, aes(x = lon, y = lat), color = \"red\", size = 1.5) +\n  theme_minimal() +\n  ggtitle(\"Distribution of Occurrence Points\") +\n  labs(x = \"Longitude\", y = \"Latitude\")"},{"path":"maxent-bias-correction.html","id":"summary-2","chapter":"10 Maxent Bias Correction","heading":"10.0.2.3 Summary","text":"Bias MaxEnt models common challenge can addressed effectively proper techniques. Recognizing correcting sampling spatial biases ensures models produce reliable predictions support informed conservation decisions.","code":""},{"path":"maxent-bias-correction.html","id":"types-of-bias","chapter":"10 Maxent Bias Correction","heading":"10.0.3 2. Types of Bias","text":"","code":""},{"path":"maxent-bias-correction.html","id":"sampling-bias","chapter":"10 Maxent Bias Correction","heading":"10.0.3.1 1. Sampling Bias","text":"Sampling bias occurs species occurrence data collected unevenly across study area. can happen due practical constraints, accessibility survey effort.Causes:\n- Easier access areas (e.g., near roads, urban centers).\n- Survey focus specific regions habitats.Examples:\n- occurrence points species clustered near cities along well-traveled paths.\n- Remote areas potentially suitable habitats underrepresented.Key Insight: Sampling bias makes seem like species abundant accessible areas, even prefers remote habitats.Visualization Example:\ncan visualize sampling bias plotting occurrence points map identifying clusters.","code":"\nlibrary(ggplot2)\nggplot() +\n  borders(\"world\", colour = \"gray85\", fill = \"gray80\") +\n  geom_point(data = occurrence_data, aes(x = lon, y = lat), color = \"blue\", size = 1.2) +\n  theme_minimal() +\n  ggtitle(\"Sampling Bias in Occurrence Data\") +\n  labs(x = \"Longitude\", y = \"Latitude\")"},{"path":"maxent-bias-correction.html","id":"spatial-bias","chapter":"10 Maxent Bias Correction","heading":"10.0.3.2 2. Spatial Bias","text":"Spatial bias happens specific environmental conditions - -represented occurrence data.Causes:\n- species primarily recorded habitats easier identify.\n- environmental conditions sampled thoroughly.Examples:\n- -representation lowland areas mountain habitats overlooked.\n- Sampling effort focused one biome (e.g., forests), ignoring potential habitats (e.g., grasslands).Watch : Spatial bias can lead misleading response curves, model falsely associates species specific conditions.Detect Spatial Bias:\n- Overlay occurrence points environmental variables (e.g., elevation, precipitation) check uneven representation.","code":""},{"path":"maxent-bias-correction.html","id":"data-bias","chapter":"10 Maxent Bias Correction","heading":"10.0.3.3 3. Data Bias","text":"Data bias results errors inconsistencies occurrence environmental datasets.Causes:\n- Misidentified species occurrence records.\n- Environmental predictors low resolution missing values.\n- Temporal mismatch occurrence environmental data.Examples:\n- Occurrence data collected decades ago may reflect current distributions.\n- Predictors like temperature precipitation may gaps inconsistencies.Pro Tip: Check errors occurrence environmental data modeling. Use functions like CoordinateCleaner R remove problematic records.Tools Address Data Bias:\n- Use GBIF similar platforms clean occurrence data.\n- Validate environmental layers inspecting resolution, extent, coordinate systems.","code":"\n# Example of checking predictor quality in R\nlibrary(raster)\nplot(predictor_layer, main = \"Inspecting Environmental Data Quality\")"},{"path":"maxent-bias-correction.html","id":"summary-3","chapter":"10 Maxent Bias Correction","heading":"10.0.3.4 Summary","text":"Recognizing biases early allows effective correction methods, ensuring models provide accurate ecologically meaningful predictions.","code":""},{"path":"maxent-bias-correction.html","id":"spatial-thinning","chapter":"10 Maxent Bias Correction","heading":"10.0.3.5 1. Spatial Thinning","text":"Definition: Spatial thinning process reducing occurrence points ensure uniform spatial distribution, minimizing -representation certain areas.works:\n- Removes closely clustered occurrence points within specified distance threshold.\n- Ensures occurrences evenly spaced.Tools:\n- spThin: thinning occurrence data spatially.\n- CoordinateCleaner: removing duplicate erroneous coordinates.Code Example:","code":"\nlibrary(spThin)\n\n# Thin occurrence data to a minimum distance of 10 km\nthinned_data <- thin(loc.data = occurrence_data,\n                     lat.col = \"lat\", lon.col = \"lon\",\n                     spec.col = \"species\", thin.par = 10,\n                     reps = 1, verbose = TRUE)\n\n# View the thinned dataset\nhead(thinned_data)"},{"path":"maxent-bias-correction.html","id":"target-group-background-sampling","chapter":"10 Maxent Bias Correction","heading":"10.0.3.6 2. Target-Group Background Sampling","text":"Definition: Selects background points based species similar sampling biases ensure realistic comparisons.works:\n- Identifies areas similar species sampled selects background points within regions.\n- Improves ecological relevance background points.Implementation MaxEnt:\n- Use set occurrence records ecologically similar species define sampling area background points.Code Example:","code":"\nlibrary(dismo)\n\n# Generate target-group background points\nbg_points <- randomPoints(predictors, n = 500, ext = target_extent)\n\n# Visualize background points\nplot(predictors[[1]], main = \"Target-Group Background Points\")\npoints(bg_points, col = \"blue\", pch = 20)"},{"path":"maxent-bias-correction.html","id":"bias-files","chapter":"10 Maxent Bias Correction","heading":"10.0.3.7 3. Bias Files","text":"Definition: Raster layers weight likelihood background points, based known sampling intensity bias patterns.works:\n- Kernel density estimation used create bias layer.\n- bias file included MaxEnt modeling guide background selection.Steps Create Bias File:\n1. Generate kernel density raster using occurrence data.\n2. Normalize raster values scale 0–1.\n3. Use bias raster input MaxEnt.Code Example:","code":"\nlibrary(raster)\nlibrary(adehabitatHR)\n\n# Create a kernel density estimate\ncoords <- occurrence_data[, c(\"lon\", \"lat\")]\nbias_layer <- kernelUD(coords, h = \"href\", grid = 100)\n\n# Convert to raster and normalize\nbias_raster <- raster(bias_layer)\nbias_raster <- bias_raster / max(values(bias_raster), na.rm = TRUE)\n\n# Visualize the bias file\nplot(bias_raster, main = \"Bias File\")"},{"path":"maxent-bias-correction.html","id":"environmental-filters","chapter":"10 Maxent Bias Correction","heading":"10.0.3.8 4. Environmental Filters","text":"Definition: Reducing extent environmental predictors focus areas biologically relevant species.works:\n- Crops predictor layers specific geographic extents regions interest.\n- Avoids including irrelevant outlier areas modeling.Code Example:","code":"\nlibrary(raster)\n\n# Crop environmental layers to a specific extent\nextent_filter <- extent(-100, -50, -30, 20)  # Define the geographic region\nfiltered_predictors <- crop(predictors, extent_filter)\n\n# Visualize filtered layers\nplot(filtered_predictors[[1]], main = \"Filtered Environmental Layer\")"},{"path":"maxent-bias-correction.html","id":"practical-application-bias-correction-in-r","chapter":"10 Maxent Bias Correction","heading":"10.0.4 4. Practical Application: Bias Correction in R","text":"","code":""},{"path":"maxent-bias-correction.html","id":"step-1-spatial-thinning","chapter":"10 Maxent Bias Correction","heading":"10.0.4.1 Step 1: Spatial Thinning","text":"Load occurrence data.Use spThin package thin data spatially.","code":""},{"path":"maxent-bias-correction.html","id":"step-2-generate-a-bias-file","chapter":"10 Maxent Bias Correction","heading":"10.0.4.2 Step 2: Generate a Bias File","text":"Create kernel density raster using occurrence points.Normalize raster scale 0–1.Use raster bias file MaxEnt.","code":""},{"path":"maxent-bias-correction.html","id":"step-3-apply-target-group-background-sampling","chapter":"10 Maxent Bias Correction","heading":"10.0.4.3 Step 3: Apply Target-Group Background Sampling","text":"Generate background points constrained regions similar species occurrence.Incorporate background points MaxEnt model.","code":""},{"path":"maxent-bias-correction.html","id":"step-4-filter-environmental-predictors","chapter":"10 Maxent Bias Correction","heading":"10.0.4.4 Step 4: Filter Environmental Predictors","text":"Define geographic extent based species’ range study area.Crop environmental layers defined extent.","code":""},{"path":"maxent-bias-correction.html","id":"code-workflow-example","chapter":"10 Maxent Bias Correction","heading":"10.0.4.5 Code Workflow Example:","text":"applying bias correction techniques, essential evaluate model improved whether changes align ecological expectations.","code":"\n# Step 1: Spatial Thinning\nthinned_data <- thin(loc.data = occurrence_data, lat.col = \"lat\", lon.col = \"lon\",\n                     spec.col = \"species\", thin.par = 10)\n\n# Step 2: Create Bias File\ncoords <- occurrence_data[, c(\"lon\", \"lat\")]\nbias_layer <- kernelUD(coords, h = \"href\", grid = 100)\nbias_raster <- raster(bias_layer) / max(values(bias_layer), na.rm = TRUE)\n\n# Step 3: Generate Target-Group Background Points\nbg_points <- randomPoints(predictors, n = 500, ext = extent(-80, -60, -40, 10))\n\n# Step 4: Filter Predictors\nfiltered_predictors <- crop(predictors, extent(-80, -60, -40, 10))"},{"path":"maxent-bias-correction.html","id":"comparison-of-metrics","chapter":"10 Maxent Bias Correction","heading":"10.0.4.6 1. Comparison of Metrics","text":"Metrics allow us assess whether bias correction led better model performance. key metrics compare bias correction include:","code":""},{"path":"maxent-bias-correction.html","id":"code-example-compare-auc-before-and-after-bias-correction","chapter":"10 Maxent Bias Correction","heading":"10.0.4.6.1 Code Example: Compare AUC Before and After Bias Correction","text":"Look ?\n- AUC TSS increase bias correction, model improved.\n- response curves biologically meaningful, correction effective.","code":"\nlibrary(pROC)\n\n# Evaluate the model BEFORE bias correction\nauc_before <- auc(roc(labels_before, predictions_before))\n\n# Evaluate the model AFTER bias correction\nauc_after <- auc(roc(labels_after, predictions_after))\n\n# Print the AUC scores for comparison\nprint(paste(\"AUC Before Correction:\", auc_before))\nprint(paste(\"AUC After Correction:\", auc_after))"},{"path":"maxent-bias-correction.html","id":"visual-inspection","chapter":"10 Maxent Bias Correction","heading":"10.0.4.7 2. Visual Inspection","text":"Suitability maps allow side--side comparison identify changes predicted distributions.","code":""},{"path":"maxent-bias-correction.html","id":"code-example-compare-suitability-maps-before-and-after-bias-correction","chapter":"10 Maxent Bias Correction","heading":"10.0.4.7.1 Code Example: Compare Suitability Maps Before and After Bias Correction","text":"Common Observations:\n- bias-corrected map removes artificial clustering, correction successful.\n- high-suitability areas shift toward biologically realistic regions, model improved.","code":"\n# Plot suitability maps before and after correction\npar(mfrow = c(1,2))  # Arrange plots side-by-side\n\nplot(suitability_map_before, main = \"Before Bias Correction\")\nplot(suitability_map_after, main = \"After Bias Correction\")"},{"path":"maxent-bias-correction.html","id":"ecological-plausibility-1","chapter":"10 Maxent Bias Correction","heading":"10.0.4.8 3. Ecological Plausibility","text":"final check compare model’s predictions known habitat preferences species.Questions Ask:\n- model predict suitable habitats species known occur?\n- removed artificial hotspots caused sampling bias?\n- predictions ecologically reasonable (e.g., placing freshwater species deserts)?","code":""},{"path":"maxent-bias-correction.html","id":"common-pitfalls-in-bias-correction","chapter":"10 Maxent Bias Correction","heading":"10.0.5 6. Common Pitfalls in Bias Correction","text":"bias correction improves models, mistakes can lead worse predictions. common pitfalls avoid .","code":""},{"path":"maxent-bias-correction.html","id":"over-thinning-leading-to-loss-of-valuable-data","chapter":"10 Maxent Bias Correction","heading":"10.0.5.1 1. Over-Thinning Leading to Loss of Valuable Data","text":"Problem: thinning reduces occurrence points much, important information lost.Solution: Use appropriate threshold (e.g., 10 km rather extreme distances like 50 km).","code":""},{"path":"maxent-bias-correction.html","id":"code-example-adjust-thinning-distance","chapter":"10 Maxent Bias Correction","heading":"10.0.5.1.1 Code Example: Adjust Thinning Distance","text":"","code":"\nthinned_data <- thin(occurrence_data, lat.col = \"lat\", lon.col = \"lon\",\n                     spec.col = \"species\", thin.par = 10)  # Use a reasonable 10 km threshold"},{"path":"maxent-bias-correction.html","id":"inappropriate-use-of-bias-layers","chapter":"10 Maxent Bias Correction","heading":"10.0.5.2 2. Inappropriate Use of Bias Layers","text":"Problem: poorly constructed bias layer may -correct remove real ecological signals.Solution: Ensure bias layers reflect actual sampling effort, just presence density.","code":""},{"path":"maxent-bias-correction.html","id":"code-example-normalize-bias-raster-properly","chapter":"10 Maxent Bias Correction","heading":"10.0.5.2.1 Code Example: Normalize Bias Raster Properly","text":"","code":"\nbias_raster <- bias_raster / max(values(bias_raster), na.rm = TRUE)  # Normalize to 0-1 scale"},{"path":"maxent-bias-correction.html","id":"neglecting-ecological-relevance-during-correction","chapter":"10 Maxent Bias Correction","heading":"10.0.5.3 3. Neglecting Ecological Relevance During Correction","text":"Problem: Removing bias without considering species ecology can lead misleading results.Solution: Always compare corrected predictions known habitat preferences.Best Practice: Use literature expert knowledge verify bias-corrected predictions make biological sense.","code":""},{"path":"maxent-bias-correction.html","id":"summary-and-best-practices","chapter":"10 Maxent Bias Correction","heading":"10.0.6 7. Summary and Best Practices","text":"Bias correction crucial step ensuring accurate ecologically valid species distribution models.","code":""},{"path":"maxent-bias-correction.html","id":"key-takeaways-for-bias-correction","chapter":"10 Maxent Bias Correction","heading":"10.0.6.1 Key Takeaways for Bias Correction","text":"","code":""},{"path":"maxent-bias-correction.html","id":"checklist-for-implementing-bias-correction-in-maxent","chapter":"10 Maxent Bias Correction","heading":"10.0.6.2 Checklist for Implementing Bias Correction in MaxEnt","text":"✅ Check clustering occurrence points.\n✅ Apply spatial thinning remove -represented areas.\n✅ Generate bias file weight background selection.\n✅ Compare suitability maps correction.\n✅ Ensure ecological validity corrected predictions.","code":""},{"path":"maxent-bias-correction.html","id":"by-following-these-best-practices-you-can-ensure-your-maxent-model-provides-realistic-unbiased-predictions-that-contribute-meaningfully-to-ecological-research-and-conservation.","chapter":"10 Maxent Bias Correction","heading":"10.1 By following these best practices, you can ensure your MaxEnt model provides realistic, unbiased predictions that contribute meaningfully to ecological research and conservation. 🚀","text":"","code":""},{"path":"cart-rf-and-gbm.html","id":"cart-rf-and-gbm","chapter":"11 Cart, RF, and GBM","heading":"11 Cart, RF, and GBM","text":"","code":""},{"path":"cart-rf-and-gbm.html","id":"introduction-to-tree-based-models","chapter":"11 Cart, RF, and GBM","heading":"11.1 1. Introduction to Tree-Based Models","text":"","code":""},{"path":"cart-rf-and-gbm.html","id":"overview-of-tree-based-machine-learning-models-in-sdm","chapter":"11 Cart, RF, and GBM","heading":"11.1.1 Overview of Tree-Based Machine Learning Models in SDM","text":"Tree-based models widely used Species Distribution Modeling (SDM) due ability handle complex, non-linear relationships species occurrences environmental variables. models split data hierarchical structures, making predictions based decision rules.three commonly used tree-based models SDM :CART (Classification Regression Trees) – simple decision tree model.Random Forest (RF) – ensemble multiple decision trees.Gradient Boosting Machine (GBM) – boosting technique builds trees sequentially.models help answer key ecological questions like:\n- environmental factors important determining species’ distribution?\n- species likely occur given environmental conditions?\n- environmental changes impact species distributions?","code":""},{"path":"cart-rf-and-gbm.html","id":"when-to-use-cart-rf-and-gbm","chapter":"11 Cart, RF, and GBM","heading":"11.1.2 When to Use CART, RF, and GBM?","text":"model unique advantages trade-offs, making useful different scenarios:","code":""},{"path":"cart-rf-and-gbm.html","id":"strengths-and-weaknesses-of-each-model","chapter":"11 Cart, RF, and GBM","heading":"11.1.3 Strengths and Weaknesses of Each Model","text":"Pro Tip: need simple, explainable model, start CART. need better accuracy, use Random Forest. best predictive power, go GBM.","code":""},{"path":"cart-rf-and-gbm.html","id":"next-steps-2","chapter":"11 Cart, RF, and GBM","heading":"11.1.4 Next Steps","text":"following sections, explore CART, RF, GBM detail, covering:\n- Theoretical foundations (work).\n- Coding demonstrations (practical implementation R using real datasets).","code":""},{"path":"cart-rf-and-gbm.html","id":"a.-theory-explanation","chapter":"11 Cart, RF, and GBM","heading":"11.1.5 A. Theory Explanation","text":"","code":""},{"path":"cart-rf-and-gbm.html","id":"what-is-cart","chapter":"11 Cart, RF, and GBM","heading":"11.1.5.1 What is CART?","text":"CART (Classification Regression Trees) simple yet powerful machine learning algorithm makes predictions splitting data smaller subsets based decision rules. model structured like tree:Nodes represent decision points based predictor variables.Branches connect decision points possible outcomes.Leaves represent final predictions (species presence/absence suitability scores).","code":""},{"path":"cart-rf-and-gbm.html","id":"how-does-cart-work","chapter":"11 Cart, RF, and GBM","heading":"11.1.5.2 How Does CART Work?","text":"Recursive Partitioning:\nalgorithm selects predictor variable splitting threshold (e.g., temperature > 15°C).\ndivides data two groups: one meets condition one doesn’t.\nprocess repeats group stopping rule met (e.g., minimum number observations per node).\nalgorithm selects predictor variable splitting threshold (e.g., temperature > 15°C).divides data two groups: one meets condition one doesn’t.process repeats group stopping rule met (e.g., minimum number observations per node).Prediction:\nmodeling presence/absence, tree assigns class (0 1).\nmodeling species suitability, tree assigns probability numerical score.\nmodeling presence/absence, tree assigns class (0 1).modeling species suitability, tree assigns probability numerical score.","code":""},{"path":"cart-rf-and-gbm.html","id":"advantages-of-cart","chapter":"11 Cart, RF, and GBM","heading":"11.1.5.3 Advantages of CART","text":"✅ Simple Interpretable – Easy understand visualize.\n✅ Handles Categorical Continuous Data – Works numerical factor variables.\n✅ Captures Nonlinear Relationships – Unlike traditional regression, can model complex relationships.","code":""},{"path":"cart-rf-and-gbm.html","id":"limitations-of-cart","chapter":"11 Cart, RF, and GBM","heading":"11.1.5.4 Limitations of CART","text":"⚠️ Prone Overfitting – Trees may grow complex fit noise data.\n⚠️ Lower Accuracy Compared RF & GBM – Ensemble methods like Random Forest Gradient Boosting often outperform single decision trees.\n⚠️ Sensitive Small Data Changes – Slight variations data can lead different tree structure.Key Takeaway:\nCART great quick insights can unstable prone overfitting. Random Forest (RF) Gradient Boosting (GBM) improve upon CART combining multiple trees.","code":""},{"path":"cart-rf-and-gbm.html","id":"b.-coding-demonstration-cart-in-r","chapter":"11 Cart, RF, and GBM","heading":"11.1.6 B. Coding Demonstration: CART in R","text":"Let’s fit CART model using rpart package visualize tree.","code":""},{"path":"cart-rf-and-gbm.html","id":"step-1-load-libraries","chapter":"11 Cart, RF, and GBM","heading":"11.1.6.1 Step 1: Load Libraries","text":"","code":"\n# Load necessary libraries\nlibrary(rpart)      # CART model\nlibrary(rpart.plot) # Visualizing trees\nlibrary(dismo)      # SDM-related datasets\nlibrary(caret)      # Model evaluation"},{"path":"cart-rf-and-gbm.html","id":"step-2-load-a-built-in-dataset","chapter":"11 Cart, RF, and GBM","heading":"11.1.6.2 Step 2: Load a Built-in Dataset","text":"use bioclim dataset dismo package, contains species presence-absence data along environmental predictors.","code":"\n# Load example SDM dataset\ndata <- dismo::bioclim\n\n# View structure of the dataset\nstr(data)\n\n# Define presence/absence variable\ndata$presence <- as.factor(data$presence)  # Convert to factor for classification"},{"path":"cart-rf-and-gbm.html","id":"step-3-split-data-into-training-and-testing-sets","chapter":"11 Cart, RF, and GBM","heading":"11.1.6.3 Step 3: Split Data into Training and Testing Sets","text":"","code":"\n# Set seed for reproducibility\nset.seed(123)\n\n# Split data into training (70%) and testing (30%)\ntrainIndex <- createDataPartition(data$presence, p = 0.7, list = FALSE)\ntrain_data <- data[trainIndex, ]\ntest_data  <- data[-trainIndex, ]"},{"path":"cart-rf-and-gbm.html","id":"step-4-fit-a-cart-model","chapter":"11 Cart, RF, and GBM","heading":"11.1.6.4 Step 4: Fit a CART Model","text":"","code":"\n# Train a CART model\ncart_model <- rpart(presence ~ bio1 + bio12 + bio5 + bio6, \n                     data = train_data, \n                     method = \"class\",  # Classification task\n                     control = rpart.control(minsplit = 10))\n\n# Print tree summary\nprint(cart_model)"},{"path":"cart-rf-and-gbm.html","id":"step-5-visualize-the-decision-tree","chapter":"11 Cart, RF, and GBM","heading":"11.1.6.5 Step 5: Visualize the Decision Tree","text":"","code":"\n# Plot the decision tree\nrpart.plot(cart_model, main = \"CART Decision Tree for Species Presence\")"},{"path":"cart-rf-and-gbm.html","id":"step-6-evaluate-model-performance-1","chapter":"11 Cart, RF, and GBM","heading":"11.1.6.6 Step 6: Evaluate Model Performance","text":"","code":"\n# Make predictions on test data\npredictions <- predict(cart_model, test_data, type = \"class\")\n\n# Generate confusion matrix\nconfusionMatrix(predictions, test_data$presence)"},{"path":"cart-rf-and-gbm.html","id":"key-observations","chapter":"11 Cart, RF, and GBM","heading":"11.1.7 Key Observations","text":"decision tree identifies key environmental thresholds influence species presence.confusion matrix evaluates accuracy, sensitivity, specificity.Next Step: Improve model accuracy using Random Forest (RF) reduce overfitting.","code":""},{"path":"cart-rf-and-gbm.html","id":"summary-4","chapter":"11 Cart, RF, and GBM","heading":"11.1.8 Summary","text":"✅ CART simple interpretable can overfit data.\n✅ helps identify key environmental factors affecting species presence.\n✅ better accuracy, ensemble methods like RF GBM preferred.","code":""},{"path":"cart-rf-and-gbm.html","id":"a.-theory-explanation-1","chapter":"11 Cart, RF, and GBM","heading":"11.1.9 A. Theory Explanation","text":"","code":""},{"path":"cart-rf-and-gbm.html","id":"what-is-random-forest","chapter":"11 Cart, RF, and GBM","heading":"11.1.9.1 What is Random Forest?","text":"Random Forest (RF) ensemble learning method builds multiple decision trees combines predictions improve accuracy. Unlike single decision tree (CART), Random Forest:Reduces overfitting averaging multiple tree predictions.Randomly selects features split, preventing single predictor dominating model.Think team decision trees, tree gives “vote,” forest decides based majority vote.","code":""},{"path":"cart-rf-and-gbm.html","id":"how-does-random-forest-work","chapter":"11 Cart, RF, and GBM","heading":"11.1.10 How Does Random Forest Work?","text":"Bootstrap Sampling:\nmodel randomly selects subsets data (replacement) train tree.\nmodel randomly selects subsets data (replacement) train tree.Feature Randomization:\ntree split, random subset predictors considered prevent overfitting.\ntree split, random subset predictors considered prevent overfitting.Voting Final Prediction:\ntree makes prediction, majority vote determines final output.\ntree makes prediction, majority vote determines final output.","code":""},{"path":"cart-rf-and-gbm.html","id":"advantages-of-random-forest","chapter":"11 Cart, RF, and GBM","heading":"11.1.11 Advantages of Random Forest","text":"✅ Higher accuracy CART – Reduces overfitting averaging multiple trees.\n✅ Handles large datasets well – Works many predictors interactions.\n✅ Captures complex relationships – Suitable non-linear species-environment relationships.","code":""},{"path":"cart-rf-and-gbm.html","id":"limitations-of-random-forest","chapter":"11 Cart, RF, and GBM","heading":"11.1.12 Limitations of Random Forest","text":"⚠️ Computationally expensive – Requires processing power single decision tree.\n⚠️ Less interpretable CART – Harder visualize decision rules.\n⚠️ Tuning needed – number trees feature selection must optimized.Use RF?\n- CART overfitting needs generalization.\n- working large datasets many predictors.\n- interactions variables complex.","code":""},{"path":"cart-rf-and-gbm.html","id":"b.-coding-demonstration-random-forest-in-r","chapter":"11 Cart, RF, and GBM","heading":"11.1.13 B. Coding Demonstration: Random Forest in R","text":"Let’s build Random Forest model using randomForest package.","code":""},{"path":"cart-rf-and-gbm.html","id":"step-1-load-libraries-1","chapter":"11 Cart, RF, and GBM","heading":"11.1.13.1 Step 1: Load Libraries","text":"","code":"\n# Load necessary libraries\nlibrary(randomForest)  # For building RF models\nlibrary(dismo)         # For SDM datasets\nlibrary(caret)         # For model evaluation"},{"path":"cart-rf-and-gbm.html","id":"step-2-load-and-prepare-data","chapter":"11 Cart, RF, and GBM","heading":"11.1.13.2 Step 2: Load and Prepare Data","text":"use bioclim dataset dismo, contains species presence-absence data environmental predictors.","code":"\n# Load example dataset\ndata <- dismo::bioclim\n\n# Convert species presence into a factor (classification task)\ndata$presence <- as.factor(data$presence)\n\n# View dataset structure\nstr(data)"},{"path":"cart-rf-and-gbm.html","id":"step-3-split-data-into-training-and-testing-sets-1","chapter":"11 Cart, RF, and GBM","heading":"11.1.13.3 Step 3: Split Data into Training and Testing Sets","text":"split data 70% training 30% testing.","code":"\n# Set seed for reproducibility\nset.seed(123)\n\n# Create training and testing sets\ntrainIndex <- createDataPartition(data$presence, p = 0.7, list = FALSE)\ntrain_data <- data[trainIndex, ]\ntest_data  <- data[-trainIndex, ]"},{"path":"cart-rf-and-gbm.html","id":"step-4-fit-a-random-forest-model","chapter":"11 Cart, RF, and GBM","heading":"11.1.13.4 Step 4: Fit a Random Forest Model","text":"Now, train Random Forest model predict species presence.","code":"\n# Train a Random Forest model\nrf_model <- randomForest(presence ~ bio1 + bio12 + bio5 + bio6, \n                         data = train_data, ntree = 500, \n                         importance = TRUE)\n\n# View model summary\nprint(rf_model)"},{"path":"cart-rf-and-gbm.html","id":"step-5-evaluate-feature-importance","chapter":"11 Cart, RF, and GBM","heading":"11.1.13.5 Step 5: Evaluate Feature Importance","text":"Random Forest provides feature importance scores, showing environmental variables influence predictions .Interpretation:\n- Higher values indicate stronger predictors.\n- Temperature precipitation often key factors species distribution models.","code":"\n# Plot variable importance\nvarImpPlot(rf_model, main = \"Variable Importance in Random Forest\")"},{"path":"cart-rf-and-gbm.html","id":"step-6-compare-rf-accuracy-with-cart","chapter":"11 Cart, RF, and GBM","heading":"11.1.13.6 Step 6: Compare RF Accuracy with CART","text":"Now, compare accuracy CART vs. Random Forest.also trained CART model, compare accuracy scores:","code":"\n# Predict species presence on test data\nrf_predictions <- predict(rf_model, test_data)\n\n# Compute confusion matrix\nconf_matrix_rf <- confusionMatrix(rf_predictions, test_data$presence)\n\n# Print accuracy\nprint(conf_matrix_rf$overall[\"Accuracy\"])\nprint(conf_matrix_cart$overall[\"Accuracy\"])  # Accuracy of CART\nprint(conf_matrix_rf$overall[\"Accuracy\"])    # Accuracy of Random Forest"},{"path":"cart-rf-and-gbm.html","id":"key-observations-1","chapter":"11 Cart, RF, and GBM","heading":"11.1.14 Key Observations","text":"RF higher accuracy CART.Important predictors ranked based contribution model.trees (ntree = 500) generally improve model performance.","code":""},{"path":"cart-rf-and-gbm.html","id":"summary-5","chapter":"11 Cart, RF, and GBM","heading":"11.1.15 Summary","text":"✅ Random Forest accurate robust CART.\n✅ Feature importance helps explain environmental factors matter.\n✅ Works well SDM less interpretable single decision tree.","code":""},{"path":"cart-rf-and-gbm.html","id":"a.-theory-explanation-2","chapter":"11 Cart, RF, and GBM","heading":"11.1.16 A. Theory Explanation","text":"","code":""},{"path":"cart-rf-and-gbm.html","id":"what-is-gbm","chapter":"11 Cart, RF, and GBM","heading":"11.1.16.1 What is GBM?","text":"Gradient Boosting Machine (GBM) ensemble learning technique builds series decision trees sequentially. Unlike Random Forest (RF), builds trees independently, GBM focuses reducing errors iteratively learning mistakes made previous trees.","code":""},{"path":"cart-rf-and-gbm.html","id":"how-gbm-works","chapter":"11 Cart, RF, and GBM","heading":"11.1.16.2 How GBM Works","text":"Starts weak model (e.g., small decision tree).Builds new trees step step, one improving previous model focusing hard--predict cases.Combines trees final model reduces overall errors.GBM optimizes predictions minimizing loss function (e.g., classification error, mean squared error).","code":""},{"path":"cart-rf-and-gbm.html","id":"advantages-of-gbm","chapter":"11 Cart, RF, and GBM","heading":"11.1.17 Advantages of GBM","text":"✅ Higher predictive accuracy CART RF – Learns past mistakes adjusts iteratively.\n✅ Handles missing data well – Can work incomplete datasets without imputation.\n✅ Works well small datasets – Unlike RF, require large amounts data.","code":""},{"path":"cart-rf-and-gbm.html","id":"limitations-of-gbm","chapter":"11 Cart, RF, and GBM","heading":"11.1.18 Limitations of GBM","text":"⚠️ Requires careful tuning – Parameters like learning rate number trees must optimized.\n⚠️ Computationally expensive – Slower training compared RF, especially large datasets.\n⚠️ Can overfit – many trees added without regularization, may learn noise.Use GBM?\n- higher accuracy needed RF CART can provide.\n- working small datasets RF might perform well.\n- okay tuning model hyperparameters better results.","code":""},{"path":"cart-rf-and-gbm.html","id":"b.-coding-demonstration-gbm-in-r","chapter":"11 Cart, RF, and GBM","heading":"11.1.19 B. Coding Demonstration: GBM in R","text":"now train GBM model using gbm package compare accuracy CART RF.","code":""},{"path":"cart-rf-and-gbm.html","id":"step-1-load-libraries-2","chapter":"11 Cart, RF, and GBM","heading":"11.1.19.1 Step 1: Load Libraries","text":"","code":"\n# Load necessary libraries\nlibrary(gbm)          # For Gradient Boosting Machine\nlibrary(dismo)        # For SDM datasets\nlibrary(caret)        # For model evaluation"},{"path":"cart-rf-and-gbm.html","id":"step-2-load-and-prepare-data-1","chapter":"11 Cart, RF, and GBM","heading":"11.1.19.2 Step 2: Load and Prepare Data","text":"use bioclim dataset, contains species presence-absence data environmental predictors.","code":"\n# Load example dataset\ndata <- dismo::bioclim\n\n# Convert species presence to a factor (classification task)\ndata$presence <- as.factor(data$presence)\n\n# View dataset structure\nstr(data)"},{"path":"cart-rf-and-gbm.html","id":"step-3-split-data-into-training-and-testing-sets-2","chapter":"11 Cart, RF, and GBM","heading":"11.1.19.3 Step 3: Split Data into Training and Testing Sets","text":"","code":"\n# Set seed for reproducibility\nset.seed(123)\n\n# Create training (70%) and testing (30%) sets\ntrainIndex <- createDataPartition(data$presence, p = 0.7, list = FALSE)\ntrain_data <- data[trainIndex, ]\ntest_data  <- data[-trainIndex, ]"},{"path":"cart-rf-and-gbm.html","id":"step-4-train-a-gbm-model","chapter":"11 Cart, RF, and GBM","heading":"11.1.19.4 Step 4: Train a GBM Model","text":"GBM requires setting hyperparameters, including:\n- Number trees (n.trees) – trees improve performance increase computation.\n- Learning rate (shrinkage) – Controls much tree contributes final prediction.\n- Tree depth (interaction.depth) – Controls complexity individual trees.","code":"\n# Train a GBM model\ngbm_model <- gbm(presence ~ bio1 + bio12 + bio5 + bio6, \n                 data = train_data,\n                 distribution = \"bernoulli\",  # For classification\n                 n.trees = 500,  \n                 shrinkage = 0.01,  \n                 interaction.depth = 3,  \n                 cv.folds = 5)  # Cross-validation to prevent overfitting\n\n# Print model summary\nsummary(gbm_model)"},{"path":"cart-rf-and-gbm.html","id":"step-5-tune-hyperparameters-for-optimal-performance","chapter":"11 Cart, RF, and GBM","heading":"11.1.19.5 Step 5: Tune Hyperparameters for Optimal Performance","text":"can optimize hyperparameters using cross-validation select best combination n.trees shrinkage.","code":"\n# Tune the GBM model by selecting the best number of trees\nbest_trees <- gbm.perf(gbm_model, method = \"cv\")\nprint(paste(\"Optimal number of trees:\", best_trees))"},{"path":"cart-rf-and-gbm.html","id":"step-6-compare-gbm-with-rf-and-cart","chapter":"11 Cart, RF, and GBM","heading":"11.1.19.6 Step 6: Compare GBM with RF and CART","text":"evaluate accuracy models compare performance.","code":"\n# Make predictions on test data\ngbm_predictions <- predict(gbm_model, test_data, n.trees = best_trees, type = \"response\")\ngbm_predictions_class <- ifelse(gbm_predictions > 0.5, \"1\", \"0\")\n\n# Generate confusion matrix for GBM\nconf_matrix_gbm <- confusionMatrix(as.factor(gbm_predictions_class), test_data$presence)\n\n# Print accuracy\nprint(conf_matrix_gbm$overall[\"Accuracy\"])"},{"path":"cart-rf-and-gbm.html","id":"final-model-comparison","chapter":"11 Cart, RF, and GBM","heading":"11.1.19.7 Final Model Comparison","text":"Let’s compare CART, RF, GBM checking accuracy scores.Expected Outcome:\n- CART: Lowest accuracy easy interpret.\n- RF: Better CART, optimized GBM.\n- GBM: Highest accuracy tuning hyperparameters.","code":"\nprint(conf_matrix_cart$overall[\"Accuracy\"])  # Accuracy of CART\nprint(conf_matrix_rf$overall[\"Accuracy\"])    # Accuracy of Random Forest\nprint(conf_matrix_gbm$overall[\"Accuracy\"])   # Accuracy of GBM"},{"path":"cart-rf-and-gbm.html","id":"key-observations-2","chapter":"11 Cart, RF, and GBM","heading":"11.1.20 Key Observations","text":"✅ GBM accurate three models.\n✅ requires tuning, results improve optimized settings.\n✅ Random Forest good balance accuracy computation.\n✅ CART useful interpretability important accuracy.","code":""},{"path":"cart-rf-and-gbm.html","id":"summary-6","chapter":"11 Cart, RF, and GBM","heading":"11.1.21 Summary","text":"✅ Use CART need explainable decision rules.\n✅ Use Random Forest need high accuracy minimal tuning.\n✅ Use GBM need maximum predictive power can afford tuning.Next, compare three models using real-world SDM applications! 🚀","code":""},{"path":"cart-rf-and-gbm.html","id":"how-do-cart-rf-and-gbm-compare","chapter":"11 Cart, RF, and GBM","heading":"11.1.22 How Do CART, RF, and GBM Compare?","text":"Now ’ve trained Classification Regression Trees (CART), Random Forest (RF), Gradient Boosting Machine (GBM), ’s time evaluate performance using AUC (Area Curve), accuracy, feature importance.model strengths weaknesses, determine best one different Species Distribution Modeling (SDM) scenarios.","code":""},{"path":"cart-rf-and-gbm.html","id":"step-1-evaluate-model-performance-with-auc-and-accuracy","chapter":"11 Cart, RF, and GBM","heading":"11.1.23 Step 1: Evaluate Model Performance with AUC and Accuracy","text":"","code":""},{"path":"cart-rf-and-gbm.html","id":"calculate-auc-for-each-model","chapter":"11 Cart, RF, and GBM","heading":"11.1.23.1 Calculate AUC for Each Model","text":"AUC (Area Curve) metric tells us well model distinguishes presence vs. absence points. higher AUC means better performance.✅ Expected Results:\n- CART → Lowest AUC (simpler model, overfitting).\n- RF → Better AUC (reduces overfitting multiple trees).\n- GBM → Best AUC (iterative learning improves accuracy).","code":"\nlibrary(pROC)\n\n# Compute AUC for CART\ncart_pred <- predict(cart_model, test_data, type = \"prob\")[,2]\ncart_auc <- auc(roc(test_data$presence, cart_pred))\n\n# Compute AUC for Random Forest\nrf_pred <- predict(rf_model, test_data, type = \"prob\")[,2]\nrf_auc <- auc(roc(test_data$presence, rf_pred))\n\n# Compute AUC for GBM\ngbm_pred <- predict(gbm_model, test_data, n.trees = best_trees, type = \"response\")\ngbm_auc <- auc(roc(test_data$presence, gbm_pred))\n\n# Print AUC Scores\nprint(paste(\"CART AUC:\", cart_auc))\nprint(paste(\"Random Forest AUC:\", rf_auc))\nprint(paste(\"GBM AUC:\", gbm_auc))"},{"path":"cart-rf-and-gbm.html","id":"compare-accuracy-across-models","chapter":"11 Cart, RF, and GBM","heading":"11.1.23.2 Compare Accuracy Across Models","text":"✅ Expected Outcome: GBM highest accuracy, followed Random Forest, CART lowest accuracy.","code":"\n# Print accuracy for CART, RF, and GBM\nprint(conf_matrix_cart$overall[\"Accuracy\"])  # CART\nprint(conf_matrix_rf$overall[\"Accuracy\"])    # Random Forest\nprint(conf_matrix_gbm$overall[\"Accuracy\"])   # GBM"},{"path":"cart-rf-and-gbm.html","id":"step-2-compare-feature-importance","chapter":"11 Cart, RF, and GBM","heading":"11.1.24 Step 2: Compare Feature Importance","text":"Feature importance helps us understand environmental variables influential predicting species presence.✅ Look ?\n- temperature (bio1) highly important, suggests species’ distribution temperature-sensitive.\n- precipitation (bio12) important, indicates reliance moisture.\n- GBM RF rank similar variables highly, CART may fewer features.","code":"\n# Plot feature importance for Random Forest\nvarImpPlot(rf_model, main = \"Feature Importance - Random Forest\")\n\n# Feature importance for GBM\nsummary(gbm_model)"},{"path":"cart-rf-and-gbm.html","id":"step-3-visualizing-model-performance","chapter":"11 Cart, RF, and GBM","heading":"11.1.25 Step 3: Visualizing Model Performance","text":"compare model outputs, can plot ROC curves three models.✅ Expected Observations:\n- green line (GBM) highest curve (best performance).\n- blue line (RF) better CART (red).","code":"\n# Plot ROC curves\nplot(roc(test_data$presence, cart_pred), col = \"red\", main = \"ROC Curve Comparison\")\nplot(roc(test_data$presence, rf_pred), col = \"blue\", add = TRUE)\nplot(roc(test_data$presence, gbm_pred), col = \"green\", add = TRUE)\nlegend(\"bottomright\", legend = c(\"CART\", \"Random Forest\", \"GBM\"), col = c(\"red\", \"blue\", \"green\"), lwd = 2)"},{"path":"cart-rf-and-gbm.html","id":"step-4-selecting-the-best-model-for-different-sdm-scenarios","chapter":"11 Cart, RF, and GBM","heading":"11.1.26 Step 4: Selecting the Best Model for Different SDM Scenarios","text":"✅ Takeaway:\n- Use CART interpretation.\n- Use Random Forest accuracy minimal tuning.\n- Use GBM best performance (computational resources allow).","code":""},{"path":"cart-rf-and-gbm.html","id":"final-thoughts","chapter":"11 Cart, RF, and GBM","heading":"11.1.27 Final Thoughts","text":"CART simple prone overfitting lower accuracy.Random Forest reduces overfitting, improves accuracy, great general-purpose SDM model.GBM provides best accuracy requires tuning computational power.","code":""},{"path":"gbm-variable-selection.html","id":"gbm-variable-selection","chapter":"12 GBM Variable Selection","heading":"12 GBM Variable Selection","text":"","code":""},{"path":"gbm-variable-selection.html","id":"introduction-to-variable-selection-in-gbm","chapter":"12 GBM Variable Selection","heading":"12.1 1. Introduction to Variable Selection in GBM","text":"","code":""},{"path":"gbm-variable-selection.html","id":"why-variable-selection-matters-in-gbm-for-sdm","chapter":"12 GBM Variable Selection","heading":"12.1.1 Why Variable Selection Matters in GBM for SDM","text":"Gradient Boosting Machines (GBM) powerful Species Distribution Modeling (SDM), can become computationally expensive prone overfitting many predictors included. Selecting important environmental variables improves:✅ Model Accuracy – Reduces noise irrelevant predictors.\n✅ Model Simplicity – Fewer variables make model easier interpret.\n✅ Faster Computation – Training predictions become efficient.\n✅ Better Generalization – model performs well unseen data.","code":""},{"path":"gbm-variable-selection.html","id":"challenges-of-using-too-many-predictors","chapter":"12 GBM Variable Selection","heading":"12.1.2 Challenges of Using Too Many Predictors","text":"GBM can handle many variables, predictors contribute equally model. Using many predictors leads :⚠️ Overfitting – model memorizes noise instead learning general patterns.\n⚠️ Longer Training Time – variables mean computations, slowing model fitting.\n⚠️ Difficult Interpretation – becomes harder explain model makes certain predictions.\n⚠️ Collinearity Issues – Highly correlated variables can distort model’s ability learn independent relationships.Example Poorly Selected Model\nImagine modeling distribution bird species 20 climate variables. 10 highly correlated, model might give redundant misleading predictions, overcomplicating interpretation.","code":""},{"path":"gbm-variable-selection.html","id":"goal-selecting-the-most-relevant-environmental-variables","chapter":"12 GBM Variable Selection","heading":"12.1.3 Goal: Selecting the Most Relevant Environmental Variables","text":"aim variable selection GBM :🔹 Identify predictors strongly influence species distribution.\n🔹 Remove weak redundant variables add noise.\n🔹 Ensure selected variables align ecological understanding.","code":""},{"path":"gbm-variable-selection.html","id":"whats-next","chapter":"12 GBM Variable Selection","heading":"12.1.4 What’s Next?","text":"next section, explore different methods selecting best variables GBM models. include:📌 Feature Importance Scores – Identify variables matter .\n📌 Recursive Feature Elimination (RFE) – Iteratively remove weakest variables.\n📌 Correlation Analysis – Avoid redundancy removing correlated predictors.\n📌 Cross-Validation-Based Selection – Keep variables improve test set performance.🚀 Let’s dive different selection methods!Selecting right predictors essential efficient accurate species distribution modeling (SDM) using Gradient Boosting Machines (GBM). five key methods used identify retain relevant environmental variables.","code":""},{"path":"gbm-variable-selection.html","id":"feature-importance-scores","chapter":"12 GBM Variable Selection","heading":"12.1.5 1. Feature Importance Scores","text":"GBM automatically assigns importance score variable based often used split data across decision trees.🔹 High-importance variables contribute significantly model accuracy.\n🔹 Low-importance variables can removed simplify model without reducing performance.use ?\n- Train initial GBM model using predictors.\n- Extract feature importance rankings remove weakest variables.Use ?\nUse feature importance first step applying variable selection methods.","code":""},{"path":"gbm-variable-selection.html","id":"recursive-feature-elimination-rfe","chapter":"12 GBM Variable Selection","heading":"12.1.6 2. Recursive Feature Elimination (RFE)","text":"Recursive Feature Elimination (RFE) iterative approach least important variables removed one one, model retrained time.🔹 Helps identify optimal number variables.\n🔹 Ensures weak predictors dilute model accuracy.use ?\n- Start predictors rank importance.\n- Remove least important variable retrain GBM model.\n- Repeat performance longer improves.Downside:\nRFE computationally expensive model retrained multiple times.","code":""},{"path":"gbm-variable-selection.html","id":"correlation-analysis","chapter":"12 GBM Variable Selection","heading":"12.1.7 3. Correlation Analysis","text":"Many environmental variables (e.g., temperature, precipitation) highly correlated, can distort GBM’s ability learn independent patterns.🔹 Goal: Identify remove redundant variables.\n🔹 Solution: Use correlation matrix Variance Inflation Factor (VIF) detect collinearity.use ?\n- Compute correlation coefficients predictors.\n- two variables highly correlated (r > 0.7), remove one.Example:\nBio1 (Annual Mean Temperature) Bio5 (Maximum Temperature Warmest Month) highly correlated, keep one.","code":""},{"path":"gbm-variable-selection.html","id":"aicbic-model-comparison","chapter":"12 GBM Variable Selection","heading":"12.1.8 4. AIC/BIC Model Comparison","text":"Model selection using Akaike Information Criterion (AIC) Bayesian Information Criterion (BIC) helps determine best predictor set based model complexity vs. accuracy.🔹 AIC favors models fewer predictors maintaining accuracy.\n🔹 BIC penalizes models many variables, ensuring simplicity.use ?\n- Fit GBM models different sets variables.\n- Compute AIC/BIC scores model.\n- Select model lowest AIC/BIC score.Best Practice:\nUse AIC/BIC alongside feature importance correlation analysis optimal variable selection.","code":""},{"path":"gbm-variable-selection.html","id":"cross-validation-based-selection","chapter":"12 GBM Variable Selection","heading":"12.1.9 5. Cross-Validation-Based Selection","text":"Cross-validation ensures variable selection improves real-world predictive performance, just training accuracy.🔹 Goal: Keep predictors improve test set performance.\n🔹 Method: Use AUC (Area Curve) Accuracy validation dataset.use ?\n- Train GBM model variables.\n- Remove predictor check AUC/accuracy decreases.\n- Keep variables consistently improve test set predictions.Warning:\nCross-validation can computationally expensive ensures final model robust.","code":""},{"path":"gbm-variable-selection.html","id":"summary-of-variable-selection-methods","chapter":"12 GBM Variable Selection","heading":"12.1.10 Summary of Variable Selection Methods","text":"","code":""},{"path":"gbm-variable-selection.html","id":"whats-next-1","chapter":"12 GBM Variable Selection","heading":"12.1.11 What’s Next?","text":"","code":""},{"path":"gbm-variable-selection.html","id":"now-that-we-understand-methods-for-variable-selection-we-will-move-to-the-coding-demonstration-applying-these-techniques-in-r-to-improve-a-gbm-based-species-distribution-model.","chapter":"12 GBM Variable Selection","heading":"12.2 Now that we understand methods for variable selection, we will move to the coding demonstration, applying these techniques in R to improve a GBM-based species distribution model. 🚀","text":"","code":""},{"path":"gbm-variable-selection.html","id":"coding-demonstration-variable-selection-in-gbm","chapter":"12 GBM Variable Selection","heading":"12.3 3. Coding Demonstration: Variable Selection in GBM","text":"coding exercise guide process selecting important variables GBM-based Species Distribution Model (SDM) R. use built-datasets packages ensure workflow reproducible.","code":""},{"path":"gbm-variable-selection.html","id":"step-1-load-an-sdm-dataset-with-multiple-environmental-variables","chapter":"12 GBM Variable Selection","heading":"12.3.1 Step 1: Load an SDM Dataset with Multiple Environmental Variables","text":"use bioclim dataset dismo package, contains species presence-absence data along environmental predictors.","code":""},{"path":"gbm-variable-selection.html","id":"load-necessary-libraries","chapter":"12 GBM Variable Selection","heading":"12.3.1.1 Load Necessary Libraries","text":"","code":"\n# Load required packages\nlibrary(gbm)          # Gradient Boosting Machine\nlibrary(dismo)        # SDM-related datasets\nlibrary(caret)        # Data partitioning and evaluation\nlibrary(corrplot)     # Correlation visualization\nlibrary(randomForest) # Feature importance comparison"},{"path":"gbm-variable-selection.html","id":"load-and-inspect-the-dataset","chapter":"12 GBM Variable Selection","heading":"12.3.1.2 Load and Inspect the Dataset","text":"","code":"\n# Load example SDM dataset\ndata <- dismo::bioclim\n\n# Convert species presence to a factor (classification task)\ndata$presence <- as.factor(data$presence)\n\n# View dataset structure\nstr(data)"},{"path":"gbm-variable-selection.html","id":"step-2-train-an-initial-gbm-model-with-all-predictors","chapter":"12 GBM Variable Selection","heading":"12.3.2 Step 2: Train an Initial GBM Model with All Predictors","text":"selecting variables, let’s train baseline GBM model environmental predictors.","code":""},{"path":"gbm-variable-selection.html","id":"split-data-into-training-and-testing-sets","chapter":"12 GBM Variable Selection","heading":"12.3.2.1 Split Data into Training and Testing Sets","text":"","code":"\n# Set seed for reproducibility\nset.seed(123)\n\n# Create training (70%) and testing (30%) sets\ntrainIndex <- createDataPartition(data$presence, p = 0.7, list = FALSE)\ntrain_data <- data[trainIndex, ]\ntest_data  <- data[-trainIndex, ]"},{"path":"gbm-variable-selection.html","id":"train-the-full-gbm-model","chapter":"12 GBM Variable Selection","heading":"12.3.2.2 Train the Full GBM Model","text":"✅ Look ?\n- Higher scores indicate variables contribute predictions.\n- Low-importance variables considered removal.","code":"\n# Train an initial GBM model\ngbm_full <- gbm(presence ~ ., \n                data = train_data,\n                distribution = \"bernoulli\",  # Classification task\n                n.trees = 500,  \n                shrinkage = 0.01,  \n                interaction.depth = 3,  \n                cv.folds = 5)  # Cross-validation to prevent overfitting\n\n# View feature importance\nsummary(gbm_full)"},{"path":"gbm-variable-selection.html","id":"step-3-compute-feature-importance-scores-and-remove-low-importance-variables","chapter":"12 GBM Variable Selection","heading":"12.3.3 Step 3: Compute Feature Importance Scores and Remove Low-Importance Variables","text":"GBM provides relative influence scores predictor. remove variables contribute little model accuracy.","code":""},{"path":"gbm-variable-selection.html","id":"plot-feature-importance","chapter":"12 GBM Variable Selection","heading":"12.3.3.1 Plot Feature Importance","text":"","code":"\n# Visualize feature importance\nbarplot(summary(gbm_full)$rel.inf, names.arg = summary(gbm_full)$var, las = 2, col = \"steelblue\",\n        main = \"Feature Importance in GBM\", cex.names = 0.8)"},{"path":"gbm-variable-selection.html","id":"remove-low-importance-variables","chapter":"12 GBM Variable Selection","heading":"12.3.3.2 Remove Low-Importance Variables","text":"✅ Look ?\n- feature importance plot highlights variables significantly impact model.\n- Removing low-importance variables improves efficiency without reducing accuracy.","code":"\n# Define a cutoff threshold for importance (e.g., remove variables < 2%)\nimportant_vars <- summary(gbm_full)$var[summary(gbm_full)$rel.inf > 2]\n\n# Retain only important variables\ntrain_data_reduced <- train_data[, c(\"presence\", important_vars)]\ntest_data_reduced  <- test_data[, c(\"presence\", important_vars)]"},{"path":"gbm-variable-selection.html","id":"step-4-perform-correlation-analysis-to-eliminate-redundant-variables","chapter":"12 GBM Variable Selection","heading":"12.3.4 Step 4: Perform Correlation Analysis to Eliminate Redundant Variables","text":"Environmental predictors often highly correlated, can introduce redundancy.","code":""},{"path":"gbm-variable-selection.html","id":"compute-a-correlation-matrix","chapter":"12 GBM Variable Selection","heading":"12.3.4.1 Compute a Correlation Matrix","text":"","code":"\n# Compute correlation matrix\ncor_matrix <- cor(train_data_reduced[, -1])  # Remove target variable\ncorrplot(cor_matrix, method = \"color\", type = \"upper\", tl.cex = 0.8, title = \"Correlation Matrix\")"},{"path":"gbm-variable-selection.html","id":"remove-highly-correlated-variables-r-0.7","chapter":"12 GBM Variable Selection","heading":"12.3.4.2 Remove Highly Correlated Variables (r > 0.7)","text":"✅ Matter?\n- Removing highly correlated variables prevents redundant information model.\n- GBM model generalizes better relying independent predictors.","code":"\n# Identify correlated pairs\nhigh_cor <- findCorrelation(cor_matrix, cutoff = 0.7, names = TRUE)\n\n# Remove correlated predictors\ntrain_data_final <- train_data_reduced[, !(colnames(train_data_reduced) %in% high_cor)]\ntest_data_final  <- test_data_reduced[, !(colnames(test_data_reduced) %in% high_cor)]"},{"path":"gbm-variable-selection.html","id":"step-5-retrain-gbm-with-reduced-features-and-compare-accuracyauc","chapter":"12 GBM Variable Selection","heading":"12.3.5 Step 5: Retrain GBM with Reduced Features and Compare Accuracy/AUC","text":"","code":""},{"path":"gbm-variable-selection.html","id":"train-the-reduced-gbm-model","chapter":"12 GBM Variable Selection","heading":"12.3.5.1 Train the Reduced GBM Model","text":"","code":"\n# Train a GBM model with selected variables\ngbm_reduced <- gbm(presence ~ ., \n                    data = train_data_final,\n                    distribution = \"bernoulli\",\n                    n.trees = 500,  \n                    shrinkage = 0.01,  \n                    interaction.depth = 3,  \n                    cv.folds = 5)\n\n# View feature importance of reduced model\nsummary(gbm_reduced)"},{"path":"gbm-variable-selection.html","id":"evaluate-model-performance-auc-and-accuracy","chapter":"12 GBM Variable Selection","heading":"12.3.5.2 Evaluate Model Performance (AUC and Accuracy)","text":"","code":"\nlibrary(pROC)\n\n# Predict on test set\nfull_pred <- predict(gbm_full, test_data, n.trees = 500, type = \"response\")\nreduced_pred <- predict(gbm_reduced, test_data_final, n.trees = 500, type = \"response\")\n\n# Compute AUC for full model\nfull_auc <- auc(roc(test_data$presence, full_pred))\n\n# Compute AUC for reduced model\nreduced_auc <- auc(roc(test_data_final$presence, reduced_pred))\n\n# Print AUC Scores\nprint(paste(\"Full GBM AUC:\", full_auc))\nprint(paste(\"Reduced GBM AUC:\", reduced_auc))"},{"path":"gbm-variable-selection.html","id":"compare-accuracy","chapter":"12 GBM Variable Selection","heading":"12.3.5.3 Compare Accuracy","text":"✅ Expected Outcome:\n- reduced model similar AUC accuracy full model fewer predictors.\n- Computation time reduced, making model efficient.","code":"\n# Convert predictions to binary classes\nfull_pred_class <- ifelse(full_pred > 0.5, \"1\", \"0\")\nreduced_pred_class <- ifelse(reduced_pred > 0.5, \"1\", \"0\")\n\n# Compute accuracy\nfull_acc <- sum(full_pred_class == test_data$presence) / nrow(test_data)\nreduced_acc <- sum(reduced_pred_class == test_data_final$presence) / nrow(test_data_final)\n\nprint(paste(\"Full Model Accuracy:\", full_acc))\nprint(paste(\"Reduced Model Accuracy:\", reduced_acc))"},{"path":"gbm-variable-selection.html","id":"key-observations-3","chapter":"12 GBM Variable Selection","heading":"12.3.6 Key Observations","text":"Feature importance analysis helps remove weak variables.Correlation filtering prevents redundancy.reduced model performs well full model efficient.","code":""},{"path":"gbm-variable-selection.html","id":"summary-of-gbm-variable-selection-process","chapter":"12 GBM Variable Selection","heading":"12.3.7 Summary of GBM Variable Selection Process","text":"","code":""},{"path":"gbm-variable-selection.html","id":"compare-model-performance-before-and-after-variable-selection","chapter":"12 GBM Variable Selection","heading":"12.3.8 1. Compare Model Performance Before and After Variable Selection","text":"compare full model (variables) reduced model (selected variables) using AUC (Area Curve) accuracy.","code":""},{"path":"gbm-variable-selection.html","id":"code-example-auc-and-accuracy-comparison","chapter":"12 GBM Variable Selection","heading":"12.3.8.1 Code Example: AUC and Accuracy Comparison","text":"✅ Expected Result:\n- AUC remains similar, reduced model just effective efficient.\n- AUC decreases significantly, important predictors may removed.","code":"\nlibrary(pROC)\n\n# Compute AUC for Full Model\nfull_pred <- predict(gbm_full, test_data, n.trees = 500, type = \"response\")\nfull_auc <- auc(roc(test_data$presence, full_pred))\n\n# Compute AUC for Reduced Model\nreduced_pred <- predict(gbm_reduced, test_data_final, n.trees = 500, type = \"response\")\nreduced_auc <- auc(roc(test_data_final$presence, reduced_pred))\n\n# Print AUC Scores\nprint(paste(\"Full Model AUC:\", full_auc))\nprint(paste(\"Reduced Model AUC:\", reduced_auc))"},{"path":"gbm-variable-selection.html","id":"visualizing-response-curves-1","chapter":"12 GBM Variable Selection","heading":"12.3.9 2. Visualizing Response Curves","text":"Response curves show environmental variables influence species suitability. Ensuring response curves remain biologically meaningful variable selection crucial.","code":""},{"path":"gbm-variable-selection.html","id":"code-example-response-curve-visualization","chapter":"12 GBM Variable Selection","heading":"12.3.9.1 Code Example: Response Curve Visualization","text":"✅ Look ?\n- Similar response curves full reduced models indicate key environmental drivers preserved.\n- response curves change drastically, important predictor may removed.","code":"\n# Plot response curves for the full model\npar(mfrow = c(1,2))\nplot.gbm(gbm_full, i.var = \"bio1\", main = \"Full Model: Bio1\")\nplot.gbm(gbm_reduced, i.var = \"bio1\", main = \"Reduced Model: Bio1\")"},{"path":"gbm-variable-selection.html","id":"assessing-computational-efficiency-gains","chapter":"12 GBM Variable Selection","heading":"12.3.10 3. Assessing Computational Efficiency Gains","text":"key advantage variable selection reducing computational time. compare training times variable selection.","code":""},{"path":"gbm-variable-selection.html","id":"code-example-compute-training-time","chapter":"12 GBM Variable Selection","heading":"12.3.10.1 Code Example: Compute Training Time","text":"✅ Expected Result:\n- reduced model train faster, improving efficiency without losing predictive power.\n- large datasets, speed improvement can significant.","code":"\n# Measure time for full model\nstart_time <- Sys.time()\ngbm_full <- gbm(presence ~ ., data = train_data, distribution = \"bernoulli\", n.trees = 500)\nend_time <- Sys.time()\nfull_time <- end_time - start_time\n\n# Measure time for reduced model\nstart_time <- Sys.time()\ngbm_reduced <- gbm(presence ~ ., data = train_data_final, distribution = \"bernoulli\", n.trees = 500)\nend_time <- Sys.time()\nreduced_time <- end_time - start_time\n\n# Print time comparison\nprint(paste(\"Full Model Training Time:\", full_time))\nprint(paste(\"Reduced Model Training Time:\", reduced_time))"},{"path":"gbm-variable-selection.html","id":"best-practices-common-pitfalls","chapter":"12 GBM Variable Selection","heading":"12.4 5. Best Practices & Common Pitfalls","text":"","code":""},{"path":"gbm-variable-selection.html","id":"avoid-over-removing-important-variables","chapter":"12 GBM Variable Selection","heading":"12.4.1 1. Avoid Over-Removing Important Variables","text":"Pitfall: Removing slightly less important variables may still affect model performance.Solution: Gradually remove variables compare AUC/accuracy step.","code":""},{"path":"gbm-variable-selection.html","id":"ensure-biologicalecological-relevance","chapter":"12 GBM Variable Selection","heading":"12.4.2 2. Ensure Biological/Ecological Relevance","text":"Pitfall: variables may statistically weak ecologically essential.Solution: Consult ecological knowledge eliminating predictors.Example:\nEven elevation (Bio6) low importance, might still critical mountain species.","code":""},{"path":"gbm-variable-selection.html","id":"balance-model-simplicity-with-accuracy","chapter":"12 GBM Variable Selection","heading":"12.4.3 3. Balance Model Simplicity with Accuracy","text":"Pitfall: Keeping many predictors makes model complex slow.Solution: Use AIC, BIC, cross-validation find best trade-accuracy simplicity.✅ Key Takeaway: goal maintain high predictive power removing unnecessary complexity.","code":""},{"path":"gbm-variable-selection.html","id":"summary-key-takeaways","chapter":"12 GBM Variable Selection","heading":"12.5 6. Summary & Key Takeaways","text":"","code":""},{"path":"gbm-variable-selection.html","id":"why-variable-selection-matters","chapter":"12 GBM Variable Selection","heading":"12.5.1 1. Why Variable Selection Matters","text":"Reducing redundant variables improves model accuracy interpretability.Removing unnecessary predictors speeds training prediction times.","code":""},{"path":"gbm-variable-selection.html","id":"recommended-workflow-for-gbm-variable-selection","chapter":"12 GBM Variable Selection","heading":"12.5.2 2. Recommended Workflow for GBM Variable Selection","text":"","code":""},{"path":"gbm-variable-selection.html","id":"iterative-refinement-for-sdm-models","chapter":"12 GBM Variable Selection","heading":"12.5.3 3. Iterative Refinement for SDM Models","text":"Reassess removal step avoid losing important predictors.Validate response curves ensure ecological interpretability.Adjust hyperparameters optimize performance feature reduction.✅ Final Takeaway:\nselecting right variables, GBM models remain accurate, efficient, ecologically meaningful, making powerful tools Species Distribution Modeling (SDM). 🚀","code":""},{"path":"model-evaluation-in-sdm.html","id":"model-evaluation-in-sdm","chapter":"13 Model Evaluation in SDM","heading":"13 Model Evaluation in SDM","text":"","code":""},{"path":"model-evaluation-in-sdm.html","id":"introduction-to-model-evaluation","chapter":"13 Model Evaluation in SDM","heading":"13.1 1. Introduction to Model Evaluation","text":"","code":""},{"path":"model-evaluation-in-sdm.html","id":"why-model-evaluation-is-important-in-sdm","chapter":"13 Model Evaluation in SDM","heading":"13.1.1 Why Model Evaluation is Important in SDM","text":"Model evaluation critical step Species Distribution Modeling (SDM) determines well model predicts distribution species different environmental conditions. Without proper evaluation, predictions may misleading lead incorrect conservation decisions.Model evaluation ensures :✅ model accurate – Predictions match real-world observations.\n✅ model generalizes well – Works new/unseen data, just training set.\n✅ Ecological validity maintained – Predictions align biological knowledge.","code":""},{"path":"model-evaluation-in-sdm.html","id":"key-objectives-of-model-evaluation","chapter":"13 Model Evaluation in SDM","heading":"13.1.2 Key Objectives of Model Evaluation","text":"1️⃣ Assessing Accuracy – well model predict species presence/absence?\n2️⃣ Avoiding Overfitting – model generalize unseen data?\n3️⃣ Ecological Relevance – predictions biologically meaningful?\n4️⃣ Comparing Multiple Models – algorithm (CART, RF, GBM, MaxEnt) performs best dataset?","code":""},{"path":"model-evaluation-in-sdm.html","id":"common-pitfalls-in-model-evaluation","chapter":"13 Model Evaluation in SDM","heading":"13.1.3 Common Pitfalls in Model Evaluation","text":"⚠️ Overfitting\n- model memorizes training data instead learning true species-environment relationships.\n- Fix: Use cross-validation limit model complexity.⚠️ Biased Datasets\n- presence records clustered sampled areas (e.g., near roads), model may falsely predict species prefer areas.\n- Fix: Use spatially explicit validation account sampling bias.⚠️ Improper Thresholding\n- Using wrong threshold presence/absence conversion can skew accuracy metrics.\n- Fix: Experiment different thresholding methods (e.g., maximum sensitivity-specificity, 10th percentile).Pro Tip: Always check ecological plausibility model. high accuracy score mean model makes biologically realistic predictions!","code":""},{"path":"model-evaluation-in-sdm.html","id":"performance-metrics-for-model-evaluation","chapter":"13 Model Evaluation in SDM","heading":"13.2 2. Performance Metrics for Model Evaluation","text":"","code":""},{"path":"model-evaluation-in-sdm.html","id":"classification-metrics-for-presenceabsence-models","chapter":"13 Model Evaluation in SDM","heading":"13.2.1 1. Classification Metrics (For Presence/Absence Models)","text":"species data binary (presence = 1, absence = 0), use classification metrics measure well model differentiates two.","code":""},{"path":"model-evaluation-in-sdm.html","id":"accuracy-overall-correctness-of-predictions","chapter":"13 Model Evaluation in SDM","heading":"13.2.1.1 Accuracy – Overall correctness of predictions","text":"Formula:\\[\nAccuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n\\]\n✅ Good balanced datasets misleading imbalanced data (e.g., species rare).","code":""},{"path":"model-evaluation-in-sdm.html","id":"sensitivity-recall-ability-to-correctly-predict-species-presence","chapter":"13 Model Evaluation in SDM","heading":"13.2.1.2 Sensitivity (Recall) – Ability to correctly predict species presence","text":"Formula:\\[\nSensitivity = \\frac{TP}{TP + FN}\n\\]\n✅ Important conservation false negatives (FN) may ignore suitable habitats.","code":""},{"path":"model-evaluation-in-sdm.html","id":"specificity-ability-to-correctly-predict-absence","chapter":"13 Model Evaluation in SDM","heading":"13.2.1.3 Specificity – Ability to correctly predict absence","text":"Formula:\\[\nSpecificity = \\frac{TN}{TN + FP}\n\\]\n✅ Useful understanding -predictions species presence.","code":""},{"path":"model-evaluation-in-sdm.html","id":"kappa-statistic-agreement-beyond-random-chance","chapter":"13 Model Evaluation in SDM","heading":"13.2.1.4 Kappa Statistic – Agreement Beyond Random Chance","text":"Kappa adjusts accuracy account random chance agreement.Formula:\\[\nKappa = \\frac{Accuracy - Expected Accuracy}{1 - Expected Accuracy}\n\\]\n✅ reliable accuracy imbalanced datasets.","code":""},{"path":"model-evaluation-in-sdm.html","id":"area-under-the-curve-auc-roc-auc-pr","chapter":"13 Model Evaluation in SDM","heading":"13.2.2 2. Area Under the Curve (AUC-ROC & AUC-PR)","text":"AUC one common evaluation metrics species distribution models.","code":""},{"path":"model-evaluation-in-sdm.html","id":"roc-curve-receiver-operating-characteristic-curve","chapter":"13 Model Evaluation in SDM","heading":"13.2.2.1 ROC Curve (Receiver Operating Characteristic Curve)","text":"Plots True Positive Rate (Sensitivity) vs. False Positive Rate (1 - Specificity)Higher AUC = Better ability distinguish presence absence.✅ Works well classification tasks.Interpreting AUC Scores:","code":""},{"path":"model-evaluation-in-sdm.html","id":"precision-recall-pr-curve-best-for-rare-species","chapter":"13 Model Evaluation in SDM","heading":"13.2.2.2 Precision-Recall (PR) Curve – Best for Rare Species","text":"species rare, AUC-ROC may misleading. Instead, use Precision-Recall Curve, evaluates model performance absences outnumber presences.Precision = many predicted presences actually correct?Recall = Sensitivity (ability detect presences).✅ Best species presence rare (e.g., endangered species).","code":""},{"path":"model-evaluation-in-sdm.html","id":"true-skill-statistic-tss","chapter":"13 Model Evaluation in SDM","heading":"13.2.3 3. True Skill Statistic (TSS)","text":"TSS alternative AUC depend prevalence (species rarity).Formula:\\[\nTSS = Sensitivity + Specificity - 1\n\\]✅ Good ecological models presence/absence evenly distributed.\n✅ Ranges -1 (worse random) 1 (perfect prediction).","code":""},{"path":"model-evaluation-in-sdm.html","id":"root-mean-square-error-rmse-for-continuous-models","chapter":"13 Model Evaluation in SDM","heading":"13.2.4 4. Root Mean Square Error (RMSE) for Continuous Models","text":"model outputs continuous suitability values (e.g., habitat suitability indices), RMSE measures much predictions deviate actual presence/absence.Formula:\\[\nRMSE = \\sqrt{\\frac{\\sum (Predicted - Observed)^2}{n}}\n\\]✅ Lower RMSE = Better fit.\n✅ Works well comparing continuous predictions (e.g., suitability scores MaxEnt).","code":""},{"path":"model-evaluation-in-sdm.html","id":"comparison-of-model-evaluation-metrics","chapter":"13 Model Evaluation in SDM","heading":"13.3 Comparison of Model Evaluation Metrics","text":"","code":""},{"path":"model-evaluation-in-sdm.html","id":"next-steps-3","chapter":"13 Model Evaluation in SDM","heading":"13.3.1 Next Steps","text":"Now understand model evaluation metrics, next section explore different validation techniques (cross-validation, train-test split, spatial validation) ensure models generalize well new data. 🚀Model validation ensures Species Distribution Models (SDMs) perform well unseen data. Without proper validation, models may overfit fail generalize real-world conditions.","code":""},{"path":"model-evaluation-in-sdm.html","id":"train-test-split","chapter":"13 Model Evaluation in SDM","heading":"13.3.2 1. Train-Test Split","text":"Use Separate Test Set?\nmodel fits training data well might fail new data. train-test split ensures model evaluated independent data measure generalization.✅ Prevents overfitting\n✅ Ensures predictions reliable\n✅ Used machine learning applications","code":""},{"path":"model-evaluation-in-sdm.html","id":"code-example-train-test-split","chapter":"13 Model Evaluation in SDM","heading":"13.3.2.1 Code Example: Train-Test Split","text":"","code":"\n# Load necessary package\nlibrary(caret)\n\n# Split data: 70% Training, 30% Testing\nset.seed(123)\ntrainIndex <- createDataPartition(data$presence, p = 0.7, list = FALSE)\ntrain_data <- data[trainIndex, ]\ntest_data  <- data[-trainIndex, ]"},{"path":"model-evaluation-in-sdm.html","id":"cross-validation-1","chapter":"13 Model Evaluation in SDM","heading":"13.3.3 2. Cross-Validation","text":"Cross-validation used don’t want lose data training. divides dataset multiple subsets trains model several times ensure stability.","code":""},{"path":"model-evaluation-in-sdm.html","id":"k-fold-cross-validation","chapter":"13 Model Evaluation in SDM","heading":"13.3.3.1 K-Fold Cross-Validation","text":"Splits data K folds (e.g., 5-fold 10-fold).model trained K-1 folds tested remaining fold.process repeats K times, results averaged.✅ reliable single train-test split\n✅ Works well small datasets","code":""},{"path":"model-evaluation-in-sdm.html","id":"code-example-k-fold-cross-validation","chapter":"13 Model Evaluation in SDM","heading":"13.3.3.2 Code Example: K-Fold Cross-Validation","text":"","code":"\n# Perform 5-fold cross-validation\ntrain_control <- trainControl(method = \"cv\", number = 5)\ncv_model <- train(presence ~ ., data = train_data, method = \"rf\", trControl = train_control)\nprint(cv_model)"},{"path":"model-evaluation-in-sdm.html","id":"leave-one-out-cross-validation-loocv","chapter":"13 Model Evaluation in SDM","heading":"13.3.3.3 Leave-One-Out Cross-Validation (LOOCV)","text":"Uses one data point test set others training.Repeats process every data point.✅ Best small datasets\n✅ Ensures data points contribute validation\n⚠️ Computationally expensive large datasets","code":""},{"path":"model-evaluation-in-sdm.html","id":"code-example-loocv","chapter":"13 Model Evaluation in SDM","heading":"13.3.3.4 Code Example: LOOCV","text":"","code":"\n# Perform Leave-One-Out Cross-Validation\ntrain_control_loocv <- trainControl(method = \"LOOCV\")\nloocv_model <- train(presence ~ ., data = train_data, method = \"rf\", trControl = train_control_loocv)\nprint(loocv_model)"},{"path":"model-evaluation-in-sdm.html","id":"spatially-explicit-validation","chapter":"13 Model Evaluation in SDM","heading":"13.3.4 3. Spatially Explicit Validation","text":"Standard validation methods assume data points independent, species occurrence points often spatially clustered. Spatially explicit validation ensures evaluation accounts spatial autocorrelation.✅ Prevents overestimating model performance\n✅ Ensures model works unsampled areas","code":""},{"path":"model-evaluation-in-sdm.html","id":"approach-block-cross-validation","chapter":"13 Model Evaluation in SDM","heading":"13.3.4.1 Approach: Block Cross-Validation","text":"Divides study area spatial blocks.Uses blocks training others testing.","code":""},{"path":"model-evaluation-in-sdm.html","id":"code-example-spatial-cross-validation","chapter":"13 Model Evaluation in SDM","heading":"13.3.4.2 Code Example: Spatial Cross-Validation","text":"","code":"\nlibrary(blockCV)\n\n# Define spatial blocks\nblocks <- spatialBlock(speciesData = data, theRange = 100000, k = 5)\n\n# Perform cross-validation with spatial blocks\ntrain_control_spatial <- trainControl(method = \"cv\", index = blocks$folds)\nspatial_model <- train(presence ~ ., data = train_data, method = \"rf\", trControl = train_control_spatial)\nprint(spatial_model)"},{"path":"model-evaluation-in-sdm.html","id":"visualizing-model-performance","chapter":"13 Model Evaluation in SDM","heading":"13.4 4. Visualizing Model Performance","text":"Model evaluation easier interpret results visualized. key ways visualize model quality.","code":""},{"path":"model-evaluation-in-sdm.html","id":"roc-and-precision-recall-curves","chapter":"13 Model Evaluation in SDM","heading":"13.4.1 1. ROC and Precision-Recall Curves","text":"","code":""},{"path":"model-evaluation-in-sdm.html","id":"roc-curve-receiver-operating-characteristic-curve-1","chapter":"13 Model Evaluation in SDM","heading":"13.4.1.1 ROC Curve (Receiver Operating Characteristic Curve)","text":"X-axis: False Positive RateY-axis: True Positive RateHigher AUC (closer 1) indicates better performance.","code":""},{"path":"model-evaluation-in-sdm.html","id":"code-example-plot-roc-curve","chapter":"13 Model Evaluation in SDM","heading":"13.4.1.2 Code Example: Plot ROC Curve","text":"","code":"\nlibrary(pROC)\n\n# Compute and plot ROC curve\nroc_curve <- roc(test_data$presence, predict(model, test_data, type = \"prob\")[,2])\nplot(roc_curve, main = \"ROC Curve\")"},{"path":"model-evaluation-in-sdm.html","id":"precision-recall-curve","chapter":"13 Model Evaluation in SDM","heading":"13.4.1.3 Precision-Recall Curve","text":"Best used imbalanced datasets absences outnumber presences.","code":""},{"path":"model-evaluation-in-sdm.html","id":"code-example-precision-recall-curve","chapter":"13 Model Evaluation in SDM","heading":"13.4.1.4 Code Example: Precision-Recall Curve","text":"","code":"\n# Compute and plot Precision-Recall curve\npr_curve <- pr.curve(scores.class0 = predict(model, test_data, type = \"prob\")[,2],\n                     weights.class0 = test_data$presence)\nplot(pr_curve, main = \"Precision-Recall Curve\")"},{"path":"model-evaluation-in-sdm.html","id":"suitability-maps-and-threshold-selection","chapter":"13 Model Evaluation in SDM","heading":"13.4.2 2. Suitability Maps and Threshold Selection","text":"Binary vs. Continuous Predictions\n- Continuous maps show habitat suitability scores.\n- Binary maps classify suitable vs. unsuitable areas based threshold.","code":""},{"path":"model-evaluation-in-sdm.html","id":"thresholding-approaches","chapter":"13 Model Evaluation in SDM","heading":"13.4.2.1 Thresholding Approaches","text":"Maximize Sensitivity-Specificity10th Percentile Presence Threshold (rare species).","code":""},{"path":"model-evaluation-in-sdm.html","id":"code-example-suitability-map-with-thresholding","chapter":"13 Model Evaluation in SDM","heading":"13.4.2.2 Code Example: Suitability Map with Thresholding","text":"","code":"\nlibrary(raster)\n\n# Predict suitability\nsuitability_map <- predict(model, raster_stack, type = \"response\")\n\n# Convert to binary presence/absence using threshold\nthreshold <- 0.5\nbinary_map <- suitability_map > threshold\n\n# Plot results\npar(mfrow = c(1,2))\nplot(suitability_map, main = \"Continuous Suitability Map\")\nplot(binary_map, main = \"Binary Presence/Absence Map\")"},{"path":"model-evaluation-in-sdm.html","id":"response-curves","chapter":"13 Model Evaluation in SDM","heading":"13.4.3 3. Response Curves","text":"Response curves show environmental variable affects species predictions.✅ Ensures ecological realism\n✅ Helps identify misleading predictors","code":""},{"path":"model-evaluation-in-sdm.html","id":"code-example-plot-response-curves","chapter":"13 Model Evaluation in SDM","heading":"13.4.3.1 Code Example: Plot Response Curves","text":"✅ Look ?\n- response curves make ecological sense?\n- important predictors showing meaningful trends?","code":"\n# Plot response curves for important variables\nplot.gbm(model, i.var = \"bio1\", main = \"Effect of Temperature (Bio1)\")\nplot.gbm(model, i.var = \"bio12\", main = \"Effect of Precipitation (Bio12)\")"},{"path":"model-evaluation-in-sdm.html","id":"summary-key-model-validation-visualization-techniques","chapter":"13 Model Evaluation in SDM","heading":"13.4.4 Summary: Key Model Validation & Visualization Techniques","text":"","code":""},{"path":"model-evaluation-in-sdm.html","id":"next-steps-4","chapter":"13 Model Evaluation in SDM","heading":"13.4.5 Next Steps","text":"Now validated model visualized results, next section explore comparing different SDM models (CART, RF, GBM, MaxEnt) choose best approach given dataset. 🚀","code":""},{"path":"model-evaluation-in-sdm.html","id":"comparing-models-cart-rf-gbm-and-maxent","chapter":"13 Model Evaluation in SDM","heading":"13.5 5. Comparing Models: CART, RF, GBM, and MaxEnt","text":"Selecting best Species Distribution Model (SDM) depends data type, research goals, computational resources. , compare CART, Random Forest (RF), Gradient Boosting Machine (GBM), MaxEnt determine useful.","code":""},{"path":"model-evaluation-in-sdm.html","id":"best-evaluation-metrics-for-tree-based-vs.-presence-background-models","chapter":"13 Model Evaluation in SDM","heading":"13.5.1 1. Best Evaluation Metrics for Tree-Based vs. Presence-Background Models","text":"Different model types require different evaluation metrics:✅ Takeaway: MaxEnt relies AUC presence-background validation, tree-based methods use accuracy classification metrics.","code":""},{"path":"model-evaluation-in-sdm.html","id":"when-to-use-different-sdm-models","chapter":"13 Model Evaluation in SDM","heading":"13.5.2 2. When to Use Different SDM Models?","text":"✅ Takeaway:\n- Use CART interpretability key.\n- Use RF balanced presence-absence classification.\n- Use GBM maximum accuracy needed.\n- Use MaxEnt presence data available.","code":""},{"path":"model-evaluation-in-sdm.html","id":"performance-trade-offs-interpretability-vs.-accuracy-vs.-computational-cost","chapter":"13 Model Evaluation in SDM","heading":"13.5.3 3. Performance Trade-Offs: Interpretability vs. Accuracy vs. Computational Cost","text":"✅ Takeaway:\n- Use CART interpretability important accuracy.\n- Use RF/GBM accuracy priority.\n- Use MaxEnt working presence-data computational efficiency needed.","code":""},{"path":"model-evaluation-in-sdm.html","id":"common-pitfalls-in-model-evaluation-1","chapter":"13 Model Evaluation in SDM","heading":"13.6 6. Common Pitfalls in Model Evaluation","text":"Even models perform well, several pitfalls can lead misleading results.","code":""},{"path":"model-evaluation-in-sdm.html","id":"overfitting-when-a-model-is-too-complex-and-fails-to-generalize","chapter":"13 Model Evaluation in SDM","heading":"13.6.1 1. Overfitting: When a Model is Too Complex and Fails to Generalize","text":"⚠️ Problem: model fits training data perfectly performs poorly new data.🔹 Solution:\n- Use cross-validation ensure generalization.\n- Regularization GBM MaxEnt prevents overfitting.\n- Limit tree depth CART RF.","code":""},{"path":"model-evaluation-in-sdm.html","id":"ignoring-spatial-bias-why-presence-points-should-be-spatially-independent","chapter":"13 Model Evaluation in SDM","heading":"13.6.2 2. Ignoring Spatial Bias: Why Presence Points Should Be Spatially Independent","text":"⚠️ Problem:\n- presence records cluster near roads specific locations, model may falsely learn species prefer areas.🔹 Solution:\n- Use spatial cross-validation instead random splits.\n- Apply bias correction MaxEnt adjust sampling effort.","code":""},{"path":"model-evaluation-in-sdm.html","id":"improper-thresholding-setting-unrealistic-suitability-cutoffs","chapter":"13 Model Evaluation in SDM","heading":"13.6.3 3. Improper Thresholding: Setting Unrealistic Suitability Cutoffs","text":"⚠️ Problem:\n- Converting continuous suitability scores binary presence-absence maps using arbitrary thresholds.🔹 Solution:\n- Use Maximize Sensitivity-Specificity Thresholding.\n- Compare multiple thresholding methods select meaningful.✅ Takeaway: Always validate threshold selection ecological knowledge.","code":""},{"path":"model-evaluation-in-sdm.html","id":"summary-best-practices","chapter":"13 Model Evaluation in SDM","heading":"13.7 7. Summary & Best Practices","text":"","code":""},{"path":"model-evaluation-in-sdm.html","id":"key-takeaways-from-model-evaluation","chapter":"13 Model Evaluation in SDM","heading":"13.7.1 Key Takeaways from Model Evaluation","text":"Different models require different evaluation metrics – RF/GBM rely classification metrics, MaxEnt uses presence-background validation.Choose models based data availability – Presence-absence models work well classification, MaxEnt best presence-data.Avoid common pitfalls – Overfitting, spatial bias, improper thresholding can reduce model reliability.","code":""},{"path":"model-evaluation-in-sdm.html","id":"recommended-workflow-for-assessing-sdm-performance","chapter":"13 Model Evaluation in SDM","heading":"13.7.2 Recommended Workflow for Assessing SDM Performance","text":"✅ Step 1: Choose SDM Model Based Data Type\n- Use CART, RF, GBM presence-absence.\n- Use MaxEnt presence-data.✅ Step 2: Apply Proper Validation Techniques\n- Use Train-Test Splitting Cross-Validation presence-absence models.\n- Use Spatial Cross-Validation presence points clustered.✅ Step 3: Select Right Performance Metrics\n- Accuracy, Sensitivity, Specificity, Kappa RF/GBM.\n- AUC, TSS, PR-Curve MaxEnt.✅ Step 4: Interpret Results Ecologically\n- Ensure response curves make biological sense.\n- Check species presence predicted ecologically relevant areas.","code":""},{"path":"model-evaluation-in-sdm.html","id":"iterative-improvements-how-to-refine-models-based-on-evaluation-results","chapter":"13 Model Evaluation in SDM","heading":"13.7.3 Iterative Improvements: How to Refine Models Based on Evaluation Results","text":"🔄 AUC low → Try different feature selection methods.\n🔄 overfitting occurs → Reduce model complexity (e.g., increase regularization GBM, limit tree depth RF).\n🔄 presence points biased → Use spatially explicit validation.✅ Final Takeaway:\nfollowing best practices model evaluation, ensure species distribution models robust, accurate, ecologically meaningful. 🚀","code":""},{"path":"model-evaluation-in-sdm.html","id":"ensemble-models","chapter":"13 Model Evaluation in SDM","heading":"13.8 # Ensemble Models","text":"","code":""},{"path":"model-evaluation-in-sdm.html","id":"introduction-to-ensemble-models","chapter":"13 Model Evaluation in SDM","heading":"13.9 1. Introduction to Ensemble Models","text":"","code":""},{"path":"model-evaluation-in-sdm.html","id":"what-are-ensemble-models","chapter":"13 Model Evaluation in SDM","heading":"13.9.1 What Are Ensemble Models?","text":"Ensemble models combine predictions multiple models increase accuracy, stability, generalization. Instead relying single model, ensembles aggregate results different models produce robust prediction.Ensemble methods particularly useful Species Distribution Modeling (SDM) different algorithms may capture different aspects species-environment relationships.","code":""},{"path":"model-evaluation-in-sdm.html","id":"why-use-ensemble-approaches-in-sdm","chapter":"13 Model Evaluation in SDM","heading":"13.9.2 Why Use Ensemble Approaches in SDM?","text":"📌 Single models may biased – Different SDM algorithms strengths weaknesses.\n📌 Combining models improves reliability – Ensembles reduce overfitting increase prediction stability.\n📌 realistic ecological interpretations – Reducing uncertainty predictions makes results reliable conservation planning.✅ Example: species’ distribution predicted differently MaxEnt (presence-) Random Forest (presence-absence), ensemble model can blend predictions better outcome.","code":""},{"path":"model-evaluation-in-sdm.html","id":"advantages-over-single-model-predictions","chapter":"13 Model Evaluation in SDM","heading":"13.9.3 Advantages Over Single-Model Predictions","text":"✅ Takeaway: Ensemble models widely used machine learning ecology improve species distribution predictions.","code":""},{"path":"model-evaluation-in-sdm.html","id":"types-of-ensemble-methods","chapter":"13 Model Evaluation in SDM","heading":"13.10 2. Types of Ensemble Methods","text":"four common ensemble modeling techniques used SDM.","code":""},{"path":"model-evaluation-in-sdm.html","id":"bagging-bootstrap-aggregating","chapter":"13 Model Evaluation in SDM","heading":"13.10.1 1. Bagging (Bootstrap Aggregating)","text":"Bagging creates multiple versions model training different random subsets data averaging predictions.📌 Example: Random Forest bagging method builds multiple decision trees averages outputs.✅ Reduces variance prevents overfitting.\n✅ Works well presence-absence models.","code":""},{"path":"model-evaluation-in-sdm.html","id":"boosting","chapter":"13 Model Evaluation in SDM","heading":"13.10.2 2. Boosting","text":"Boosting builds models sequentially, new model learns errors previous models improve performance.📌 Example: Gradient Boosting Machine (GBM) sequentially refines weak learners strong model.✅ Improves accuracy learning mistakes.\n✅ Works well complex, nonlinear relationships.","code":""},{"path":"model-evaluation-in-sdm.html","id":"stacking-model-stacking","chapter":"13 Model Evaluation in SDM","heading":"13.10.3 3. Stacking (Model Stacking)","text":"Stacking meta-learning approach combines predictions multiple models trains another model learn predictions trust .📌 Example: Combining MaxEnt, Random Forest, GBM final predictive model.✅ Learns best combination models optimal predictions.\n✅ Reduces individual model biases.","code":""},{"path":"model-evaluation-in-sdm.html","id":"weighted-ensemble-models","chapter":"13 Model Evaluation in SDM","heading":"13.10.4 4. Weighted Ensemble Models","text":"Instead treating models equally, weighted ensembles assign different weights different models based performance metrics (e.g., AUC, TSS).📌 Example: MaxEnt performs best presence-data, gets higher weight CART GBM final ensemble.✅ Balances strengths weaknesses different models.\n✅ Can customized based model reliability.","code":""},{"path":"model-evaluation-in-sdm.html","id":"summary-choosing-the-right-ensemble-method","chapter":"13 Model Evaluation in SDM","heading":"13.10.5 Summary: Choosing the Right Ensemble Method","text":"","code":""},{"path":"model-evaluation-in-sdm.html","id":"implementing-ensemble-models-in-r","chapter":"13 Model Evaluation in SDM","heading":"13.11 3. Implementing Ensemble Models in R","text":"section, train multiple Species Distribution Models (SDMs) using CART, Random Forest (RF), Gradient Boosting Machine (GBM), MaxEnt, combine predictions ensemble model improved accuracy robustness.","code":""},{"path":"model-evaluation-in-sdm.html","id":"step-1-load-necessary-libraries","chapter":"13 Model Evaluation in SDM","heading":"13.11.1 Step 1: Load Necessary Libraries","text":"","code":"\n# Load required libraries\nlibrary(dismo)        # For species distribution modeling\nlibrary(rpart)        # CART (Decision Tree)\nlibrary(randomForest) # Random Forest\nlibrary(gbm)         # Gradient Boosting Machine\nlibrary(caret)       # Model training and evaluation\nlibrary(ENMeval)     # MaxEnt modeling\nlibrary(terra)       # Handling raster data"},{"path":"model-evaluation-in-sdm.html","id":"step-2-load-and-prepare-data-2","chapter":"13 Model Evaluation in SDM","heading":"13.11.2 Step 2: Load and Prepare Data","text":"use presence-absence data dismo::bioclim environmental predictors.","code":"\n# Load species data\ndata <- dismo::bioclim\n\n# Convert presence to a factor (classification task)\ndata$presence <- as.factor(data$presence)\n\n# Split into Training and Testing Sets (70% Training, 30% Testing)\nset.seed(123)\ntrainIndex <- createDataPartition(data$presence, p = 0.7, list = FALSE)\ntrain_data <- data[trainIndex, ]\ntest_data  <- data[-trainIndex, ]"},{"path":"model-evaluation-in-sdm.html","id":"step-3-train-individual-sdm-models","chapter":"13 Model Evaluation in SDM","heading":"13.11.3 Step 3: Train Individual SDM Models","text":"","code":""},{"path":"model-evaluation-in-sdm.html","id":"cart-model-decision-tree","chapter":"13 Model Evaluation in SDM","heading":"13.11.3.1 1. CART Model (Decision Tree)","text":"","code":"\ncart_model <- rpart(presence ~ ., data = train_data, method = \"class\")\ncart_pred <- predict(cart_model, test_data, type = \"prob\")[,2]  # Probability of presence"},{"path":"model-evaluation-in-sdm.html","id":"random-forest-model","chapter":"13 Model Evaluation in SDM","heading":"13.11.3.2 2. Random Forest Model","text":"","code":"\nrf_model <- randomForest(presence ~ ., data = train_data, ntree = 500, importance = TRUE)\nrf_pred <- predict(rf_model, test_data, type = \"prob\")[,2]  # Probability of presence"},{"path":"model-evaluation-in-sdm.html","id":"gradient-boosting-machine-gbm","chapter":"13 Model Evaluation in SDM","heading":"13.11.3.3 3. Gradient Boosting Machine (GBM)","text":"","code":"\ngbm_model <- gbm(presence ~ ., data = train_data, distribution = \"bernoulli\", \n                 n.trees = 500, shrinkage = 0.01, interaction.depth = 3)\ngbm_pred <- predict(gbm_model, test_data, n.trees = 500, type = \"response\")"},{"path":"model-evaluation-in-sdm.html","id":"maxent-model-presence-only","chapter":"13 Model Evaluation in SDM","heading":"13.11.3.4 4. MaxEnt Model (Presence-Only)","text":"","code":"\n# Prepare data for MaxEnt\npresence_points <- train_data[train_data$presence == 1, c(\"lon\", \"lat\")]\nbackground_points <- randomPoints(predictors, 500)  # Generate background points\n\n# Train MaxEnt model\nmaxent_model <- maxent(predictors, presence_points)\n\n# Predict probabilities\nmaxent_pred <- predict(maxent_model, test_data)"},{"path":"model-evaluation-in-sdm.html","id":"step-4-combine-predictions-into-an-ensemble-model","chapter":"13 Model Evaluation in SDM","heading":"13.11.4 Step 4: Combine Predictions into an Ensemble Model","text":"use simple mean ensemble averaging probability predictions models.Alternatively, weighted ensembles can used giving higher weights models better AUC scores.","code":"\n# Combine model predictions\nensemble_pred <- (cart_pred + rf_pred + gbm_pred + maxent_pred) / 4\n# Compute AUC scores for each model\nlibrary(pROC)\nauc_cart <- auc(roc(test_data$presence, cart_pred))\nauc_rf <- auc(roc(test_data$presence, rf_pred))\nauc_gbm <- auc(roc(test_data$presence, gbm_pred))\nauc_maxent <- auc(roc(test_data$presence, maxent_pred))\n\n# Compute weighted ensemble based on AUC\ntotal_auc <- auc_cart + auc_rf + auc_gbm + auc_maxent\nweights <- c(auc_cart, auc_rf, auc_gbm, auc_maxent) / total_auc\nensemble_pred_weighted <- (weights[1] * cart_pred + \n                           weights[2] * rf_pred + \n                           weights[3] * gbm_pred + \n                           weights[4] * maxent_pred)"},{"path":"model-evaluation-in-sdm.html","id":"step-5-evaluate-ensemble-performance-vs.-individual-models","chapter":"13 Model Evaluation in SDM","heading":"13.11.5 Step 5: Evaluate Ensemble Performance vs. Individual Models","text":"","code":""},{"path":"model-evaluation-in-sdm.html","id":"compute-auc-scores","chapter":"13 Model Evaluation in SDM","heading":"13.11.5.1 1. Compute AUC Scores","text":"","code":"\nauc_ensemble <- auc(roc(test_data$presence, ensemble_pred))\nauc_weighted_ensemble <- auc(roc(test_data$presence, ensemble_pred_weighted))\n\nprint(paste(\"CART AUC:\", auc_cart))\nprint(paste(\"Random Forest AUC:\", auc_rf))\nprint(paste(\"GBM AUC:\", auc_gbm))\nprint(paste(\"MaxEnt AUC:\", auc_maxent))\nprint(paste(\"Simple Ensemble AUC:\", auc_ensemble))\nprint(paste(\"Weighted Ensemble AUC:\", auc_weighted_ensemble))"},{"path":"model-evaluation-in-sdm.html","id":"compare-accuracy-1","chapter":"13 Model Evaluation in SDM","heading":"13.11.5.2 2. Compare Accuracy","text":"✅ Expected Results:\n- ensemble models outperform individual models.\n- Weighted ensembles perform better simple averaging prioritize better-performing models.","code":"\n# Convert probabilities to presence/absence using 0.5 threshold\nensemble_pred_class <- ifelse(ensemble_pred > 0.5, \"1\", \"0\")\nweighted_ensemble_pred_class <- ifelse(ensemble_pred_weighted > 0.5, \"1\", \"0\")\n\n# Compute accuracy for each model\naccuracy_ensemble <- sum(ensemble_pred_class == test_data$presence) / nrow(test_data)\naccuracy_weighted_ensemble <- sum(weighted_ensemble_pred_class == test_data$presence) / nrow(test_data)\n\nprint(paste(\"Ensemble Accuracy:\", accuracy_ensemble))\nprint(paste(\"Weighted Ensemble Accuracy:\", accuracy_weighted_ensemble))"},{"path":"model-evaluation-in-sdm.html","id":"summary-ensemble-modeling-in-sdm","chapter":"13 Model Evaluation in SDM","heading":"13.11.6 Summary: Ensemble Modeling in SDM","text":"✅ Final Takeaway:\n- Simple averaging models easy way improve accuracy.\n- Weighted ensembles provide even better performance giving higher importance stronger models.","code":""},{"path":"model-evaluation-in-sdm.html","id":"benefits-of-ensemble-models","chapter":"13 Model Evaluation in SDM","heading":"13.11.7 Benefits of Ensemble Models","text":"Ensemble models provide significant improvements single-model predictions, making valuable Species Distribution Modeling (SDM).✅ Improved Accuracy\n- combining multiple models, ensembles capture different aspects species-environment relationships, leading better predictions.\n- Example: MaxEnt overpredicts species presence RF underpredicts, ensemble balances biases.✅ Reduced Overfitting\n- Individual models (e.g., CART) may overfit training data, ensembles smooth inconsistencies.\n- Methods like bagging (RF) boosting (GBM) prevent models learning noise.✅ Better Generalization New Data\n- Combining models makes predictions stable across different environmental conditions.\n- Example: single model performs poorly certain regions, others ensemble compensate.✅ Robust Predictions Conservation\n- SDMs used conservation planning, reducing uncertainty crucial.\n- Example: habitat suitability mapping, ensembles minimize risk false negatives (missing critical habitats).","code":""},{"path":"model-evaluation-in-sdm.html","id":"limitations-of-ensemble-models","chapter":"13 Model Evaluation in SDM","heading":"13.11.8 Limitations of Ensemble Models","text":"⚠️ Increased Computational Cost\n- Training multiple models requires processing power time.\n- Example: Running CART, RF, GBM, MaxEnt together takes longer single MaxEnt model.⚠️ Model Complexity\n- Understanding ensemble makes prediction harder understanding single decision tree (CART).\n- Solution: Feature importance analysis can help interpret variables matter .⚠️ Difficult Interpretation\n- stakeholders (e.g., policymakers, conservation planners) prefer simple, interpretable models.\n- Example: single decision tree (CART) easy explain, complex GBM ensemble .✅ Takeaway:\nUse ensembles accuracy critical mindful computational costs interpretability challenges.","code":""},{"path":"model-evaluation-in-sdm.html","id":"summary-best-practices-1","chapter":"13 Model Evaluation in SDM","heading":"13.12 5. Summary & Best Practices","text":"","code":""},{"path":"model-evaluation-in-sdm.html","id":"when-to-use-ensemble-models-in-sdm","chapter":"13 Model Evaluation in SDM","heading":"13.12.1 When to Use Ensemble Models in SDM","text":"📌 multiple models provide different results, need consensus prediction.\n📌 want reduce uncertainty species distribution maps.\n📌 working complex environmental datasets single models struggle.\n📌 prioritizing accuracy interpretability (e.g., conservation planning).","code":""},{"path":"model-evaluation-in-sdm.html","id":"recommended-workflow-for-building-and-evaluating-ensembles","chapter":"13 Model Evaluation in SDM","heading":"13.12.2 Recommended Workflow for Building and Evaluating Ensembles","text":"✅ Step 1: Train Multiple Models\n- Use CART, RF, GBM, MaxEnt generate individual predictions.✅ Step 2: Select Best Models\n- Remove models perform poorly (low AUC, TSS, Kappa scores).✅ Step 3: Combine Predictions Using Ensemble Approach\n- Simple averaging (mean models).\n- Weighted averaging (assign higher weight better-performing models).✅ Step 4: Evaluate Ensemble Performance\n- Compare ensemble AUC accuracy individual models.\n- Use response curves check ecological validity.✅ Step 5: Visualize Interpret Results\n- Generate ensemble suitability maps compare individual model maps.\n- Ensure predictions align ecological expectations.","code":""},{"path":"model-evaluation-in-sdm.html","id":"how-to-select-the-best-models-for-ensemble-predictions","chapter":"13 Model Evaluation in SDM","heading":"13.12.3 How to Select the Best Models for Ensemble Predictions?","text":"✅ Final Takeaway\n- Ensemble models provide higher accuracy, reduced overfitting, better generalization.\n- However, require computation can harder interpret.\n- Choose right ensemble approach based dataset conservation goals.","code":""}]
