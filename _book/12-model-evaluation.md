# Model Evaluation in SDM

---

## **1. Introduction to Model Evaluation**  

---

### **Why Model Evaluation is Important in SDM**  

Model evaluation is a **critical step** in **Species Distribution Modeling (SDM)** because it determines how well a model predicts the distribution of species in different environmental conditions. **Without proper evaluation, predictions may be misleading and lead to incorrect conservation decisions.**  

Model evaluation ensures that:  

‚úÖ **The model is accurate** ‚Äì Predictions match real-world observations.  
‚úÖ **The model generalizes well** ‚Äì Works on new/unseen data, not just the training set.  
‚úÖ **Ecological validity is maintained** ‚Äì Predictions align with biological knowledge.  

---

### **Key Objectives of Model Evaluation**  

1Ô∏è‚É£ **Assessing Accuracy** ‚Äì How well does the model predict species presence/absence?  
2Ô∏è‚É£ **Avoiding Overfitting** ‚Äì Does the model generalize to unseen data?  
3Ô∏è‚É£ **Ecological Relevance** ‚Äì Are predictions biologically meaningful?  
4Ô∏è‚É£ **Comparing Multiple Models** ‚Äì Which algorithm (CART, RF, GBM, MaxEnt) performs best for the dataset?  

---

### **Common Pitfalls in Model Evaluation**  

‚ö†Ô∏è **Overfitting**  
- The model memorizes training data instead of learning true species-environment relationships.  
- **Fix**: Use cross-validation and limit model complexity.  

‚ö†Ô∏è **Biased Datasets**  
- If presence records are clustered in sampled areas (e.g., near roads), the model may falsely predict that species prefer those areas.  
- **Fix**: Use **spatially explicit validation** and account for sampling bias.  

‚ö†Ô∏è **Improper Thresholding**  
- Using the wrong threshold for presence/absence conversion can **skew accuracy metrics**.  
- **Fix**: Experiment with different **thresholding methods** (e.g., maximum sensitivity-specificity, 10th percentile).  

::: {.rmdtip}
**Pro Tip:** Always check the **ecological plausibility** of the model. A high accuracy score does not mean the model makes biologically realistic predictions!
:::

---

## **2. Performance Metrics for Model Evaluation**  

---

### **1. Classification Metrics (For Presence/Absence Models)**  

When species data is **binary (presence = 1, absence = 0)**, we use classification metrics to measure how well the model differentiates between the two.

#### **Accuracy** ‚Äì Overall correctness of predictions  
**Formula:**  
\[
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
\]
‚úÖ **Good for balanced datasets** but **misleading for imbalanced data** (e.g., when species are rare).  

---

#### **Sensitivity (Recall)** ‚Äì Ability to correctly predict species presence  
**Formula:**  
\[
Sensitivity = \frac{TP}{TP + FN}
\]
‚úÖ **Important for conservation** because false negatives (FN) may ignore suitable habitats.  

---

#### **Specificity** ‚Äì Ability to correctly predict absence  
**Formula:**  
\[
Specificity = \frac{TN}{TN + FP}
\]
‚úÖ **Useful in understanding over-predictions** of species presence.  

---

#### **Kappa Statistic** ‚Äì Agreement Beyond Random Chance  
Kappa adjusts accuracy to account for **random chance agreement**.  

**Formula:**  
\[
Kappa = \frac{Accuracy - Expected Accuracy}{1 - Expected Accuracy}
\]
‚úÖ **More reliable than accuracy** for imbalanced datasets.  

---

### **2. Area Under the Curve (AUC-ROC & AUC-PR)**  

AUC is one of the most common evaluation metrics in **species distribution models**.

#### **ROC Curve (Receiver Operating Characteristic Curve)**  
- Plots **True Positive Rate (Sensitivity)** vs. **False Positive Rate (1 - Specificity)**  
- **Higher AUC = Better ability to distinguish presence from absence**.  

‚úÖ **Works well for most classification tasks.**  

**Interpreting AUC Scores:**  

| **AUC Score** | **Model Performance** |
|--------------|----------------------|
| 0.5          | Random Guessing       |
| 0.7 - 0.8    | Fair                 |
| 0.8 - 0.9    | Good                 |
| 0.9 - 1.0    | Excellent            |

---

#### **Precision-Recall (PR) Curve ‚Äì Best for Rare Species**  

When species are **rare**, AUC-ROC may be misleading. Instead, use the **Precision-Recall Curve**, which evaluates model performance when **absences outnumber presences**.

- **Precision** = How many predicted presences were actually correct?  
- **Recall** = Sensitivity (ability to detect all presences).  

‚úÖ **Best when species presence is rare (e.g., endangered species).**  

---

### **3. True Skill Statistic (TSS)**  

TSS is an alternative to **AUC** that **does not depend on prevalence (species rarity)**.

**Formula:**  
\[
TSS = Sensitivity + Specificity - 1
\]

‚úÖ **Good for ecological models where presence/absence is not evenly distributed.**  
‚úÖ **Ranges from -1 (worse than random) to 1 (perfect prediction).**  

---

### **4. Root Mean Square Error (RMSE) for Continuous Models**  

When the **model outputs continuous suitability values (e.g., habitat suitability indices)**, RMSE measures how much **predictions deviate from actual presence/absence**.

**Formula:**  
\[
RMSE = \sqrt{\frac{\sum (Predicted - Observed)^2}{n}}
\]

‚úÖ **Lower RMSE = Better fit.**  
‚úÖ **Works well when comparing continuous predictions (e.g., suitability scores from MaxEnt).**  

---

## **Comparison of Model Evaluation Metrics**

| **Metric**      | **Best For**                     | **Weaknesses** |
|----------------|--------------------------------|---------------|
| **Accuracy**    | Balanced presence/absence datasets. | Misleading for imbalanced data. |
| **Sensitivity** | Conservation-focused predictions. | Can overestimate species presence. |
| **Specificity** | Preventing false positives. | Ignores presence misclassification. |
| **Kappa**       | Adjusting accuracy for chance. | Harder to interpret. |
| **AUC-ROC**     | General model evaluation. | Can be biased for rare species. |
| **AUC-PR**      | Rare species modeling. | Not commonly used in SDM. |
| **TSS**         | Presence-background models. | Less known outside ecology. |
| **RMSE**        | Continuous suitability models. | Doesn‚Äôt work for presence/absence. |

---

### **Next Steps**  

Now that we understand **model evaluation metrics**, the next section will explore **different validation techniques** (cross-validation, train-test split, and spatial validation) to ensure our models generalize well to new data. üöÄ

---
## **3. Model Validation Approaches**  

---

Model validation ensures that our **Species Distribution Models (SDMs)** perform well on unseen data. Without proper validation, models may **overfit** or fail to **generalize** to real-world conditions.

---

### **1. Train-Test Split**  

**Why Use a Separate Test Set?**  
A model that fits training data well might **fail on new data**. The **train-test split** ensures that the model is evaluated on **independent data** to measure generalization.

‚úÖ **Prevents overfitting**  
‚úÖ **Ensures predictions are reliable**  
‚úÖ **Used in all machine learning applications**  

#### **Code Example: Train-Test Split**
```r
# Load necessary package
library(caret)

# Split data: 70% Training, 30% Testing
set.seed(123)
trainIndex <- createDataPartition(data$presence, p = 0.7, list = FALSE)
train_data <- data[trainIndex, ]
test_data  <- data[-trainIndex, ]
```

---

### **2. Cross-Validation**  

Cross-validation is used when we **don‚Äôt want to lose data for training**. It divides the dataset into **multiple subsets** and trains the model several times to ensure stability.

---

#### **K-Fold Cross-Validation**  

- Splits data into **K folds** (e.g., **5-fold or 10-fold**).
- The model is trained on **K-1 folds** and tested on the remaining fold.
- The process repeats **K times**, and results are averaged.

‚úÖ **More reliable than a single train-test split**  
‚úÖ **Works well with small datasets**  

#### **Code Example: K-Fold Cross-Validation**
```r
# Perform 5-fold cross-validation
train_control <- trainControl(method = "cv", number = 5)
cv_model <- train(presence ~ ., data = train_data, method = "rf", trControl = train_control)
print(cv_model)
```

---

#### **Leave-One-Out Cross-Validation (LOOCV)**  

- Uses **one data point** as the test set and **all others** as training.
- Repeats this process for every data point.

‚úÖ **Best for very small datasets**  
‚úÖ **Ensures all data points contribute to validation**  
‚ö†Ô∏è **Computationally expensive for large datasets**  

#### **Code Example: LOOCV**
```r
# Perform Leave-One-Out Cross-Validation
train_control_loocv <- trainControl(method = "LOOCV")
loocv_model <- train(presence ~ ., data = train_data, method = "rf", trControl = train_control_loocv)
print(loocv_model)
```

---

### **3. Spatially Explicit Validation**  

Standard validation methods assume that data points are **independent**, but species occurrence points are often **spatially clustered**. **Spatially explicit validation** ensures that evaluation accounts for spatial autocorrelation.

‚úÖ **Prevents overestimating model performance**  
‚úÖ **Ensures the model works in unsampled areas**  

#### **Approach: Block Cross-Validation**  

- Divides the study area into **spatial blocks**.
- Uses some blocks for **training** and others for **testing**.

#### **Code Example: Spatial Cross-Validation**
```r
library(blockCV)

# Define spatial blocks
blocks <- spatialBlock(speciesData = data, theRange = 100000, k = 5)

# Perform cross-validation with spatial blocks
train_control_spatial <- trainControl(method = "cv", index = blocks$folds)
spatial_model <- train(presence ~ ., data = train_data, method = "rf", trControl = train_control_spatial)
print(spatial_model)
```

---

## **4. Visualizing Model Performance**  

Model evaluation is **easier to interpret** when results are visualized. Below are key ways to visualize model quality.

---

### **1. ROC and Precision-Recall Curves**  

#### **ROC Curve (Receiver Operating Characteristic Curve)**  
- **X-axis**: False Positive Rate  
- **Y-axis**: True Positive Rate  
- Higher **AUC** (closer to 1) indicates better performance.

#### **Code Example: Plot ROC Curve**
```r
library(pROC)

# Compute and plot ROC curve
roc_curve <- roc(test_data$presence, predict(model, test_data, type = "prob")[,2])
plot(roc_curve, main = "ROC Curve")
```

---

#### **Precision-Recall Curve**  

Best used for **imbalanced datasets** where absences outnumber presences.

#### **Code Example: Precision-Recall Curve**
```r
# Compute and plot Precision-Recall curve
pr_curve <- pr.curve(scores.class0 = predict(model, test_data, type = "prob")[,2],
                     weights.class0 = test_data$presence)
plot(pr_curve, main = "Precision-Recall Curve")
```

---

### **2. Suitability Maps and Threshold Selection**  

**Binary vs. Continuous Predictions**  
- **Continuous maps** show habitat suitability scores.  
- **Binary maps** classify suitable vs. unsuitable areas based on a threshold.  

#### **Thresholding Approaches**  
- **Maximize Sensitivity-Specificity**  
- **10th Percentile Presence Threshold** (for rare species).  

#### **Code Example: Suitability Map with Thresholding**
```r
library(raster)

# Predict suitability
suitability_map <- predict(model, raster_stack, type = "response")

# Convert to binary presence/absence using threshold
threshold <- 0.5
binary_map <- suitability_map > threshold

# Plot results
par(mfrow = c(1,2))
plot(suitability_map, main = "Continuous Suitability Map")
plot(binary_map, main = "Binary Presence/Absence Map")
```

---

### **3. Response Curves**  

Response curves show how **each environmental variable affects species predictions**.

‚úÖ **Ensures ecological realism**  
‚úÖ **Helps identify misleading predictors**  

#### **Code Example: Plot Response Curves**
```r
# Plot response curves for important variables
plot.gbm(model, i.var = "bio1", main = "Effect of Temperature (Bio1)")
plot.gbm(model, i.var = "bio12", main = "Effect of Precipitation (Bio12)")
```

‚úÖ **What to Look For?**  
- Do the response curves **make ecological sense**?  
- Are important predictors showing meaningful **trends**?  

---

### **Summary: Key Model Validation & Visualization Techniques**

| **Method**                         | **Purpose**                                       | **Best For** |
|-------------------------------------|-------------------------------------------------|-------------|
| **Train-Test Split**                | Evaluates performance on unseen data. | General model validation. |
| **K-Fold Cross-Validation**         | Ensures stability across multiple splits. | Large datasets. |
| **LOOCV**                           | Uses every point for training/testing. | Small datasets. |
| **Spatial Cross-Validation**        | Prevents spatial overfitting. | SDMs with clustered data. |
| **ROC Curve**                       | Evaluates sensitivity-specificity trade-off. | Presence/absence models. |
| **Precision-Recall Curve**          | Measures performance for rare species. | Imbalanced datasets. |
| **Suitability Maps**                | Visualizes habitat suitability. | Presence-only models (e.g., MaxEnt). |
| **Response Curves**                 | Ensures predictor-response relationships make sense. | All SDMs. |

---

### **Next Steps**
Now that we have **validated the model and visualized results**, the next section will explore **comparing different SDM models (CART, RF, GBM, MaxEnt) to choose the best approach for a given dataset.** üöÄ

---

## **5. Comparing Models: CART, RF, GBM, and MaxEnt**  

Selecting the best **Species Distribution Model (SDM)** depends on **data type**, **research goals**, and **computational resources**. Here, we compare **CART, Random Forest (RF), Gradient Boosting Machine (GBM), and MaxEnt** to determine when each is most useful.

---

### **1. Best Evaluation Metrics for Tree-Based vs. Presence-Background Models**  

Different model types require different evaluation metrics:  

| **Metric**      | **CART & RF (Classification)** | **GBM (Boosting)** | **MaxEnt (Presence-Only)** |
|----------------|------------------------------|-------------------|--------------------------|
| **Accuracy**   | ‚úÖ Good for presence/absence | ‚úÖ Works well | ‚ùå Not applicable |
| **AUC-ROC**    | ‚úÖ Good for all classification models | ‚úÖ Strong predictor | ‚úÖ Best for MaxEnt |
| **Precision-Recall** | ‚úÖ Works for imbalanced data | ‚úÖ Works for boosting models | ‚úÖ Works for presence-only data |
| **TSS**        | ‚úÖ Useful in SDMs | ‚úÖ More informative than accuracy | ‚úÖ Common in presence-background models |
| **Response Curves** | ‚úÖ Helps interpret variable impact | ‚úÖ Useful for interpretation | ‚úÖ Essential for ecological validation |

‚úÖ **Takeaway**: **MaxEnt relies more on AUC and presence-background validation**, while tree-based methods use **accuracy and classification metrics**.  

---

### **2. When to Use Different SDM Models?**  

| **Model**    | **Best Used For** | **Advantages** | **Disadvantages** |
|-------------|------------------|---------------|------------------|
| **CART** | Simple rules-based classification. | Easy to interpret. | Prone to overfitting. |
| **Random Forest (RF)** | Presence-absence classification. | Handles nonlinear relationships, reduces overfitting. | Computationally expensive. |
| **GBM** | High-accuracy modeling. | Best predictive power. | Requires tuning, slow training. |
| **MaxEnt** | Presence-only data modeling. | Handles small datasets, works without absence data. | Cannot model true absence data. |

‚úÖ **Takeaway**:  
- **Use CART when interpretability is key.**  
- **Use RF for balanced presence-absence classification.**  
- **Use GBM when maximum accuracy is needed.**  
- **Use MaxEnt when only presence data is available.**  

---

### **3. Performance Trade-Offs: Interpretability vs. Accuracy vs. Computational Cost**  

| **Factor**   | **CART** | **RF** | **GBM** | **MaxEnt** |
|-------------|---------|--------|--------|---------|
| **Interpretability** | ‚úÖ Very High | ‚ùå Harder to interpret | ‚ùå Harder to interpret | ‚úÖ Response curves help interpret results |
| **Accuracy** | ‚ùå Low | ‚úÖ High | ‚úÖ Very High | ‚úÖ High for presence-only |
| **Computational Cost** | ‚úÖ Fast | ‚ùå Medium-High | ‚ùå High | ‚úÖ Efficient for presence-only |

‚úÖ **Takeaway**:  
- **Use CART when interpretability is more important than accuracy.**  
- **Use RF/GBM when accuracy is the priority.**  
- **Use MaxEnt when working with presence-only data and computational efficiency is needed.**  

---

## **6. Common Pitfalls in Model Evaluation**  

Even when models perform well, **several pitfalls** can lead to misleading results.

### **1. Overfitting: When a Model is Too Complex and Fails to Generalize**  

‚ö†Ô∏è **Problem**: A model that fits training data perfectly but performs poorly on new data.  

üîπ **Solution**:  
- Use **cross-validation** to ensure generalization.  
- **Regularization** in GBM and MaxEnt prevents overfitting.  
- **Limit tree depth** in CART and RF.  

---

### **2. Ignoring Spatial Bias: Why Presence Points Should Be Spatially Independent**  

‚ö†Ô∏è **Problem**:  
- If presence records **cluster near roads or specific locations**, the model may falsely learn that species prefer those areas.  

üîπ **Solution**:  
- Use **spatial cross-validation** instead of random splits.  
- **Apply bias correction** in MaxEnt to adjust for sampling effort.  

---

### **3. Improper Thresholding: Setting Unrealistic Suitability Cutoffs**  

‚ö†Ô∏è **Problem**:  
- Converting **continuous suitability scores** into **binary presence-absence maps** using arbitrary thresholds.  

üîπ **Solution**:  
- Use **Maximize Sensitivity-Specificity Thresholding**.  
- Compare **multiple thresholding methods** to select the most meaningful.  

‚úÖ **Takeaway**: Always validate threshold selection with ecological knowledge.  

---

## **7. Summary & Best Practices**  

### **Key Takeaways from Model Evaluation**  
- **Different models require different evaluation metrics** ‚Äì RF/GBM rely on classification metrics, while MaxEnt uses presence-background validation.  
- **Choose models based on data availability** ‚Äì Presence-absence models work well for classification, while MaxEnt is best for presence-only data.  
- **Avoid common pitfalls** ‚Äì Overfitting, spatial bias, and improper thresholding can reduce model reliability.  

---

### **Recommended Workflow for Assessing SDM Performance**  

‚úÖ **Step 1: Choose an SDM Model Based on Data Type**  
- Use **CART, RF, or GBM for presence-absence**.  
- Use **MaxEnt for presence-only data**.  

‚úÖ **Step 2: Apply Proper Validation Techniques**  
- Use **Train-Test Splitting** or **Cross-Validation** for presence-absence models.  
- Use **Spatial Cross-Validation** when presence points are clustered.  

‚úÖ **Step 3: Select the Right Performance Metrics**  
- **Accuracy, Sensitivity, Specificity, Kappa** for RF/GBM.  
- **AUC, TSS, PR-Curve** for MaxEnt.  

‚úÖ **Step 4: Interpret Results Ecologically**  
- Ensure that **response curves make biological sense**.  
- Check that **species presence is predicted in ecologically relevant areas**.  

---

### **Iterative Improvements: How to Refine Models Based on Evaluation Results**  

üîÑ **If AUC is low** ‚Üí Try different feature selection methods.  
üîÑ **If overfitting occurs** ‚Üí Reduce model complexity (e.g., increase regularization in GBM, limit tree depth in RF).  
üîÑ **If presence points are biased** ‚Üí Use spatially explicit validation.  

‚úÖ **Final Takeaway**:  
By following **best practices in model evaluation**, you ensure that **species distribution models are robust, accurate, and ecologically meaningful**. üöÄ
